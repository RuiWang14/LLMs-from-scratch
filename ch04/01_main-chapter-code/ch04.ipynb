{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08f4321d-d32a-4a90-bfc7-e923f316b2f8",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9295b2-182b-490b-8325-83a67c4a001d",
   "metadata": {},
   "source": [
    "# Chapter 4: Implementing a GPT model from Scratch To Generate Text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9eac223-a125-40f7-bacc-bd0d890450c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib version: 3.10.0\n",
      "torch version: 2.7.0\n",
      "tiktoken version: 0.9.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "print(\"matplotlib version:\", version(\"matplotlib\"))\n",
    "print(\"torch version:\", version(\"torch\"))\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7da97ed-e02f-4d7f-b68e-a0eba3716e02",
   "metadata": {},
   "source": [
    "- In this chapter, we implement a GPT-like LLM architecture; the next chapter will focus on training this LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4f11e0-4434-4979-9dee-e1207df0eb01",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/01.webp\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fe99ab-0bcf-4778-a6b5-6db81fb826ef",
   "metadata": {},
   "source": [
    "## 4.1 Coding an LLM architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad72d1ff-d82d-4e33-a88e-3c1a8831797b",
   "metadata": {},
   "source": [
    "- Chapter 1 discussed models like GPT and Llama, which generate words sequentially and are based on the decoder part of the original transformer architecture\n",
    "- Therefore, these LLMs are often referred to as \"decoder-like\" LLMs\n",
    "- Compared to conventional deep learning models, LLMs are larger, mainly due to their vast number of parameters, not the amount of code\n",
    "- We'll see that many elements are repeated in an LLM's architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5213e9-bd1c-437e-aee8-f5e8fb717251",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/02.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d43f5e2-fb51-434a-b9be-abeef6b98d99",
   "metadata": {},
   "source": [
    "- In previous chapters, we used small embedding dimensions for token inputs and outputs for ease of illustration, ensuring they fit on a single page\n",
    "- In this chapter, we consider embedding and model sizes akin to a small GPT-2 model\n",
    "- We'll specifically code the architecture of the smallest GPT-2 model (124 million parameters), as outlined in Radford et al.'s [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) (note that the initial report lists it as 117M parameters, but this was later corrected in the model weight repository)\n",
    "- Chapter 6 will show how to load pretrained weights into our implementation, which will be compatible with model sizes of 345, 762, and 1542 million parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21baa14d-24b8-4820-8191-a2808f7fbabc",
   "metadata": {},
   "source": [
    "- Configuration details for the 124 million parameter GPT-2 model include:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ed66875-1f24-445d-add6-006aae3c5707",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12fcd28-d210-4c57-8be6-06cfcd5d73a4",
   "metadata": {},
   "source": [
    "- We use short variable names to avoid long lines of code later\n",
    "- `\"vocab_size\"` indicates a vocabulary size of 50,257 words, supported by the BPE tokenizer discussed in Chapter 2\n",
    "- `\"context_length\"` represents the model's maximum input token count, as enabled by positional embeddings covered in Chapter 2\n",
    "- `\"emb_dim\"` is the embedding size for token inputs, converting each input token into a 768-dimensional vector\n",
    "- `\"n_heads\"` is the number of attention heads in the multi-head attention mechanism implemented in Chapter 3\n",
    "- `\"n_layers\"` is the number of transformer blocks within the model, which we'll implement in upcoming sections\n",
    "- `\"drop_rate\"` is the dropout mechanism's intensity, discussed in Chapter 3; 0.1 means dropping 10% of hidden units during training to mitigate overfitting\n",
    "- `\"qkv_bias\"` decides if the `Linear` layers in the multi-head attention mechanism (from Chapter 3) should include a bias vector when computing query (Q), key (K), and value (V) tensors; we'll disable this option, which is standard practice in modern LLMs; however, we'll revisit this later when loading pretrained GPT-2 weights from OpenAI into our reimplementation in chapter 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adce779-857b-4418-9501-12a7f3818d88",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/03.webp\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "619c2eed-f8ea-4ff5-92c3-feda0f29b227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        # position embedding\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        # Use a placeholder for TransformerBlock\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        \n",
    "        # Use a placeholder for LayerNorm\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "\n",
    "        # print(\"batch_size, seq_len\",batch_size, seq_len)\n",
    "        # print(\"tok_embeds\",tok_embeds.shape)\n",
    "        # print(\"pos_embeds\",pos_embeds.shape)\n",
    "        # print(\"x\",x.shape)\n",
    "        # print(\"logits\", logits.shape)\n",
    "        # print(\"forward finish!\")\n",
    "        return logits\n",
    "\n",
    "\n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        # A simple placeholder\n",
    "\n",
    "    def forward(self, x):\n",
    "        # This block does nothing and just returns its input.\n",
    "        return x\n",
    "\n",
    "\n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "        # The parameters here are just to mimic the LayerNorm interface.\n",
    "\n",
    "    def forward(self, x):\n",
    "        # This layer does nothing and just returns its input.\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9665e8ab-20ca-4100-b9b9-50d9bdee33be",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/04.webp?123\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "794b6b6c-d36f-411e-a7db-8ac566a87fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "batch = []\n",
    "\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e60af91-4dc3-4b3d-8f8c-e196311ffe85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Encoding 'gpt2'>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "009238cd-0160-4834-979c-309710986bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size, seq_len 2 4\n",
      "tok_embeds torch.Size([2, 4, 768])\n",
      "pos_embeds torch.Size([4, 768])\n",
      "x torch.Size([2, 4, 768])\n",
      "logits torch.Size([2, 4, 50257])\n",
      "forward finish!\n",
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[-1.2034,  0.3201, -0.7130,  ..., -1.5548, -0.2390, -0.4667],\n",
      "         [-0.1192,  0.4539, -0.4432,  ...,  0.2392,  1.3469,  1.2430],\n",
      "         [ 0.5307,  1.6720, -0.4695,  ...,  1.1966,  0.0111,  0.5835],\n",
      "         [ 0.0139,  1.6755, -0.3388,  ...,  1.1586, -0.0435, -1.0400]],\n",
      "\n",
      "        [[-1.0908,  0.1798, -0.9484,  ..., -1.6047,  0.2439, -0.4530],\n",
      "         [-0.7860,  0.5581, -0.0610,  ...,  0.4835, -0.0077,  1.6621],\n",
      "         [ 0.3567,  1.2698, -0.6398,  ..., -0.0162, -0.1296,  0.3717],\n",
      "         [-0.2407, -0.7349, -0.5102,  ...,  2.0057, -0.3694,  0.1814]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "logits = model(batch)\n",
    "print(\"Output shape:\", logits.shape)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ee22ab2-2968-4fca-a6e1-c8934c33133e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DummyGPTModel(\n",
      "  (tok_emb): Embedding(50257, 768)\n",
      "  (pos_emb): Embedding(1024, 768)\n",
      "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
      "  (trf_blocks): Sequential(\n",
      "    (0): DummyTransformerBlock()\n",
      "    (1): DummyTransformerBlock()\n",
      "    (2): DummyTransformerBlock()\n",
      "    (3): DummyTransformerBlock()\n",
      "    (4): DummyTransformerBlock()\n",
      "    (5): DummyTransformerBlock()\n",
      "    (6): DummyTransformerBlock()\n",
      "    (7): DummyTransformerBlock()\n",
      "    (8): DummyTransformerBlock()\n",
      "    (9): DummyTransformerBlock()\n",
      "    (10): DummyTransformerBlock()\n",
      "    (11): DummyTransformerBlock()\n",
      "  )\n",
      "  (final_norm): DummyLayerNorm()\n",
      "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4, 50257])\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "print(batch.shape)\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fad0fe-895d-4493-9e48-962e2d46c66f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Note**\n",
    "\n",
    "- If you are running this code on Windows or Linux, the resulting values above may look like as follows:\n",
    "    \n",
    "```\n",
    "Output shape: torch.Size([2, 4, 50257])\n",
    "tensor([[[-0.9289,  0.2748, -0.7557,  ..., -1.6070,  0.2702, -0.5888],\n",
    "         [-0.4476,  0.1726,  0.5354,  ..., -0.3932,  1.5285,  0.8557],\n",
    "         [ 0.5680,  1.6053, -0.2155,  ...,  1.1624,  0.1380,  0.7425],\n",
    "         [ 0.0447,  2.4787, -0.8843,  ...,  1.3219, -0.0864, -0.5856]],\n",
    "\n",
    "        [[-1.5474, -0.0542, -1.0571,  ..., -1.8061, -0.4494, -0.6747],\n",
    "         [-0.8422,  0.8243, -0.1098,  ..., -0.1434,  0.2079,  1.2046],\n",
    "         [ 0.1355,  1.1858, -0.1453,  ...,  0.0869, -0.1590,  0.1552],\n",
    "         [ 0.1666, -0.8138,  0.2307,  ...,  2.5035, -0.3055, -0.3083]]],\n",
    "       grad_fn=<UnsafeViewBackward0>)\n",
    "```\n",
    "\n",
    "- Since these are just random numbers, this is not a reason for concern, and you can proceed with the remainder of the chapter without issues\n",
    "- One possible reason for this discrepancy is the differing behavior of `nn.Dropout` across operating systems, depending on how PyTorch was compiled, as discussed [here on the PyTorch issue tracker](https://github.com/pytorch/pytorch/issues/121595)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8332a00-98da-4eb4-b882-922776a89917",
   "metadata": {},
   "source": [
    "## 4.2 Normalizing activations with layer normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066cfb81-d59b-4d95-afe3-e43cf095f292",
   "metadata": {},
   "source": [
    "- Layer normalization, also known as LayerNorm ([Ba et al. 2016](https://arxiv.org/abs/1607.06450)), centers the activations of a neural network layer around a mean of 0 and normalizes their variance to 1\n",
    "- This stabilizes training and enables faster convergence to effective weights\n",
    "- Layer normalization is applied both before and after the multi-head attention module within the transformer block, which we will implement later; it's also applied before the final output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314ac47a-69cc-4597-beeb-65bed3b5910f",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/05.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab49940-6b35-4397-a80e-df8d092770a7",
   "metadata": {},
   "source": [
    "- Let's see how layer normalization works by passing a small input sample through a simple neural network layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "79e1b463-dc3f-44ac-9cdb-9d5b6f64eb9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
      "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "Sequential(\n",
      "  (0): Linear(in_features=5, out_features=6, bias=True)\n",
      "  (1): ReLU()\n",
      ")\n",
      "tensor([[-0.1115,  0.1204, -0.3696, -0.2404, -1.1969],\n",
      "        [ 0.2093, -0.9724, -0.7550,  0.3239, -0.1085]]) torch.Size([2, 5])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "# create 2 training examples with 5 dimensions (features) each\n",
    "batch_example = torch.randn(2, 5) \n",
    "\n",
    "layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\n",
    "out = layer(batch_example)\n",
    "print(out)\n",
    "\n",
    "print(layer)\n",
    "print(batch_example,batch_example.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fccc29e-71fc-4c16-898c-6137c6ea5d2e",
   "metadata": {},
   "source": [
    "- Let's compute the mean and variance for each of the 2 inputs above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9888f79e-8e69-44aa-8a19-cd34292adbf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[0.1324],\n",
      "        [0.2170]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[0.0231],\n",
      "        [0.0398]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mean = out.mean(dim=-1, keepdim=True)\n",
    "var = out.var(dim=-1, keepdim=True)\n",
    "\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052eda3e-b395-48c4-acd4-eb8083bab958",
   "metadata": {},
   "source": [
    "- The normalization is applied to each of the two inputs (rows) independently; using dim=-1 applies the calculation across the last dimension (in this case, the feature dimension) instead of the row dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570db83a-205c-4f6f-b219-1f6195dde1a7",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/06.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8ecbc7-eb14-4fa1-b5d0-7e1ff9694f99",
   "metadata": {},
   "source": [
    "- Subtracting the mean and dividing by the square-root of the variance (standard deviation) centers the inputs to have a mean of 0 and a variance of 1 across the column (feature) dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9a1d1bb9-3341-4c9a-bc2a-d2489bf89cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized layer outputs:\n",
      " tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n",
      "        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Mean:\n",
      " tensor([[9.9341e-09],\n",
      "        [1.9868e-08]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out_norm = (out - mean) / torch.sqrt(var)\n",
    "print(\"Normalized layer outputs:\\n\", out_norm)\n",
    "\n",
    "mean = out_norm.mean(dim=-1, keepdim=True)\n",
    "var = out_norm.var(dim=-1, keepdim=True)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac62b90c-7156-4979-9a79-ce1fb92969c1",
   "metadata": {},
   "source": [
    "- Each input is centered at 0 and has a unit variance of 1; to improve readability, we can disable PyTorch's scientific notation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3e06c34b-c68a-4b36-afbe-b30eda4eca39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[    0.0000],\n",
      "        [    0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(sci_mode=False)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944fb958-d4ed-43cc-858d-00052bb6b31a",
   "metadata": {},
   "source": [
    "- Above, we normalized the features of each input\n",
    "- Now, using the same idea, we can implement a `LayerNorm` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3333a305-aa3d-460a-bcce-b80662d464d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56c3908-7544-4808-b8cb-5d0a55bcca72",
   "metadata": {},
   "source": [
    "**Scale and shift**\n",
    "\n",
    "- Note that in addition to performing the normalization by subtracting the mean and dividing by the variance, we added two trainable parameters, a `scale` and a `shift` parameter\n",
    "- The initial `scale` (multiplying by 1) and `shift` (adding 0) values don't have any effect; however, `scale` and `shift` are trainable parameters that the LLM automatically adjusts during training if it is determined that doing so would improve the model's performance on its training task\n",
    "- This allows the model to learn appropriate scaling and shifting that best suit the data it is processing\n",
    "- Note that we also add a smaller value (`eps`) before computing the square root of the variance; this is to avoid division-by-zero errors if the variance is 0\n",
    "\n",
    "**Biased variance**\n",
    "- In the variance calculation above, setting `unbiased=False` means using the formula $\\frac{\\sum_i (x_i - \\bar{x})^2}{n}$ to compute the variance where n is the sample size (here, the number of features or columns); this formula does not include Bessel's correction (which uses `n-1` in the denominator), thus providing a biased estimate of the variance \n",
    "- For LLMs, where the embedding dimension `n` is very large, the difference between using n and `n-1`\n",
    " is negligible\n",
    "- However, GPT-2 was trained with a biased variance in the normalization layers, which is why we also adopted this setting for compatibility reasons with the pretrained weights that we will load in later chapters\n",
    "\n",
    "- Let's now try out `LayerNorm` in practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "23b1000a-e613-4b43-bd90-e54deed8d292",
   "metadata": {},
   "outputs": [],
   "source": [
    "ln = LayerNorm(emb_dim=5)\n",
    "out_ln = ln(batch_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "94c12de2-1cab-46e0-a099-e2e470353bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[    -0.0000],\n",
      "        [     0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mean = out_ln.mean(dim=-1, keepdim=True)\n",
    "var = out_ln.var(dim=-1, unbiased=False, keepdim=True)\n",
    "\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e136cfc4-7c89-492e-b120-758c272bca8c",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/07.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11190e7d-8c29-4115-824a-e03702f9dd54",
   "metadata": {},
   "source": [
    "## 4.3 Implementing a feed forward network with GELU activations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0585dfb-f21e-40e5-973f-2f63ad5cb169",
   "metadata": {},
   "source": [
    "- In this section, we implement a small neural network submodule that is used as part of the transformer block in LLMs\n",
    "- We start with the activation function\n",
    "- In deep learning, ReLU (Rectified Linear Unit) activation functions are commonly used due to their simplicity and effectiveness in various neural network architectures\n",
    "- In LLMs, various other types of activation functions are used beyond the traditional ReLU; two notable examples are GELU (Gaussian Error Linear Unit) and SwiGLU (Swish-Gated Linear Unit)\n",
    "- GELU and SwiGLU are more complex, smooth activation functions incorporating Gaussian and sigmoid-gated linear units, respectively, offering better performance for deep learning models, unlike the simpler, piecewise linear function of ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d482ce7-e493-4bfc-a820-3ea99f564ebc",
   "metadata": {},
   "source": [
    "- GELU ([Hendrycks and Gimpel 2016](https://arxiv.org/abs/1606.08415)) can be implemented in several ways; the exact version is defined as GELU(x)=x⋅Φ(x), where Φ(x) is the cumulative distribution function of the standard Gaussian distribution.\n",
    "- In practice, it's common to implement a computationally cheaper approximation: $\\text{GELU}(x) \\approx 0.5 \\cdot x \\cdot \\left(1 + \\tanh\\left[\\sqrt{\\frac{2}{\\pi}} \\cdot \\left(x + 0.044715 \\cdot x^3\\right)\\right]\\right)\n",
    "$ (the original GPT-2 model was also trained with this approximation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f84694b7-95f3-4323-b6d6-0a73df278e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fc5487d2-2576-4118-80a7-56c4caac2e71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "gelu, relu = GELU(), nn.ReLU()\n",
    "\n",
    "# Some sample data\n",
    "x = torch.linspace(-3, 3, 100)\n",
    "y_gelu, y_relu = gelu(x), relu(x)\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "for i, (y, label) in enumerate(zip([y_gelu, y_relu], [\"GELU\", \"ReLU\"]), 1):\n",
    "    plt.subplot(1, 2, i)\n",
    "    plt.plot(x, y)\n",
    "    plt.title(f\"{label} activation function\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(f\"{label}(x)\")\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd01662-14cb-43fd-bffd-2d702813de2d",
   "metadata": {},
   "source": [
    "- As we can see, ReLU is a piecewise linear function that outputs the input directly if it is positive; otherwise, it outputs zero\n",
    "- GELU is a smooth, non-linear function that approximates ReLU but with a non-zero gradient for negative values (except at approximately -0.75)\n",
    "\n",
    "- Next, let's implement the small neural network module, `FeedForward`, that we will be using in the LLM's transformer block later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9275c879-b148-4579-a107-86827ca14d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7c4976e2-0261-418e-b042-c5be98c2ccaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n"
     ]
    }
   ],
   "source": [
    "print(GPT_CONFIG_124M[\"emb_dim\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcaacfa-3cfc-4c9e-b668-b71a2753145a",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/09.webp?12\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "928e7f7c-d0b1-499f-8d07-4cadb428a6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 768])\n"
     ]
    }
   ],
   "source": [
    "ffn = FeedForward(GPT_CONFIG_124M)\n",
    "\n",
    "# input shape: [batch_size, num_token, emb_size]\n",
    "x = torch.rand(2, 3, 768) \n",
    "out = ffn(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8756c5-6b04-443b-93d0-e555a316c377",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/10.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5da2a50-04f4-4388-af23-ad32e405a972",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/11.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffcb905-53c7-4886-87d2-4464c5fecf89",
   "metadata": {},
   "source": [
    "## 4.4 Adding shortcut connections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffae416c-821e-4bfa-a741-8af4ba5db00e",
   "metadata": {},
   "source": [
    "- Next, let's talk about the concept behind shortcut connections, also called skip or residual connections\n",
    "- Originally, shortcut connections were proposed in deep networks for computer vision (residual networks) to mitigate vanishing gradient problems\n",
    "- A shortcut connection creates an alternative shorter path for the gradient to flow through the network\n",
    "- This is achieved by adding the output of one layer to the output of a later layer, usually skipping one or more layers in between\n",
    "- Let's illustrate this idea with a small example network:\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/12.webp?123\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cfd241-a32e-4601-8790-784b82f2f23e",
   "metadata": {},
   "source": [
    "- In code, it looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "05473938-799c-49fd-86d4-8ed65f94fee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleDeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes, use_shortcut):\n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), GELU())\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            # Compute the output of the current layer\n",
    "            layer_output = layer(x)\n",
    "            # Check if shortcut can be applied\n",
    "            if self.use_shortcut and x.shape == layer_output.shape:\n",
    "                x = x + layer_output\n",
    "            else:\n",
    "                x = layer_output\n",
    "        return x\n",
    "\n",
    "\n",
    "def print_gradients(model, x):\n",
    "    # Forward pass\n",
    "    output = model(x)\n",
    "    target = torch.tensor([[0.]])\n",
    "\n",
    "    # Calculate loss based on how close the target\n",
    "    # and output are\n",
    "    loss = nn.MSELoss()\n",
    "    loss = loss(output, target)\n",
    "    \n",
    "    # Backward pass to calculate the gradients\n",
    "    loss.backward()\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            # Print the mean absolute gradient of the weights\n",
    "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39bf277-b3db-4bb1-84ce-7a20caff1011",
   "metadata": {},
   "source": [
    "- Let's print the gradient values first **without** shortcut connections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c75f43cc-6923-4018-b980-26023086572c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.00020173584925942123\n",
      "layers.1.0.weight has gradient mean of 0.00012011159560643137\n",
      "layers.2.0.weight has gradient mean of 0.0007152040489017963\n",
      "layers.3.0.weight has gradient mean of 0.0013988736318424344\n",
      "layers.4.0.weight has gradient mean of 0.005049645435065031\n"
     ]
    }
   ],
   "source": [
    "layer_sizes = [3, 3, 3, 3, 3, 1]  \n",
    "\n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model_without_shortcut = ExampleDeepNeuralNetwork(\n",
    "    layer_sizes, use_shortcut=False\n",
    ")\n",
    "print_gradients(model_without_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837fd5d4-7345-4663-97f5-38f19dfde621",
   "metadata": {},
   "source": [
    "- Next, let's print the gradient values **with** shortcut connections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "11b7c0c2-f9dd-4dd5-b096-a05c48c5f6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.22169792652130127\n",
      "layers.1.0.weight has gradient mean of 0.20694108307361603\n",
      "layers.2.0.weight has gradient mean of 0.3289699852466583\n",
      "layers.3.0.weight has gradient mean of 0.2665732204914093\n",
      "layers.4.0.weight has gradient mean of 1.3258541822433472\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model_with_shortcut = ExampleDeepNeuralNetwork(\n",
    "    layer_sizes, use_shortcut=True\n",
    ")\n",
    "print_gradients(model_with_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ff783a-46f0-49c5-a7a9-26a525764b6e",
   "metadata": {},
   "source": [
    "- As we can see based on the output above, shortcut connections prevent the gradients from vanishing in the early layers (towards `layer.0`)\n",
    "- We will use this concept of a shortcut connection next when we implement a transformer block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae578ca-e564-42cf-8635-a2267047cdff",
   "metadata": {},
   "source": [
    "## 4.5 Connecting attention and linear layers in a transformer block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3daac6f-6545-4258-8f2d-f45a7394f429",
   "metadata": {},
   "source": [
    "- In this section, we now combine the previous concepts into a so-called transformer block\n",
    "- A transformer block combines the causal multi-head attention module from the previous chapter with the linear layers, the feed forward neural network we implemented in an earlier section\n",
    "- In addition, the transformer block also uses dropout and shortcut connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0e1e8176-e5e3-4152-b1aa-0bbd7891dfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the `previous_chapters.py` file is not available locally,\n",
    "# you can import it from the `llms-from-scratch` PyPI package.\n",
    "# For details, see: https://github.com/rasbt/LLMs-from-scratch/tree/main/pkg\n",
    "# E.g.,\n",
    "# from llms_from_scratch.ch03 import MultiHeadAttention\n",
    "\n",
    "from previous_chapters import MultiHeadAttention\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"], \n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        # Shortcut connection for feed forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b64d16-94a6-4d13-8c85-9494c50478a9",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/13.webp?1\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d2d375-87bd-4153-9040-63a1e6a2b7cb",
   "metadata": {},
   "source": [
    "- Suppose we have 2 input samples with 6 tokens each, where each token is a 768-dimensional embedding vector; then this transformer block applies self-attention, followed by linear layers, to produce an output of similar size\n",
    "- You can think of the output as an augmented version of the context vectors we discussed in the previous chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3fb45a63-b1f3-4b08-b525-dafbc8228405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "x = torch.rand(2, 4, 768)  # Shape: [batch_size, num_tokens, emb_dim]\n",
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "output = block(x)\n",
    "\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9e4ee4-cf23-4583-b1fd-317abb4fcd13",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/14.webp?1\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46618527-15ac-4c32-ad85-6cfea83e006e",
   "metadata": {},
   "source": [
    "## 4.6 Coding the GPT model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec7d03d-9ff3-4ca3-ad67-01b67c2f5457",
   "metadata": {},
   "source": [
    "- We are almost there: now let's plug in the transformer block into the architecture we coded at the very beginning of this chapter so that we obtain a usable GPT architecture\n",
    "- Note that the transformer block is repeated multiple times; in the case of the smallest 124M GPT-2 model, we repeat it 12 times:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7b362d-f8c5-48d2-8ebd-722480ac5073",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/15.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324e4b5d-ed89-4fdf-9a52-67deee0593bc",
   "metadata": {},
   "source": [
    "- The corresponding code implementation, where `cfg[\"n_layers\"] = 12`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c61de39c-d03c-4a32-8b57-f49ac3834857",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        \n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2750270f-c45d-4410-8767-a6adbd05d5c3",
   "metadata": {},
   "source": [
    "- Using the configuration of the 124M parameter model, we can now instantiate this GPT model with random initial weights as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ef94fd9c-4e9d-470d-8f8e-dd23d1bb1f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch:\n",
      " tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "\n",
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[ 0.3613,  0.4222, -0.0711,  ...,  0.3483,  0.4661, -0.2838],\n",
      "         [-0.1792, -0.5660, -0.9485,  ...,  0.0477,  0.5181, -0.3168],\n",
      "         [ 0.7120,  0.0332,  0.1085,  ...,  0.1018, -0.4327, -0.2553],\n",
      "         [-1.0076,  0.3418, -0.1190,  ...,  0.7195,  0.4023,  0.0532]],\n",
      "\n",
      "        [[-0.2564,  0.0900,  0.0335,  ...,  0.2659,  0.4454, -0.6806],\n",
      "         [ 0.1230,  0.3653, -0.2074,  ...,  0.7705,  0.2710,  0.2246],\n",
      "         [ 1.0558,  1.0318, -0.2800,  ...,  0.6936,  0.3205, -0.3178],\n",
      "         [-0.1565,  0.3926,  0.3288,  ...,  1.2630, -0.1858,  0.0388]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "GPTModel(\n",
      "  (tok_emb): Embedding(50257, 768)\n",
      "  (pos_emb): Embedding(1024, 768)\n",
      "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
      "  (trf_blocks): Sequential(\n",
      "    (0): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (3): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (4): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (5): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (6): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (7): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (8): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (9): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (10): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (11): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_norm): LayerNorm()\n",
      "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "out = model(batch)\n",
    "print(\"Input batch:\\n\", batch)\n",
    "print(\"\\nOutput shape:\", out.shape)\n",
    "print(out)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d616e7a-568b-4921-af29-bd3f4683cd2e",
   "metadata": {},
   "source": [
    "- We will train this model in the next chapter\n",
    "- However, a quick note about its size: we previously referred to it as a 124M parameter model; we can double check this number as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "84fb8be4-9d3b-402b-b3da-86b663aac33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 163,009,536\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[     0.3374,     -0.1778,     -0.3035,  ...,     -0.3181,\n",
       "              -1.3936,      0.5226],\n",
       "         [     0.2579,      0.3420,     -0.8168,  ...,     -0.4098,\n",
       "               0.4978,     -0.3721],\n",
       "         [     0.7957,      0.5350,      0.9427,  ...,     -1.0749,\n",
       "               0.0955,     -1.4138],\n",
       "         ...,\n",
       "         [    -0.7128,     -0.5019,      1.4119,  ...,     -0.1498,\n",
       "              -0.4898,     -1.0620],\n",
       "         [     2.0646,      1.1190,      0.3849,  ...,     -0.7202,\n",
       "              -0.5570,      0.9864],\n",
       "         [     0.0011,     -0.7532,     -0.1792,  ...,     -0.3244,\n",
       "               0.2606,      0.5889]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.8769,  0.2550,  0.8441,  ..., -1.0354,  1.3085,  1.7957],\n",
       "         [-1.0029,  0.0995,  1.2459,  ...,  1.5453, -0.1126, -1.5197],\n",
       "         [ 1.3317,  0.7561,  0.9077,  ...,  0.0830,  1.8336, -2.2225],\n",
       "         ...,\n",
       "         [-0.1055, -1.1941, -1.1472,  ..., -1.4544,  0.2918, -0.5483],\n",
       "         [ 0.2218, -0.3332, -1.3375,  ..., -0.4280, -0.0806,  1.7783],\n",
       "         [ 1.1077,  0.0933,  0.8395,  ..., -1.4756,  1.1813,  2.3671]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0266,  0.0049, -0.0182,  ...,  0.0070, -0.0124, -0.0275],\n",
       "         [ 0.0156, -0.0022, -0.0125,  ..., -0.0274,  0.0311,  0.0285],\n",
       "         [ 0.0044, -0.0063, -0.0033,  ..., -0.0279, -0.0054, -0.0342],\n",
       "         ...,\n",
       "         [ 0.0150, -0.0191,  0.0103,  ..., -0.0287,  0.0078,  0.0257],\n",
       "         [ 0.0301, -0.0164,  0.0020,  ...,  0.0142, -0.0351,  0.0306],\n",
       "         [-0.0109, -0.0126, -0.0245,  ...,  0.0004,  0.0029,  0.0042]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0115, -0.0161,  0.0170,  ..., -0.0193, -0.0269,  0.0054],\n",
       "         [ 0.0125, -0.0345, -0.0224,  ..., -0.0216,  0.0346,  0.0022],\n",
       "         [-0.0311, -0.0035, -0.0185,  ...,  0.0320,  0.0299, -0.0143],\n",
       "         ...,\n",
       "         [-0.0225, -0.0181,  0.0014,  ..., -0.0070,  0.0272,  0.0243],\n",
       "         [ 0.0238, -0.0182, -0.0289,  ...,  0.0255, -0.0353,  0.0074],\n",
       "         [-0.0251, -0.0006,  0.0302,  ..., -0.0309,  0.0350,  0.0207]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0301,  0.0241,  0.0034,  ..., -0.0299, -0.0174,  0.0045],\n",
       "         [ 0.0273,  0.0173, -0.0071,  ...,  0.0114,  0.0329,  0.0273],\n",
       "         [-0.0012,  0.0062,  0.0189,  ..., -0.0198,  0.0092, -0.0010],\n",
       "         ...,\n",
       "         [ 0.0180, -0.0353, -0.0344,  ..., -0.0247,  0.0071,  0.0232],\n",
       "         [ 0.0301,  0.0354,  0.0320,  ...,  0.0084,  0.0132,  0.0334],\n",
       "         [-0.0122, -0.0111, -0.0168,  ..., -0.0063,  0.0201,  0.0099]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0200, -0.0237, -0.0298,  ..., -0.0132, -0.0345,  0.0227],\n",
       "         [ 0.0090, -0.0287, -0.0344,  ..., -0.0360,  0.0031, -0.0080],\n",
       "         [-0.0122, -0.0287, -0.0022,  ...,  0.0148, -0.0062,  0.0047],\n",
       "         ...,\n",
       "         [ 0.0328,  0.0292,  0.0227,  ..., -0.0281,  0.0156,  0.0137],\n",
       "         [-0.0209, -0.0133, -0.0031,  ...,  0.0205,  0.0308,  0.0156],\n",
       "         [ 0.0280, -0.0056, -0.0019,  ...,  0.0179,  0.0034,  0.0140]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([    -0.0119,     -0.0170,      0.0257,      0.0052,      0.0259,\n",
       "              0.0279,      0.0081,     -0.0217,      0.0091,     -0.0042,\n",
       "              0.0093,      0.0222,     -0.0055,     -0.0053,     -0.0360,\n",
       "             -0.0037,      0.0020,     -0.0019,      0.0173,     -0.0062,\n",
       "             -0.0164,      0.0079,      0.0108,      0.0100,     -0.0313,\n",
       "             -0.0201,      0.0196,      0.0343,      0.0114,      0.0080,\n",
       "             -0.0200,     -0.0280,      0.0342,      0.0188,      0.0296,\n",
       "             -0.0225,     -0.0223,      0.0082,      0.0188,     -0.0174,\n",
       "              0.0214,      0.0347,      0.0047,     -0.0086,      0.0024,\n",
       "              0.0118,     -0.0329,      0.0288,     -0.0176,      0.0328,\n",
       "             -0.0308,     -0.0134,     -0.0041,     -0.0174,      0.0245,\n",
       "              0.0359,     -0.0238,     -0.0244,      0.0281,     -0.0328,\n",
       "             -0.0078,     -0.0343,      0.0178,      0.0042,      0.0058,\n",
       "              0.0099,      0.0074,     -0.0163,      0.0144,      0.0259,\n",
       "              0.0151,      0.0140,     -0.0263,      0.0076,      0.0232,\n",
       "             -0.0144,     -0.0109,      0.0180,     -0.0141,     -0.0090,\n",
       "             -0.0061,     -0.0256,     -0.0293,     -0.0002,      0.0075,\n",
       "              0.0122,     -0.0025,     -0.0001,     -0.0231,     -0.0261,\n",
       "             -0.0009,     -0.0057,      0.0297,     -0.0001,      0.0092,\n",
       "             -0.0041,      0.0058,     -0.0190,      0.0136,     -0.0035,\n",
       "             -0.0018,     -0.0166,     -0.0016,      0.0200,      0.0176,\n",
       "             -0.0015,     -0.0345,     -0.0204,      0.0164,      0.0334,\n",
       "             -0.0272,     -0.0139,     -0.0111,      0.0103,      0.0064,\n",
       "             -0.0259,     -0.0022,      0.0099,     -0.0211,     -0.0076,\n",
       "              0.0021,     -0.0330,      0.0315,      0.0167,      0.0334,\n",
       "              0.0059,      0.0198,      0.0122,      0.0327,     -0.0258,\n",
       "              0.0236,     -0.0087,      0.0003,     -0.0270,     -0.0146,\n",
       "             -0.0157,     -0.0168,      0.0279,     -0.0185,     -0.0033,\n",
       "              0.0318,     -0.0169,      0.0142,     -0.0010,      0.0279,\n",
       "             -0.0076,      0.0123,     -0.0070,      0.0124,      0.0083,\n",
       "             -0.0211,      0.0129,      0.0031,      0.0319,      0.0244,\n",
       "              0.0182,      0.0158,     -0.0213,      0.0118,     -0.0086,\n",
       "             -0.0255,      0.0108,      0.0185,      0.0051,      0.0172,\n",
       "             -0.0297,      0.0043,      0.0131,     -0.0290,     -0.0072,\n",
       "             -0.0064,      0.0094,      0.0190,      0.0016,      0.0038,\n",
       "              0.0147,      0.0218,     -0.0137,     -0.0182,      0.0246,\n",
       "             -0.0072,     -0.0171,     -0.0141,      0.0121,      0.0106,\n",
       "             -0.0352,      0.0320,     -0.0266,     -0.0035,      0.0234,\n",
       "              0.0019,     -0.0303,     -0.0235,     -0.0101,      0.0056,\n",
       "              0.0044,     -0.0094,     -0.0017,     -0.0136,      0.0311,\n",
       "             -0.0151,     -0.0054,      0.0098,     -0.0254,     -0.0288,\n",
       "             -0.0323,     -0.0112,     -0.0116,     -0.0107,      0.0241,\n",
       "              0.0125,     -0.0325,     -0.0182,     -0.0331,     -0.0001,\n",
       "              0.0225,      0.0198,     -0.0005,     -0.0211,      0.0078,\n",
       "              0.0011,     -0.0292,     -0.0075,     -0.0226,      0.0269,\n",
       "              0.0229,     -0.0283,     -0.0333,     -0.0101,      0.0107,\n",
       "              0.0142,     -0.0168,     -0.0186,     -0.0257,      0.0353,\n",
       "             -0.0337,      0.0324,      0.0113,      0.0205,      0.0078,\n",
       "              0.0063,     -0.0285,      0.0091,     -0.0314,     -0.0251,\n",
       "             -0.0135,      0.0057,     -0.0073,      0.0301,      0.0133,\n",
       "              0.0119,     -0.0183,      0.0122,      0.0322,     -0.0013,\n",
       "              0.0325,      0.0165,      0.0065,      0.0012,     -0.0005,\n",
       "             -0.0165,      0.0109,      0.0147,      0.0221,     -0.0067,\n",
       "             -0.0167,     -0.0047,      0.0122,     -0.0130,     -0.0249,\n",
       "             -0.0293,     -0.0201,     -0.0116,     -0.0061,      0.0223,\n",
       "              0.0121,      0.0353,     -0.0141,     -0.0127,      0.0176,\n",
       "              0.0109,      0.0109,      0.0027,      0.0072,      0.0019,\n",
       "             -0.0259,      0.0297,      0.0083,      0.0189,      0.0288,\n",
       "              0.0011,      0.0198,      0.0279,      0.0273,      0.0351,\n",
       "             -0.0041,     -0.0356,      0.0127,      0.0132,      0.0080,\n",
       "             -0.0226,      0.0260,     -0.0110,     -0.0133,     -0.0197,\n",
       "              0.0093,      0.0057,      0.0189,     -0.0139,     -0.0243,\n",
       "              0.0127,      0.0115,     -0.0074,      0.0042,     -0.0290,\n",
       "             -0.0196,     -0.0023,      0.0055,     -0.0173,      0.0223,\n",
       "             -0.0126,      0.0099,      0.0342,     -0.0012,     -0.0121,\n",
       "             -0.0213,      0.0148,     -0.0020,      0.0304,     -0.0137,\n",
       "             -0.0125,      0.0244,     -0.0332,     -0.0065,     -0.0116,\n",
       "             -0.0037,     -0.0223,      0.0121,     -0.0358,      0.0351,\n",
       "              0.0346,      0.0078,     -0.0102,      0.0314,      0.0011,\n",
       "              0.0341,     -0.0265,     -0.0089,     -0.0197,     -0.0199,\n",
       "              0.0327,      0.0101,      0.0165,     -0.0104,      0.0330,\n",
       "             -0.0286,      0.0090,     -0.0100,      0.0106,     -0.0127,\n",
       "              0.0325,     -0.0259,      0.0104,     -0.0346,     -0.0037,\n",
       "             -0.0088,      0.0250,      0.0106,      0.0059,      0.0078,\n",
       "              0.0125,      0.0210,     -0.0349,     -0.0004,     -0.0024,\n",
       "             -0.0024,     -0.0180,     -0.0304,      0.0054,      0.0189,\n",
       "             -0.0147,      0.0269,     -0.0182,     -0.0041,     -0.0113,\n",
       "             -0.0114,      0.0013,     -0.0069,      0.0049,     -0.0099,\n",
       "              0.0063,      0.0072,      0.0185,     -0.0021,     -0.0176,\n",
       "              0.0054,      0.0011,     -0.0361,      0.0095,      0.0091,\n",
       "              0.0183,      0.0055,     -0.0079,     -0.0343,     -0.0144,\n",
       "             -0.0059,     -0.0305,      0.0161,     -0.0091,     -0.0313,\n",
       "             -0.0001,     -0.0257,     -0.0289,      0.0304,     -0.0109,\n",
       "              0.0225,      0.0188,      0.0326,      0.0148,     -0.0190,\n",
       "             -0.0330,      0.0209,      0.0076,      0.0249,     -0.0210,\n",
       "             -0.0149,      0.0346,      0.0129,      0.0020,     -0.0197,\n",
       "              0.0060,     -0.0034,     -0.0219,      0.0195,     -0.0359,\n",
       "             -0.0008,     -0.0339,      0.0253,      0.0075,     -0.0227,\n",
       "             -0.0092,      0.0315,      0.0345,      0.0150,      0.0357,\n",
       "              0.0107,      0.0265,      0.0147,     -0.0358,      0.0035,\n",
       "              0.0182,     -0.0309,     -0.0250,     -0.0042,      0.0262,\n",
       "             -0.0210,     -0.0260,     -0.0084,      0.0330,     -0.0072,\n",
       "              0.0339,      0.0222,      0.0193,     -0.0297,      0.0094,\n",
       "             -0.0008,     -0.0135,     -0.0045,      0.0204,     -0.0321,\n",
       "             -0.0278,      0.0292,     -0.0129,     -0.0225,      0.0234,\n",
       "              0.0340,     -0.0292,     -0.0104,     -0.0316,      0.0156,\n",
       "              0.0071,     -0.0177,     -0.0210,     -0.0292,     -0.0200,\n",
       "              0.0324,     -0.0246,     -0.0180,     -0.0294,      0.0349,\n",
       "             -0.0242,     -0.0023,      0.0146,     -0.0249,     -0.0051,\n",
       "             -0.0017,     -0.0269,     -0.0105,     -0.0030,     -0.0092,\n",
       "              0.0266,      0.0055,     -0.0294,      0.0185,      0.0154,\n",
       "              0.0017,      0.0129,     -0.0027,     -0.0032,     -0.0043,\n",
       "             -0.0146,      0.0141,      0.0176,     -0.0297,      0.0091,\n",
       "             -0.0213,     -0.0312,     -0.0272,      0.0211,     -0.0358,\n",
       "             -0.0151,      0.0143,     -0.0342,     -0.0221,     -0.0284,\n",
       "             -0.0159,     -0.0145,     -0.0320,      0.0268,     -0.0333,\n",
       "              0.0263,     -0.0037,     -0.0233,      0.0230,     -0.0018,\n",
       "             -0.0187,      0.0116,     -0.0331,      0.0326,     -0.0116,\n",
       "             -0.0117,     -0.0053,      0.0312,     -0.0029,     -0.0156,\n",
       "              0.0254,      0.0259,      0.0227,     -0.0239,     -0.0272,\n",
       "              0.0317,      0.0080,      0.0219,     -0.0263,      0.0104,\n",
       "              0.0167,      0.0321,     -0.0237,     -0.0252,     -0.0058,\n",
       "              0.0235,     -0.0012,     -0.0314,     -0.0201,     -0.0304,\n",
       "             -0.0100,     -0.0269,     -0.0278,     -0.0046,      0.0250,\n",
       "             -0.0137,     -0.0170,      0.0356,     -0.0231,     -0.0189,\n",
       "              0.0049,      0.0133,     -0.0335,     -0.0100,      0.0026,\n",
       "              0.0350,     -0.0332,     -0.0079,      0.0005,     -0.0351,\n",
       "              0.0003,     -0.0224,     -0.0254,     -0.0102,     -0.0039,\n",
       "             -0.0231,     -0.0185,     -0.0324,     -0.0050,      0.0094,\n",
       "             -0.0255,      0.0252,      0.0071,      0.0155,      0.0301,\n",
       "              0.0253,      0.0191,     -0.0064,     -0.0026,      0.0037,\n",
       "             -0.0073,      0.0224,     -0.0212,     -0.0186,      0.0223,\n",
       "             -0.0178,     -0.0022,      0.0177,     -0.0306,     -0.0105,\n",
       "             -0.0073,     -0.0026,      0.0086,      0.0090,      0.0093,\n",
       "              0.0050,      0.0176,      0.0016,      0.0329,      0.0325,\n",
       "              0.0277,     -0.0258,     -0.0182,     -0.0127,     -0.0340,\n",
       "              0.0260,     -0.0215,      0.0164,     -0.0334,      0.0302,\n",
       "             -0.0285,     -0.0179,     -0.0242,     -0.0310,     -0.0352,\n",
       "             -0.0023,      0.0089,     -0.0189,      0.0208,     -0.0094,\n",
       "              0.0012,     -0.0288,      0.0251,      0.0155,     -0.0104,\n",
       "              0.0178,     -0.0308,     -0.0001,      0.0351,      0.0202,\n",
       "              0.0165,     -0.0191,     -0.0083,      0.0132,     -0.0330,\n",
       "              0.0151,     -0.0329,      0.0342,      0.0353,     -0.0359,\n",
       "              0.0069,      0.0062,     -0.0139,      0.0357,      0.0078,\n",
       "             -0.0275,      0.0127,      0.0028,     -0.0316,      0.0213,\n",
       "              0.0185,      0.0269,     -0.0339,     -0.0144,      0.0260,\n",
       "             -0.0284,     -0.0214,     -0.0071,      0.0100,      0.0138,\n",
       "              0.0124,      0.0128,     -0.0067,      0.0301,     -0.0271,\n",
       "              0.0089,     -0.0240,     -0.0144,      0.0297,     -0.0027,\n",
       "              0.0166,      0.0051,     -0.0066,      0.0000,     -0.0214,\n",
       "              0.0109,     -0.0277,      0.0191,     -0.0026,      0.0158,\n",
       "             -0.0355,     -0.0325,     -0.0303,     -0.0032,      0.0164,\n",
       "             -0.0293,     -0.0326,     -0.0187,     -0.0234,     -0.0150,\n",
       "              0.0257,      0.0346,      0.0231,     -0.0308,      0.0342,\n",
       "              0.0299,      0.0262,      0.0264,     -0.0348,      0.0184,\n",
       "             -0.0201,      0.0154,      0.0133,      0.0225,      0.0320,\n",
       "              0.0255,     -0.0059,     -0.0059,     -0.0021,     -0.0136,\n",
       "              0.0065,     -0.0130,     -0.0029,      0.0175,      0.0100,\n",
       "             -0.0356,     -0.0267,     -0.0072,      0.0246,     -0.0210,\n",
       "             -0.0361,      0.0116,     -0.0223,      0.0128,      0.0154,\n",
       "              0.0132,     -0.0231,     -0.0119,     -0.0209,     -0.0292,\n",
       "              0.0131,     -0.0062,     -0.0088,      0.0079,     -0.0215,\n",
       "             -0.0257,     -0.0338,      0.0086,     -0.0118,      0.0273,\n",
       "             -0.0256,      0.0328,      0.0169], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0336, -0.0036, -0.0128,  ..., -0.0312, -0.0294,  0.0130],\n",
       "         [ 0.0216,  0.0181, -0.0336,  ...,  0.0205, -0.0280, -0.0001],\n",
       "         [-0.0146,  0.0315,  0.0201,  ..., -0.0145, -0.0138,  0.0161],\n",
       "         ...,\n",
       "         [-0.0338, -0.0213,  0.0178,  ..., -0.0134, -0.0237,  0.0106],\n",
       "         [ 0.0072, -0.0168, -0.0250,  ..., -0.0247,  0.0098, -0.0216],\n",
       "         [-0.0037,  0.0189,  0.0167,  ..., -0.0288,  0.0056,  0.0142]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0298, -0.0201, -0.0020,  ..., -0.0304, -0.0224,  0.0110],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0091, -0.0130,  0.0076,  ...,  0.0103,  0.0011, -0.0177],\n",
       "         [-0.0040, -0.0043, -0.0099,  ..., -0.0101, -0.0006, -0.0110],\n",
       "         [ 0.0029,  0.0168, -0.0013,  ..., -0.0180,  0.0117, -0.0057],\n",
       "         ...,\n",
       "         [-0.0161, -0.0058,  0.0027,  ...,  0.0176,  0.0149, -0.0172],\n",
       "         [ 0.0018, -0.0177, -0.0113,  ...,  0.0085,  0.0084, -0.0146],\n",
       "         [-0.0149, -0.0097,  0.0016,  ..., -0.0145, -0.0039, -0.0148]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([     0.0002,      0.0050,     -0.0028,     -0.0078,      0.0120,\n",
       "             -0.0133,     -0.0157,      0.0007,      0.0029,      0.0048,\n",
       "             -0.0080,     -0.0083,      0.0177,      0.0103,     -0.0104,\n",
       "             -0.0056,     -0.0145,     -0.0034,      0.0007,      0.0108,\n",
       "              0.0155,      0.0115,     -0.0120,     -0.0167,      0.0086,\n",
       "             -0.0023,      0.0113,     -0.0108,     -0.0057,     -0.0008,\n",
       "             -0.0157,      0.0056,      0.0172,     -0.0053,      0.0001,\n",
       "              0.0111,     -0.0094,     -0.0071,     -0.0005,      0.0062,\n",
       "             -0.0131,      0.0055,      0.0021,      0.0034,      0.0093,\n",
       "              0.0154,      0.0009,     -0.0064,      0.0001,      0.0093,\n",
       "             -0.0052,     -0.0017,     -0.0133,      0.0092,      0.0014,\n",
       "             -0.0062,      0.0084,     -0.0037,      0.0040,      0.0069,\n",
       "             -0.0103,      0.0130,      0.0101,      0.0111,     -0.0089,\n",
       "              0.0115,      0.0137,     -0.0062,     -0.0170,      0.0134,\n",
       "             -0.0007,     -0.0049,      0.0163,      0.0101,      0.0173,\n",
       "             -0.0060,      0.0007,      0.0051,      0.0055,     -0.0119,\n",
       "             -0.0169,      0.0119,      0.0036,      0.0114,      0.0001,\n",
       "             -0.0130,     -0.0146,     -0.0057,     -0.0115,     -0.0017,\n",
       "              0.0046,      0.0088,     -0.0051,      0.0161,      0.0144,\n",
       "              0.0069,      0.0123,      0.0008,     -0.0054,      0.0005,\n",
       "             -0.0030,     -0.0103,      0.0022,      0.0112,      0.0018,\n",
       "              0.0113,      0.0029,     -0.0048,     -0.0082,      0.0024,\n",
       "             -0.0068,      0.0037,     -0.0073,      0.0139,      0.0134,\n",
       "             -0.0105,      0.0178,      0.0106,     -0.0086,     -0.0095,\n",
       "              0.0039,      0.0051,     -0.0090,      0.0108,     -0.0009,\n",
       "             -0.0087,      0.0002,     -0.0139,      0.0053,     -0.0004,\n",
       "              0.0128,      0.0030,     -0.0002,     -0.0063,      0.0079,\n",
       "             -0.0031,     -0.0075,     -0.0116,      0.0136,     -0.0085,\n",
       "              0.0027,      0.0107,      0.0001,      0.0110,     -0.0054,\n",
       "              0.0096,     -0.0080,      0.0050,     -0.0100,     -0.0109,\n",
       "             -0.0041,      0.0044,     -0.0128,      0.0162,     -0.0099,\n",
       "             -0.0080,      0.0088,      0.0019,     -0.0119,     -0.0080,\n",
       "              0.0172,      0.0084,     -0.0153,     -0.0028,      0.0092,\n",
       "              0.0092,     -0.0132,      0.0144,      0.0063,      0.0180,\n",
       "              0.0154,     -0.0171,      0.0148,     -0.0130,      0.0033,\n",
       "              0.0063,     -0.0120,      0.0127,      0.0080,     -0.0001,\n",
       "              0.0037,      0.0145,      0.0109,      0.0010,     -0.0072,\n",
       "              0.0022,      0.0167,      0.0079,     -0.0123,      0.0112,\n",
       "             -0.0028,     -0.0163,      0.0172,      0.0048,     -0.0031,\n",
       "             -0.0110,      0.0048,     -0.0148,      0.0138,      0.0132,\n",
       "             -0.0148,     -0.0087,      0.0138,     -0.0051,     -0.0131,\n",
       "              0.0084,      0.0169,     -0.0082,     -0.0111,      0.0105,\n",
       "              0.0074,     -0.0018,     -0.0006,      0.0090,      0.0059,\n",
       "              0.0013,      0.0067,      0.0118,      0.0136,      0.0134,\n",
       "             -0.0105,      0.0082,      0.0068,      0.0124,      0.0115,\n",
       "              0.0170,     -0.0000,      0.0099,      0.0098,      0.0081,\n",
       "              0.0124,      0.0147,      0.0107,     -0.0117,      0.0161,\n",
       "              0.0099,      0.0042,      0.0106,     -0.0142,      0.0122,\n",
       "              0.0119,      0.0152,      0.0010,      0.0111,      0.0144,\n",
       "              0.0163,     -0.0134,      0.0089,      0.0073,     -0.0047,\n",
       "             -0.0039,      0.0122,      0.0132,     -0.0028,     -0.0108,\n",
       "             -0.0026,     -0.0112,     -0.0065,     -0.0047,      0.0177,\n",
       "             -0.0008,      0.0083,     -0.0154,     -0.0156,      0.0146,\n",
       "              0.0098,     -0.0008,      0.0001,      0.0129,      0.0154,\n",
       "             -0.0073,      0.0082,     -0.0086,     -0.0090,      0.0126,\n",
       "             -0.0047,      0.0108,     -0.0122,      0.0056,     -0.0150,\n",
       "             -0.0176,     -0.0100,     -0.0123,      0.0017,      0.0071,\n",
       "             -0.0089,      0.0169,      0.0079,      0.0038,      0.0018,\n",
       "             -0.0139,     -0.0179,      0.0127,      0.0121,     -0.0122,\n",
       "             -0.0163,     -0.0070,     -0.0061,      0.0164,      0.0136,\n",
       "             -0.0003,     -0.0057,     -0.0001,      0.0153,     -0.0134,\n",
       "              0.0160,     -0.0042,     -0.0016,      0.0017,     -0.0071,\n",
       "             -0.0152,     -0.0011,      0.0166,      0.0089,     -0.0045,\n",
       "              0.0013,      0.0070,      0.0101,      0.0175,     -0.0099,\n",
       "              0.0045,      0.0145,     -0.0002,     -0.0148,      0.0112,\n",
       "             -0.0131,     -0.0107,      0.0014,     -0.0006,     -0.0143,\n",
       "             -0.0028,      0.0055,     -0.0137,     -0.0082,      0.0039,\n",
       "             -0.0109,      0.0137,     -0.0102,      0.0009,     -0.0009,\n",
       "              0.0160,      0.0101,     -0.0035,      0.0156,      0.0115,\n",
       "              0.0113,     -0.0126,      0.0127,     -0.0169,      0.0046,\n",
       "             -0.0059,     -0.0056,      0.0172,      0.0037,     -0.0015,\n",
       "             -0.0032,      0.0173,      0.0123,     -0.0023,      0.0075,\n",
       "             -0.0110,      0.0083,      0.0118,      0.0077,     -0.0126,\n",
       "              0.0065,      0.0091,     -0.0180,      0.0163,     -0.0012,\n",
       "             -0.0155,      0.0097,     -0.0012,      0.0041,     -0.0024,\n",
       "             -0.0069,     -0.0133,      0.0144,      0.0160,      0.0167,\n",
       "              0.0131,      0.0019,     -0.0103,     -0.0089,     -0.0066,\n",
       "              0.0008,     -0.0046,      0.0043,     -0.0172,      0.0137,\n",
       "             -0.0033,      0.0123,     -0.0094,     -0.0161,      0.0098,\n",
       "             -0.0005,     -0.0030,     -0.0078,     -0.0056,     -0.0144,\n",
       "             -0.0010,     -0.0020,     -0.0033,      0.0109,     -0.0026,\n",
       "              0.0035,     -0.0169,     -0.0020,      0.0154,      0.0134,\n",
       "              0.0103,      0.0131,     -0.0047,      0.0048,      0.0063,\n",
       "              0.0066,     -0.0093,     -0.0091,     -0.0072,     -0.0019,\n",
       "             -0.0096,      0.0087,      0.0061,      0.0084,     -0.0120,\n",
       "             -0.0031,     -0.0144,      0.0151,     -0.0017,     -0.0101,\n",
       "             -0.0048,      0.0133,      0.0028,     -0.0133,      0.0023,\n",
       "              0.0161,      0.0124,     -0.0047,     -0.0133,      0.0107,\n",
       "              0.0073,      0.0155,      0.0065,     -0.0029,     -0.0105,\n",
       "              0.0167,      0.0005,      0.0030,      0.0180,     -0.0044,\n",
       "              0.0164,     -0.0010,      0.0142,     -0.0153,      0.0073,\n",
       "             -0.0150,     -0.0083,      0.0087,      0.0118,     -0.0017,\n",
       "             -0.0019,     -0.0033,      0.0071,     -0.0136,      0.0152,\n",
       "             -0.0079,      0.0150,     -0.0124,      0.0144,      0.0147,\n",
       "              0.0138,     -0.0146,      0.0150,     -0.0146,     -0.0033,\n",
       "             -0.0152,      0.0128,     -0.0050,     -0.0055,      0.0041,\n",
       "             -0.0023,      0.0099,      0.0039,     -0.0029,      0.0081,\n",
       "              0.0028,     -0.0030,     -0.0044,      0.0073,     -0.0143,\n",
       "             -0.0029,      0.0092,      0.0112,     -0.0176,     -0.0124,\n",
       "             -0.0006,     -0.0172,      0.0117,     -0.0122,     -0.0065,\n",
       "              0.0079,     -0.0118,     -0.0035,      0.0095,     -0.0055,\n",
       "             -0.0049,      0.0092,     -0.0021,      0.0026,     -0.0055,\n",
       "              0.0065,      0.0164,     -0.0121,     -0.0097,     -0.0094,\n",
       "              0.0007,     -0.0180,     -0.0004,      0.0097,     -0.0027,\n",
       "             -0.0160,     -0.0127,      0.0053,      0.0157,     -0.0123,\n",
       "              0.0067,     -0.0097,      0.0132,      0.0102,      0.0127,\n",
       "             -0.0080,     -0.0138,      0.0131,     -0.0101,     -0.0177,\n",
       "             -0.0117,     -0.0176,     -0.0044,     -0.0117,     -0.0051,\n",
       "             -0.0071,      0.0121,     -0.0129,     -0.0017,     -0.0006,\n",
       "             -0.0091,     -0.0135,      0.0147,      0.0103,      0.0133,\n",
       "             -0.0125,     -0.0132,      0.0158,      0.0159,      0.0116,\n",
       "              0.0117,     -0.0123,      0.0024,     -0.0102,     -0.0114,\n",
       "              0.0109,     -0.0079,      0.0071,      0.0094,     -0.0176,\n",
       "              0.0087,     -0.0143,      0.0006,      0.0162,     -0.0178,\n",
       "             -0.0029,      0.0066,      0.0024,      0.0142,      0.0157,\n",
       "             -0.0106,     -0.0137,      0.0043,      0.0086,     -0.0013,\n",
       "              0.0155,      0.0086,      0.0091,      0.0131,     -0.0073,\n",
       "              0.0036,      0.0052,      0.0110,     -0.0059,     -0.0152,\n",
       "              0.0146,     -0.0138,      0.0020,      0.0120,     -0.0003,\n",
       "              0.0014,      0.0060,      0.0082,     -0.0129,     -0.0106,\n",
       "              0.0136,     -0.0005,     -0.0089,     -0.0056,      0.0137,\n",
       "             -0.0062,      0.0004,      0.0169,     -0.0038,      0.0130,\n",
       "             -0.0025,     -0.0147,      0.0086,     -0.0107,      0.0051,\n",
       "              0.0006,      0.0038,      0.0056,     -0.0077,      0.0118,\n",
       "              0.0105,      0.0008,      0.0122,     -0.0035,      0.0066,\n",
       "             -0.0179,      0.0142,     -0.0110,      0.0113,     -0.0008,\n",
       "             -0.0175,     -0.0118,      0.0028,     -0.0172,     -0.0038,\n",
       "              0.0085,     -0.0168,     -0.0133,     -0.0005,     -0.0012,\n",
       "             -0.0120,     -0.0032,     -0.0146,     -0.0121,      0.0108,\n",
       "             -0.0014,     -0.0061,      0.0085,     -0.0075,      0.0133,\n",
       "              0.0001,      0.0168,      0.0111,      0.0143,      0.0114,\n",
       "              0.0070,      0.0176,      0.0014,     -0.0141,     -0.0107,\n",
       "             -0.0166,      0.0021,      0.0048,     -0.0169,     -0.0160,\n",
       "             -0.0133,     -0.0177,     -0.0059,     -0.0152,     -0.0012,\n",
       "              0.0082,      0.0150,      0.0167,      0.0083,      0.0150,\n",
       "             -0.0068,      0.0167,      0.0042,      0.0132,      0.0114,\n",
       "              0.0154,     -0.0097,      0.0137,      0.0064,      0.0145,\n",
       "              0.0019,     -0.0003,      0.0016,     -0.0103,     -0.0072,\n",
       "             -0.0175,      0.0152,      0.0112,      0.0047,      0.0040,\n",
       "             -0.0061,      0.0049,      0.0157,     -0.0081,      0.0027,\n",
       "             -0.0047,      0.0178,      0.0120,      0.0046,      0.0079,\n",
       "              0.0081,      0.0006,      0.0041,     -0.0049,      0.0045,\n",
       "             -0.0014,     -0.0100,     -0.0013,     -0.0069,      0.0128,\n",
       "             -0.0118,      0.0158,      0.0002,     -0.0004,     -0.0089,\n",
       "              0.0031,      0.0047,     -0.0009,      0.0177,     -0.0052,\n",
       "              0.0018,      0.0064,      0.0115,     -0.0174,     -0.0176,\n",
       "             -0.0139,     -0.0035,      0.0170,      0.0143,     -0.0152,\n",
       "             -0.0075,      0.0086,     -0.0110,     -0.0102,     -0.0035,\n",
       "             -0.0131,     -0.0060,     -0.0116,      0.0008,     -0.0056,\n",
       "              0.0005,      0.0176,      0.0124,     -0.0173,      0.0100,\n",
       "             -0.0095,      0.0018,     -0.0027,     -0.0141,      0.0163,\n",
       "             -0.0163,      0.0003,      0.0079,     -0.0175,     -0.0011,\n",
       "              0.0130,     -0.0068,      0.0098,     -0.0029,     -0.0179,\n",
       "             -0.0167,      0.0167,      0.0007], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0151, -0.0277, -0.0339,  ..., -0.0276,  0.0181, -0.0308],\n",
       "         [ 0.0359, -0.0352,  0.0189,  ..., -0.0292, -0.0157, -0.0196],\n",
       "         [ 0.0010,  0.0265, -0.0135,  ...,  0.0311, -0.0235,  0.0288],\n",
       "         ...,\n",
       "         [ 0.0023,  0.0113,  0.0009,  ...,  0.0310, -0.0093,  0.0282],\n",
       "         [-0.0004, -0.0021,  0.0163,  ...,  0.0002,  0.0226,  0.0039],\n",
       "         [-0.0049, -0.0300, -0.0137,  ..., -0.0193, -0.0361, -0.0099]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0154, -0.0066,  0.0188,  ...,  0.0124,  0.0092,  0.0001],\n",
       "         [ 0.0101,  0.0074, -0.0322,  ..., -0.0106, -0.0124,  0.0085],\n",
       "         [-0.0242, -0.0106,  0.0334,  ...,  0.0266, -0.0147, -0.0014],\n",
       "         ...,\n",
       "         [ 0.0088,  0.0354, -0.0253,  ...,  0.0290,  0.0173, -0.0311],\n",
       "         [-0.0076,  0.0197, -0.0273,  ..., -0.0123,  0.0277,  0.0246],\n",
       "         [ 0.0264,  0.0199,  0.0315,  ..., -0.0125,  0.0351,  0.0149]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[     0.0353,     -0.0359,     -0.0191,  ...,     -0.0357,\n",
       "              -0.0298,     -0.0010],\n",
       "         [     0.0057,     -0.0275,     -0.0326,  ...,      0.0133,\n",
       "               0.0303,     -0.0294],\n",
       "         [     0.0130,      0.0341,      0.0076,  ...,     -0.0134,\n",
       "              -0.0085,      0.0018],\n",
       "         ...,\n",
       "         [     0.0086,     -0.0220,     -0.0317,  ...,     -0.0049,\n",
       "               0.0055,     -0.0186],\n",
       "         [     0.0337,      0.0058,     -0.0231,  ...,      0.0228,\n",
       "               0.0066,     -0.0034],\n",
       "         [     0.0000,      0.0144,      0.0061,  ...,     -0.0309,\n",
       "               0.0199,     -0.0000]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0218, -0.0206, -0.0117,  ..., -0.0315, -0.0066, -0.0159],\n",
       "         [ 0.0325,  0.0296,  0.0218,  ..., -0.0352,  0.0202, -0.0197],\n",
       "         [-0.0185,  0.0096,  0.0291,  ...,  0.0355, -0.0157,  0.0173],\n",
       "         ...,\n",
       "         [ 0.0318, -0.0116,  0.0011,  ..., -0.0088, -0.0191, -0.0164],\n",
       "         [ 0.0014, -0.0238,  0.0304,  ..., -0.0192,  0.0338, -0.0051],\n",
       "         [ 0.0026, -0.0069, -0.0118,  ..., -0.0169,  0.0208, -0.0168]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([    -0.0152,     -0.0347,     -0.0290,     -0.0319,      0.0043,\n",
       "              0.0339,      0.0343,     -0.0089,      0.0170,      0.0171,\n",
       "             -0.0112,      0.0082,     -0.0324,     -0.0152,      0.0004,\n",
       "             -0.0214,      0.0233,     -0.0073,      0.0092,      0.0261,\n",
       "             -0.0213,     -0.0324,     -0.0042,      0.0205,     -0.0263,\n",
       "             -0.0251,      0.0230,      0.0146,      0.0013,     -0.0015,\n",
       "             -0.0054,     -0.0358,      0.0264,      0.0215,      0.0198,\n",
       "              0.0146,      0.0144,     -0.0249,     -0.0212,     -0.0239,\n",
       "             -0.0358,     -0.0088,     -0.0255,     -0.0045,     -0.0047,\n",
       "              0.0052,     -0.0080,      0.0229,      0.0167,     -0.0079,\n",
       "             -0.0185,     -0.0319,      0.0224,     -0.0042,     -0.0190,\n",
       "             -0.0013,      0.0241,     -0.0069,      0.0080,      0.0152,\n",
       "             -0.0157,      0.0067,      0.0053,     -0.0152,     -0.0360,\n",
       "             -0.0227,      0.0244,      0.0121,      0.0052,     -0.0239,\n",
       "             -0.0261,     -0.0278,     -0.0291,      0.0250,     -0.0145,\n",
       "             -0.0157,      0.0164,     -0.0047,      0.0169,      0.0152,\n",
       "             -0.0070,      0.0061,      0.0140,      0.0118,     -0.0165,\n",
       "             -0.0129,     -0.0132,      0.0341,      0.0173,     -0.0335,\n",
       "             -0.0115,     -0.0148,      0.0357,     -0.0062,     -0.0109,\n",
       "             -0.0098,     -0.0026,     -0.0270,     -0.0220,      0.0213,\n",
       "              0.0034,     -0.0013,     -0.0200,     -0.0219,     -0.0305,\n",
       "              0.0297,      0.0111,     -0.0296,     -0.0151,     -0.0120,\n",
       "              0.0257,     -0.0162,     -0.0230,      0.0100,     -0.0202,\n",
       "             -0.0180,      0.0017,     -0.0037,     -0.0110,     -0.0203,\n",
       "             -0.0118,     -0.0305,      0.0134,      0.0292,      0.0100,\n",
       "             -0.0361,      0.0182,     -0.0011,      0.0230,      0.0113,\n",
       "              0.0337,     -0.0039,      0.0352,     -0.0335,     -0.0358,\n",
       "             -0.0234,      0.0286,     -0.0220,      0.0099,     -0.0083,\n",
       "             -0.0065,     -0.0327,     -0.0029,     -0.0194,      0.0174,\n",
       "             -0.0034,     -0.0102,     -0.0325,      0.0011,      0.0057,\n",
       "             -0.0188,     -0.0022,     -0.0310,      0.0336,     -0.0055,\n",
       "             -0.0100,      0.0169,     -0.0224,     -0.0327,     -0.0347,\n",
       "              0.0142,     -0.0004,      0.0277,      0.0045,      0.0211,\n",
       "              0.0333,     -0.0138,     -0.0175,      0.0197,      0.0122,\n",
       "              0.0110,      0.0312,     -0.0063,      0.0075,     -0.0113,\n",
       "              0.0323,     -0.0056,     -0.0257,     -0.0019,     -0.0272,\n",
       "             -0.0106,      0.0328,      0.0109,      0.0319,     -0.0127,\n",
       "             -0.0056,     -0.0011,      0.0196,      0.0148,     -0.0001,\n",
       "              0.0082,      0.0328,      0.0176,      0.0111,      0.0249,\n",
       "              0.0088,      0.0315,      0.0297,      0.0154,      0.0010,\n",
       "              0.0300,     -0.0001,      0.0351,     -0.0110,      0.0323,\n",
       "             -0.0238,     -0.0131,      0.0118,      0.0030,     -0.0210,\n",
       "              0.0187,      0.0243,     -0.0338,      0.0036,      0.0175,\n",
       "              0.0095,      0.0196,      0.0228,      0.0312,     -0.0127,\n",
       "              0.0239,     -0.0095,      0.0226,     -0.0265,      0.0007,\n",
       "             -0.0237,      0.0060,     -0.0150,     -0.0336,     -0.0099,\n",
       "              0.0008,     -0.0032,      0.0285,      0.0106,      0.0246,\n",
       "             -0.0291,      0.0084,      0.0074,     -0.0107,      0.0236,\n",
       "              0.0296,      0.0069,      0.0121,     -0.0174,      0.0218,\n",
       "              0.0076,     -0.0037,     -0.0206,      0.0058,      0.0208,\n",
       "              0.0278,      0.0036,      0.0094,     -0.0178,      0.0190,\n",
       "             -0.0215,     -0.0153,     -0.0022,      0.0301,     -0.0059,\n",
       "              0.0227,     -0.0345,     -0.0275,     -0.0345,     -0.0249,\n",
       "             -0.0106,     -0.0126,     -0.0219,     -0.0226,      0.0052,\n",
       "              0.0112,     -0.0029,      0.0013,      0.0080,     -0.0324,\n",
       "              0.0161,     -0.0110,     -0.0278,      0.0249,     -0.0301,\n",
       "             -0.0037,     -0.0112,      0.0296,      0.0069,      0.0121,\n",
       "              0.0230,     -0.0216,      0.0059,     -0.0222,      0.0071,\n",
       "              0.0190,     -0.0205,      0.0075,      0.0152,     -0.0178,\n",
       "              0.0282,      0.0250,      0.0050,      0.0307,      0.0253,\n",
       "              0.0232,      0.0285,     -0.0002,     -0.0226,     -0.0171,\n",
       "              0.0032,     -0.0089,      0.0332,     -0.0233,      0.0241,\n",
       "              0.0015,     -0.0225,      0.0333,     -0.0333,      0.0054,\n",
       "              0.0126,      0.0113,     -0.0026,     -0.0296,     -0.0105,\n",
       "              0.0076,      0.0116,     -0.0313,     -0.0226,      0.0103,\n",
       "              0.0076,     -0.0241,     -0.0112,      0.0126,     -0.0267,\n",
       "             -0.0015,     -0.0062,     -0.0018,      0.0221,     -0.0238,\n",
       "             -0.0056,      0.0264,      0.0072,      0.0258,      0.0005,\n",
       "             -0.0305,      0.0027,      0.0070,      0.0032,     -0.0098,\n",
       "              0.0031,     -0.0143,     -0.0361,      0.0186,     -0.0180,\n",
       "              0.0208,      0.0265,      0.0066,     -0.0316,      0.0013,\n",
       "             -0.0195,      0.0074,      0.0018,     -0.0327,     -0.0325,\n",
       "              0.0016,      0.0023,     -0.0230,      0.0058,     -0.0212,\n",
       "              0.0122,      0.0165,     -0.0045,     -0.0134,     -0.0220,\n",
       "             -0.0337,      0.0014,     -0.0161,     -0.0075,      0.0077,\n",
       "              0.0341,      0.0237,     -0.0203,      0.0072,      0.0086,\n",
       "             -0.0187,     -0.0354,     -0.0232,     -0.0035,      0.0030,\n",
       "             -0.0308,     -0.0351,     -0.0200,     -0.0275,     -0.0061,\n",
       "              0.0264,      0.0072,      0.0051,     -0.0107,     -0.0158,\n",
       "              0.0350,      0.0107,      0.0111,      0.0076,     -0.0205,\n",
       "              0.0135,     -0.0095,      0.0119,     -0.0033,      0.0260,\n",
       "             -0.0296,     -0.0331,     -0.0128,     -0.0088,      0.0119,\n",
       "              0.0075,     -0.0321,      0.0354,     -0.0240,     -0.0013,\n",
       "             -0.0193,      0.0258,     -0.0012,      0.0080,     -0.0031,\n",
       "              0.0297,      0.0212,     -0.0047,      0.0185,     -0.0254,\n",
       "             -0.0328,      0.0282,     -0.0066,     -0.0331,     -0.0145,\n",
       "              0.0239,      0.0249,     -0.0117,      0.0002,     -0.0006,\n",
       "             -0.0316,      0.0139,      0.0331,      0.0323,     -0.0217,\n",
       "              0.0309,      0.0293,     -0.0233,      0.0037,     -0.0293,\n",
       "              0.0326,      0.0111,     -0.0173,     -0.0308,     -0.0012,\n",
       "             -0.0337,      0.0352,     -0.0168,      0.0108,     -0.0266,\n",
       "              0.0094,     -0.0035,     -0.0228,      0.0181,     -0.0132,\n",
       "              0.0035,      0.0335,     -0.0178,     -0.0029,      0.0169,\n",
       "             -0.0168,     -0.0059,     -0.0032,      0.0217,     -0.0070,\n",
       "              0.0337,     -0.0115,     -0.0322,     -0.0185,     -0.0156,\n",
       "              0.0139,     -0.0176,     -0.0203,     -0.0254,      0.0023,\n",
       "             -0.0066,     -0.0213,      0.0139,      0.0094,     -0.0089,\n",
       "             -0.0128,      0.0321,     -0.0322,      0.0155,     -0.0019,\n",
       "              0.0355,     -0.0047,      0.0256,      0.0292,      0.0066,\n",
       "             -0.0360,      0.0093,     -0.0115,     -0.0170,      0.0097,\n",
       "             -0.0335,      0.0232,     -0.0261,     -0.0311,     -0.0131,\n",
       "              0.0076,     -0.0234,      0.0176,      0.0262,     -0.0037,\n",
       "             -0.0028,      0.0037,      0.0293,     -0.0348,      0.0129,\n",
       "              0.0006,      0.0180,      0.0078,      0.0164,     -0.0069,\n",
       "              0.0268,      0.0312,      0.0129,      0.0254,      0.0141,\n",
       "             -0.0344,     -0.0260,      0.0279,     -0.0169,      0.0144,\n",
       "              0.0275,     -0.0357,     -0.0206,      0.0222,      0.0141,\n",
       "             -0.0151,     -0.0337,      0.0204,     -0.0303,     -0.0097,\n",
       "             -0.0110,     -0.0196,      0.0321,     -0.0218,      0.0111,\n",
       "              0.0131,     -0.0030,     -0.0218,     -0.0116,     -0.0145,\n",
       "             -0.0008,      0.0020,      0.0011,     -0.0044,      0.0026,\n",
       "              0.0053,      0.0016,     -0.0164,      0.0272,      0.0106,\n",
       "              0.0250,     -0.0296,      0.0020,      0.0132,      0.0234,\n",
       "             -0.0211,      0.0181,     -0.0015,     -0.0334,     -0.0112,\n",
       "              0.0298,     -0.0330,      0.0127,     -0.0178,     -0.0169,\n",
       "              0.0278,     -0.0063,      0.0060,      0.0314,      0.0150,\n",
       "             -0.0226,     -0.0219,      0.0223,     -0.0299,     -0.0001,\n",
       "              0.0130,      0.0095,      0.0251,      0.0286,      0.0022,\n",
       "             -0.0036,     -0.0227,     -0.0307,      0.0021,      0.0359,\n",
       "             -0.0222,     -0.0301,      0.0041,      0.0060,     -0.0348,\n",
       "             -0.0297,      0.0154,      0.0092,      0.0276,     -0.0327,\n",
       "              0.0072,      0.0360,     -0.0203,     -0.0318,     -0.0151,\n",
       "             -0.0228,     -0.0336,     -0.0028,      0.0207,      0.0112,\n",
       "              0.0029,     -0.0196,     -0.0224,      0.0323,      0.0054,\n",
       "              0.0022,     -0.0080,     -0.0358,      0.0264,     -0.0278,\n",
       "             -0.0079,      0.0167,     -0.0172,      0.0268,     -0.0300,\n",
       "             -0.0268,     -0.0307,      0.0175,     -0.0163,     -0.0246,\n",
       "             -0.0340,      0.0278,      0.0234,      0.0218,     -0.0110,\n",
       "             -0.0300,     -0.0097,      0.0116,     -0.0173,      0.0057,\n",
       "             -0.0053,      0.0148,      0.0138,      0.0132,      0.0033,\n",
       "              0.0120,      0.0248,     -0.0360,     -0.0276,     -0.0127,\n",
       "              0.0130,      0.0029,     -0.0330,     -0.0307,      0.0290,\n",
       "              0.0076,     -0.0230,     -0.0278,     -0.0251,     -0.0082,\n",
       "              0.0177,     -0.0256,      0.0178,      0.0194,      0.0293,\n",
       "              0.0308,     -0.0257,      0.0299,     -0.0316,      0.0044,\n",
       "              0.0345,      0.0066,     -0.0291,      0.0038,      0.0056,\n",
       "              0.0000,     -0.0234,      0.0215,      0.0228,     -0.0253,\n",
       "             -0.0336,      0.0356,      0.0274,      0.0239,      0.0157,\n",
       "              0.0120,     -0.0256,      0.0090,     -0.0267,      0.0219,\n",
       "             -0.0128,      0.0134,     -0.0273,     -0.0270,     -0.0092,\n",
       "              0.0038,      0.0235,      0.0089,      0.0108,     -0.0334,\n",
       "             -0.0224,      0.0197,      0.0083,     -0.0059,      0.0340,\n",
       "              0.0298,     -0.0297,      0.0108,      0.0331,     -0.0237,\n",
       "             -0.0181,     -0.0044,      0.0286,      0.0159,      0.0146,\n",
       "             -0.0059,      0.0054,     -0.0170,     -0.0241,      0.0309,\n",
       "             -0.0273,      0.0187,      0.0214,      0.0051,      0.0091,\n",
       "              0.0084,     -0.0056,     -0.0012,     -0.0342,      0.0006,\n",
       "             -0.0249,      0.0166,      0.0312,      0.0059,      0.0316,\n",
       "             -0.0333,      0.0189,     -0.0060,      0.0225,     -0.0039,\n",
       "              0.0045,      0.0196,      0.0049,     -0.0189,     -0.0183,\n",
       "              0.0195,      0.0167,      0.0093,      0.0088,      0.0200,\n",
       "              0.0182,      0.0116,      0.0211,     -0.0124,     -0.0206,\n",
       "              0.0353,      0.0024,      0.0315,     -0.0305,      0.0310,\n",
       "             -0.0020,     -0.0230,     -0.0114], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0272,  0.0215,  0.0222,  ..., -0.0011, -0.0191, -0.0006],\n",
       "         [-0.0100, -0.0352, -0.0119,  ..., -0.0031,  0.0108, -0.0344],\n",
       "         [-0.0178, -0.0173, -0.0102,  ..., -0.0246,  0.0272, -0.0276],\n",
       "         ...,\n",
       "         [-0.0195,  0.0007,  0.0352,  ..., -0.0088, -0.0189,  0.0130],\n",
       "         [ 0.0139, -0.0354,  0.0024,  ...,  0.0181,  0.0151, -0.0184],\n",
       "         [-0.0098,  0.0226,  0.0153,  ...,  0.0278, -0.0337, -0.0010]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0041,  0.0007, -0.0314,  ...,  0.0068, -0.0331, -0.0212],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[    -0.0051,     -0.0160,     -0.0019,  ...,     -0.0101,\n",
       "              -0.0045,     -0.0030],\n",
       "         [     0.0114,     -0.0105,      0.0168,  ...,     -0.0029,\n",
       "               0.0035,      0.0059],\n",
       "         [    -0.0114,     -0.0017,     -0.0065,  ...,     -0.0044,\n",
       "               0.0001,     -0.0147],\n",
       "         ...,\n",
       "         [     0.0061,      0.0118,      0.0062,  ...,      0.0043,\n",
       "               0.0155,      0.0152],\n",
       "         [    -0.0143,      0.0129,     -0.0059,  ...,      0.0152,\n",
       "               0.0011,     -0.0052],\n",
       "         [    -0.0087,      0.0144,     -0.0037,  ...,      0.0083,\n",
       "              -0.0116,      0.0071]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([     0.0077,     -0.0080,     -0.0103,      0.0084,      0.0035,\n",
       "             -0.0142,     -0.0114,      0.0095,      0.0107,      0.0040,\n",
       "             -0.0018,     -0.0038,     -0.0097,     -0.0106,     -0.0164,\n",
       "             -0.0002,     -0.0069,      0.0001,      0.0168,     -0.0098,\n",
       "             -0.0143,     -0.0059,     -0.0047,     -0.0106,      0.0066,\n",
       "              0.0101,      0.0007,      0.0084,     -0.0004,     -0.0152,\n",
       "             -0.0128,     -0.0075,      0.0120,     -0.0018,     -0.0096,\n",
       "              0.0104,     -0.0170,      0.0167,      0.0009,      0.0156,\n",
       "              0.0153,      0.0006,     -0.0014,     -0.0011,      0.0076,\n",
       "              0.0013,     -0.0043,      0.0049,      0.0104,     -0.0166,\n",
       "              0.0176,      0.0143,     -0.0160,     -0.0152,     -0.0085,\n",
       "             -0.0073,      0.0016,     -0.0066,      0.0158,      0.0006,\n",
       "             -0.0001,     -0.0125,     -0.0091,      0.0018,     -0.0038,\n",
       "             -0.0133,     -0.0104,     -0.0135,      0.0046,      0.0178,\n",
       "              0.0122,      0.0159,      0.0149,      0.0085,      0.0006,\n",
       "              0.0148,     -0.0019,      0.0103,      0.0055,      0.0161,\n",
       "              0.0088,     -0.0100,      0.0134,      0.0178,      0.0131,\n",
       "             -0.0102,      0.0143,      0.0011,      0.0065,      0.0093,\n",
       "              0.0021,     -0.0138,     -0.0077,     -0.0093,      0.0101,\n",
       "              0.0039,     -0.0143,      0.0156,      0.0004,      0.0093,\n",
       "             -0.0016,      0.0015,     -0.0007,      0.0040,     -0.0071,\n",
       "              0.0115,     -0.0064,      0.0002,      0.0011,      0.0004,\n",
       "             -0.0110,     -0.0103,     -0.0047,     -0.0128,      0.0028,\n",
       "             -0.0085,      0.0081,     -0.0066,      0.0049,      0.0148,\n",
       "             -0.0021,      0.0048,     -0.0144,     -0.0070,      0.0149,\n",
       "              0.0030,      0.0081,     -0.0050,      0.0107,      0.0124,\n",
       "             -0.0086,      0.0067,     -0.0086,      0.0098,      0.0173,\n",
       "              0.0029,      0.0034,     -0.0101,      0.0070,      0.0155,\n",
       "              0.0036,     -0.0118,      0.0075,     -0.0064,     -0.0013,\n",
       "              0.0114,     -0.0137,     -0.0088,     -0.0044,      0.0162,\n",
       "             -0.0163,      0.0137,     -0.0003,      0.0059,      0.0023,\n",
       "              0.0176,      0.0050,     -0.0005,      0.0177,      0.0172,\n",
       "             -0.0154,      0.0164,      0.0025,      0.0131,     -0.0030,\n",
       "              0.0007,      0.0126,     -0.0149,     -0.0103,      0.0079,\n",
       "             -0.0052,      0.0146,      0.0172,      0.0067,     -0.0029,\n",
       "              0.0147,     -0.0131,     -0.0009,      0.0068,      0.0105,\n",
       "             -0.0022,     -0.0127,     -0.0163,      0.0159,     -0.0156,\n",
       "              0.0121,     -0.0090,     -0.0109,     -0.0055,     -0.0168,\n",
       "             -0.0136,     -0.0056,     -0.0068,      0.0075,      0.0019,\n",
       "             -0.0136,      0.0016,      0.0061,      0.0151,      0.0047,\n",
       "              0.0009,      0.0078,      0.0012,      0.0068,      0.0012,\n",
       "             -0.0180,      0.0041,      0.0090,      0.0173,     -0.0147,\n",
       "             -0.0065,      0.0053,     -0.0154,      0.0177,      0.0107,\n",
       "             -0.0026,      0.0110,      0.0074,     -0.0016,     -0.0180,\n",
       "             -0.0167,      0.0144,      0.0029,      0.0099,      0.0071,\n",
       "              0.0034,     -0.0086,      0.0057,      0.0002,     -0.0001,\n",
       "             -0.0049,      0.0040,     -0.0143,      0.0038,     -0.0114,\n",
       "             -0.0110,     -0.0044,     -0.0098,      0.0157,      0.0075,\n",
       "             -0.0084,     -0.0149,      0.0097,      0.0049,      0.0040,\n",
       "              0.0005,     -0.0062,      0.0037,     -0.0067,      0.0124,\n",
       "              0.0010,      0.0028,      0.0063,     -0.0036,     -0.0163,\n",
       "              0.0082,      0.0168,     -0.0093,     -0.0056,      0.0172,\n",
       "             -0.0129,      0.0120,      0.0062,     -0.0116,     -0.0118,\n",
       "             -0.0068,      0.0094,      0.0171,      0.0005,     -0.0158,\n",
       "             -0.0103,      0.0169,     -0.0178,     -0.0156,     -0.0046,\n",
       "              0.0092,     -0.0138,      0.0012,     -0.0107,     -0.0086,\n",
       "             -0.0011,     -0.0163,      0.0130,     -0.0028,     -0.0001,\n",
       "             -0.0107,      0.0173,     -0.0155,     -0.0166,      0.0090,\n",
       "             -0.0071,     -0.0057,      0.0127,     -0.0074,      0.0158,\n",
       "              0.0168,      0.0171,      0.0029,     -0.0109,     -0.0056,\n",
       "             -0.0021,     -0.0053,      0.0137,      0.0093,      0.0079,\n",
       "              0.0076,     -0.0010,      0.0134,     -0.0030,      0.0021,\n",
       "              0.0053,     -0.0018,     -0.0027,      0.0138,      0.0085,\n",
       "             -0.0100,     -0.0070,     -0.0077,      0.0145,     -0.0113,\n",
       "              0.0051,     -0.0082,      0.0073,     -0.0066,     -0.0007,\n",
       "             -0.0063,     -0.0161,     -0.0154,      0.0100,     -0.0131,\n",
       "              0.0092,      0.0148,      0.0156,      0.0160,     -0.0099,\n",
       "              0.0177,      0.0036,      0.0053,      0.0076,      0.0053,\n",
       "              0.0076,      0.0142,      0.0115,      0.0146,     -0.0004,\n",
       "              0.0108,     -0.0048,      0.0085,     -0.0079,     -0.0084,\n",
       "              0.0047,      0.0171,      0.0065,     -0.0067,      0.0071,\n",
       "             -0.0061,      0.0147,      0.0134,     -0.0119,     -0.0010,\n",
       "              0.0170,     -0.0084,     -0.0093,     -0.0041,     -0.0106,\n",
       "             -0.0085,     -0.0128,      0.0125,     -0.0175,      0.0102,\n",
       "              0.0102,     -0.0165,      0.0120,     -0.0090,      0.0014,\n",
       "              0.0151,     -0.0021,      0.0128,      0.0041,     -0.0113,\n",
       "              0.0009,      0.0100,      0.0019,     -0.0046,     -0.0154,\n",
       "              0.0043,     -0.0158,      0.0070,     -0.0048,     -0.0075,\n",
       "             -0.0074,      0.0044,     -0.0020,     -0.0058,      0.0024,\n",
       "              0.0178,      0.0043,     -0.0168,      0.0162,      0.0149,\n",
       "              0.0167,      0.0059,     -0.0016,     -0.0158,     -0.0106,\n",
       "             -0.0065,     -0.0021,     -0.0110,      0.0086,     -0.0073,\n",
       "             -0.0114,     -0.0146,      0.0158,      0.0094,      0.0160,\n",
       "             -0.0102,      0.0047,      0.0092,     -0.0071,      0.0025,\n",
       "              0.0169,     -0.0163,     -0.0086,      0.0002,     -0.0137,\n",
       "              0.0026,     -0.0099,     -0.0114,      0.0149,     -0.0147,\n",
       "             -0.0160,     -0.0165,      0.0164,     -0.0113,      0.0091,\n",
       "              0.0095,      0.0040,     -0.0013,      0.0172,     -0.0008,\n",
       "             -0.0054,      0.0141,      0.0055,      0.0080,      0.0035,\n",
       "             -0.0050,      0.0099,     -0.0170,     -0.0175,     -0.0152,\n",
       "              0.0028,     -0.0021,     -0.0172,     -0.0099,     -0.0024,\n",
       "              0.0058,     -0.0052,     -0.0101,     -0.0023,     -0.0127,\n",
       "             -0.0062,     -0.0131,      0.0139,      0.0044,      0.0054,\n",
       "              0.0112,      0.0157,      0.0106,      0.0126,      0.0046,\n",
       "              0.0032,      0.0028,      0.0152,      0.0160,      0.0157,\n",
       "              0.0067,     -0.0031,      0.0174,      0.0042,     -0.0035,\n",
       "              0.0117,      0.0150,     -0.0147,      0.0081,     -0.0176,\n",
       "              0.0095,      0.0162,      0.0007,      0.0018,      0.0036,\n",
       "             -0.0026,     -0.0084,      0.0013,      0.0097,      0.0096,\n",
       "              0.0067,      0.0118,     -0.0031,      0.0101,      0.0141,\n",
       "              0.0010,     -0.0145,      0.0131,     -0.0076,     -0.0166,\n",
       "             -0.0117,     -0.0115,     -0.0026,      0.0130,     -0.0159,\n",
       "             -0.0135,     -0.0135,     -0.0094,     -0.0155,      0.0052,\n",
       "             -0.0023,      0.0179,      0.0087,     -0.0121,      0.0121,\n",
       "              0.0009,      0.0043,     -0.0083,     -0.0121,      0.0126,\n",
       "              0.0127,     -0.0059,      0.0115,     -0.0150,     -0.0166,\n",
       "              0.0004,     -0.0174,      0.0025,     -0.0171,      0.0106,\n",
       "             -0.0171,      0.0088,      0.0030,      0.0086,      0.0026,\n",
       "              0.0129,     -0.0078,      0.0147,      0.0030,      0.0072,\n",
       "              0.0125,     -0.0068,      0.0115,     -0.0122,     -0.0146,\n",
       "              0.0014,     -0.0113,      0.0166,      0.0015,     -0.0104,\n",
       "             -0.0098,     -0.0114,      0.0146,      0.0012,      0.0033,\n",
       "             -0.0107,     -0.0129,     -0.0098,     -0.0017,      0.0076,\n",
       "              0.0173,      0.0077,     -0.0015,      0.0168,     -0.0088,\n",
       "             -0.0078,     -0.0008,     -0.0165,      0.0100,      0.0057,\n",
       "             -0.0032,      0.0016,      0.0020,      0.0137,      0.0140,\n",
       "              0.0158,      0.0120,     -0.0133,     -0.0178,     -0.0128,\n",
       "              0.0036,      0.0014,      0.0162,     -0.0012,      0.0142,\n",
       "              0.0178,     -0.0116,     -0.0138,     -0.0161,      0.0095,\n",
       "             -0.0083,      0.0152,     -0.0026,     -0.0149,     -0.0148,\n",
       "             -0.0131,      0.0173,      0.0053,     -0.0124,     -0.0107,\n",
       "              0.0088,      0.0172,      0.0107,     -0.0071,     -0.0130,\n",
       "             -0.0066,      0.0059,     -0.0079,      0.0001,      0.0071,\n",
       "              0.0157,      0.0153,      0.0109,     -0.0096,     -0.0099,\n",
       "              0.0072,     -0.0006,     -0.0080,      0.0017,      0.0121,\n",
       "             -0.0119,      0.0173,     -0.0135,      0.0163,     -0.0068,\n",
       "             -0.0023,      0.0078,     -0.0163,     -0.0009,     -0.0051,\n",
       "             -0.0014,      0.0038,      0.0124,      0.0012,     -0.0033,\n",
       "              0.0141,     -0.0045,     -0.0119,      0.0086,     -0.0021,\n",
       "              0.0124,     -0.0073,     -0.0155,      0.0179,      0.0137,\n",
       "             -0.0159,      0.0021,     -0.0130,      0.0023,     -0.0163,\n",
       "              0.0013,     -0.0045,     -0.0018,     -0.0107,     -0.0068,\n",
       "              0.0087,     -0.0021,     -0.0016,      0.0082,      0.0016,\n",
       "              0.0143,      0.0057,     -0.0152,     -0.0158,     -0.0099,\n",
       "             -0.0096,      0.0033,      0.0084,      0.0154,     -0.0051,\n",
       "              0.0046,     -0.0180,      0.0043,      0.0114,     -0.0032,\n",
       "              0.0117,      0.0111,      0.0168,     -0.0075,      0.0164,\n",
       "             -0.0024,     -0.0090,     -0.0071,      0.0176,     -0.0125,\n",
       "             -0.0145,     -0.0110,      0.0041,     -0.0069,      0.0166,\n",
       "             -0.0169,     -0.0051,     -0.0052,     -0.0174,      0.0033,\n",
       "             -0.0159,      0.0169,      0.0047,     -0.0056,      0.0101,\n",
       "             -0.0133,     -0.0163,      0.0059,      0.0002,     -0.0169,\n",
       "             -0.0102,      0.0172,      0.0143,     -0.0009,      0.0174,\n",
       "              0.0103,     -0.0024,     -0.0108,      0.0016,      0.0111,\n",
       "              0.0045,     -0.0010,      0.0011,      0.0014,     -0.0015,\n",
       "              0.0066,     -0.0062,      0.0062,      0.0043,      0.0026,\n",
       "              0.0083,      0.0165,     -0.0015,     -0.0097,     -0.0074,\n",
       "             -0.0049,      0.0153,     -0.0053,     -0.0089,      0.0135,\n",
       "              0.0014,      0.0024,      0.0097,     -0.0086,      0.0172,\n",
       "              0.0028,      0.0072,      0.0173,      0.0125,      0.0058,\n",
       "             -0.0119,     -0.0115,      0.0110,      0.0018,     -0.0008,\n",
       "             -0.0106,     -0.0159,     -0.0043,      0.0074,     -0.0118,\n",
       "              0.0042,      0.0088,     -0.0056,     -0.0148,      0.0139,\n",
       "             -0.0096,     -0.0021,      0.0039], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0264, -0.0033,  0.0135,  ...,  0.0151,  0.0345, -0.0213],\n",
       "         [ 0.0135, -0.0188,  0.0294,  ..., -0.0137,  0.0251,  0.0035],\n",
       "         [-0.0151, -0.0184,  0.0031,  ...,  0.0267, -0.0170,  0.0258],\n",
       "         ...,\n",
       "         [ 0.0171, -0.0292,  0.0325,  ...,  0.0132, -0.0274, -0.0211],\n",
       "         [-0.0045, -0.0263, -0.0312,  ...,  0.0308,  0.0007,  0.0283],\n",
       "         [ 0.0024, -0.0169,  0.0084,  ...,  0.0035,  0.0142,  0.0311]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0143, -0.0205, -0.0265,  ...,  0.0211,  0.0167,  0.0293],\n",
       "         [ 0.0227, -0.0118,  0.0084,  ..., -0.0328,  0.0327,  0.0219],\n",
       "         [-0.0035,  0.0122, -0.0246,  ...,  0.0011,  0.0237,  0.0025],\n",
       "         ...,\n",
       "         [ 0.0073,  0.0269, -0.0235,  ...,  0.0020, -0.0297,  0.0027],\n",
       "         [-0.0129, -0.0207, -0.0085,  ...,  0.0305,  0.0102,  0.0002],\n",
       "         [ 0.0001,  0.0177,  0.0044,  ..., -0.0075,  0.0215, -0.0291]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0009,  0.0263, -0.0240,  ...,  0.0241,  0.0313,  0.0291],\n",
       "         [-0.0069,  0.0137, -0.0211,  ...,  0.0041, -0.0099, -0.0298],\n",
       "         [ 0.0161, -0.0221, -0.0058,  ..., -0.0228, -0.0086, -0.0320],\n",
       "         ...,\n",
       "         [ 0.0353, -0.0318, -0.0093,  ...,  0.0338, -0.0090,  0.0298],\n",
       "         [ 0.0074, -0.0332, -0.0271,  ..., -0.0024,  0.0064, -0.0200],\n",
       "         [ 0.0191, -0.0309, -0.0353,  ...,  0.0233, -0.0058, -0.0180]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[    -0.0084,      0.0134,      0.0149,  ...,      0.0320,\n",
       "               0.0002,     -0.0150],\n",
       "         [     0.0142,     -0.0260,      0.0183,  ...,     -0.0064,\n",
       "              -0.0190,      0.0081],\n",
       "         [     0.0138,     -0.0269,     -0.0218,  ...,      0.0071,\n",
       "               0.0143,     -0.0340],\n",
       "         ...,\n",
       "         [     0.0074,      0.0354,      0.0152,  ...,     -0.0148,\n",
       "              -0.0331,      0.0078],\n",
       "         [     0.0185,      0.0175,      0.0251,  ...,     -0.0208,\n",
       "               0.0216,      0.0001],\n",
       "         [     0.0181,      0.0247,      0.0306,  ...,      0.0197,\n",
       "              -0.0095,      0.0239]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([    -0.0198,     -0.0028,      0.0327,      0.0133,     -0.0318,\n",
       "             -0.0306,     -0.0180,      0.0102,     -0.0032,      0.0008,\n",
       "             -0.0299,     -0.0072,      0.0150,     -0.0339,     -0.0007,\n",
       "             -0.0172,      0.0189,     -0.0048,      0.0108,      0.0224,\n",
       "              0.0298,      0.0290,      0.0088,     -0.0238,     -0.0215,\n",
       "              0.0068,     -0.0301,     -0.0225,     -0.0178,      0.0242,\n",
       "              0.0054,     -0.0325,     -0.0203,     -0.0152,     -0.0184,\n",
       "              0.0186,      0.0094,      0.0185,      0.0299,     -0.0349,\n",
       "             -0.0323,      0.0215,      0.0088,     -0.0304,     -0.0095,\n",
       "             -0.0120,      0.0065,     -0.0023,      0.0333,      0.0332,\n",
       "             -0.0103,     -0.0084,      0.0078,      0.0007,      0.0119,\n",
       "              0.0172,     -0.0173,     -0.0152,     -0.0018,     -0.0295,\n",
       "             -0.0135,     -0.0290,      0.0263,     -0.0027,     -0.0009,\n",
       "              0.0018,     -0.0231,     -0.0138,      0.0244,     -0.0060,\n",
       "             -0.0266,     -0.0068,      0.0179,     -0.0112,      0.0228,\n",
       "              0.0018,     -0.0240,     -0.0186,      0.0310,      0.0105,\n",
       "             -0.0325,     -0.0147,      0.0254,     -0.0067,     -0.0269,\n",
       "             -0.0319,      0.0217,     -0.0099,     -0.0333,      0.0252,\n",
       "              0.0058,      0.0026,     -0.0019,     -0.0291,     -0.0128,\n",
       "             -0.0199,      0.0117,     -0.0250,     -0.0296,      0.0219,\n",
       "             -0.0126,      0.0143,     -0.0148,     -0.0275,      0.0158,\n",
       "              0.0100,      0.0225,      0.0014,      0.0347,      0.0099,\n",
       "              0.0219,      0.0092,     -0.0267,     -0.0255,      0.0228,\n",
       "             -0.0117,     -0.0346,     -0.0262,      0.0044,     -0.0318,\n",
       "             -0.0011,      0.0014,      0.0170,      0.0236,     -0.0216,\n",
       "              0.0071,     -0.0350,      0.0337,      0.0242,     -0.0349,\n",
       "             -0.0176,      0.0306,     -0.0256,     -0.0180,      0.0105,\n",
       "              0.0321,      0.0203,     -0.0343,      0.0044,      0.0353,\n",
       "              0.0089,      0.0056,     -0.0242,     -0.0167,     -0.0219,\n",
       "              0.0299,     -0.0144,      0.0158,      0.0005,     -0.0126,\n",
       "              0.0236,      0.0248,     -0.0039,     -0.0115,     -0.0216,\n",
       "              0.0027,     -0.0298,      0.0213,      0.0069,      0.0225,\n",
       "              0.0013,      0.0085,      0.0356,     -0.0018,      0.0247,\n",
       "             -0.0151,     -0.0339,     -0.0048,     -0.0222,      0.0115,\n",
       "             -0.0113,     -0.0125,      0.0244,     -0.0010,     -0.0331,\n",
       "              0.0039,     -0.0275,     -0.0231,     -0.0345,      0.0310,\n",
       "             -0.0333,      0.0271,      0.0186,     -0.0020,     -0.0150,\n",
       "             -0.0257,      0.0219,     -0.0113,      0.0197,      0.0307,\n",
       "             -0.0313,      0.0218,     -0.0099,      0.0061,     -0.0193,\n",
       "              0.0278,      0.0095,      0.0117,     -0.0012,      0.0006,\n",
       "              0.0241,     -0.0270,      0.0325,     -0.0103,     -0.0056,\n",
       "             -0.0290,      0.0084,      0.0308,      0.0262,      0.0107,\n",
       "              0.0319,     -0.0229,      0.0089,     -0.0251,     -0.0033,\n",
       "             -0.0071,      0.0178,      0.0065,     -0.0323,      0.0157,\n",
       "             -0.0220,     -0.0203,     -0.0265,     -0.0179,      0.0051,\n",
       "             -0.0304,     -0.0170,     -0.0110,      0.0124,     -0.0079,\n",
       "              0.0210,     -0.0003,      0.0120,      0.0168,     -0.0351,\n",
       "             -0.0010,     -0.0230,     -0.0257,      0.0194,      0.0029,\n",
       "              0.0109,     -0.0089,     -0.0259,     -0.0183,     -0.0218,\n",
       "              0.0109,      0.0275,      0.0070,     -0.0178,      0.0334,\n",
       "              0.0132,      0.0337,      0.0037,      0.0162,     -0.0161,\n",
       "              0.0200,      0.0019,      0.0306,     -0.0197,     -0.0341,\n",
       "             -0.0338,     -0.0097,     -0.0251,      0.0331,     -0.0263,\n",
       "              0.0019,     -0.0048,      0.0058,     -0.0246,     -0.0342,\n",
       "              0.0127,     -0.0197,     -0.0355,      0.0131,     -0.0186,\n",
       "              0.0084,      0.0179,      0.0191,     -0.0042,     -0.0126,\n",
       "              0.0141,      0.0302,     -0.0312,      0.0317,     -0.0211,\n",
       "             -0.0038,      0.0198,     -0.0206,      0.0226,      0.0037,\n",
       "             -0.0221,     -0.0001,      0.0153,      0.0202,     -0.0049,\n",
       "              0.0329,     -0.0197,      0.0246,     -0.0303,      0.0026,\n",
       "              0.0188,     -0.0008,     -0.0018,      0.0199,     -0.0291,\n",
       "              0.0343,      0.0105,      0.0284,      0.0075,      0.0091,\n",
       "              0.0158,      0.0061,     -0.0079,      0.0035,     -0.0250,\n",
       "              0.0263,      0.0141,     -0.0358,      0.0106,      0.0112,\n",
       "             -0.0100,     -0.0107,     -0.0089,      0.0143,      0.0022,\n",
       "              0.0343,     -0.0071,     -0.0150,      0.0127,      0.0131,\n",
       "              0.0356,      0.0009,      0.0212,      0.0080,     -0.0157,\n",
       "             -0.0036,      0.0127,      0.0224,     -0.0226,      0.0211,\n",
       "             -0.0130,     -0.0005,      0.0044,      0.0144,     -0.0099,\n",
       "             -0.0337,      0.0005,      0.0114,     -0.0199,      0.0234,\n",
       "             -0.0329,     -0.0129,      0.0281,     -0.0359,     -0.0027,\n",
       "             -0.0343,     -0.0111,      0.0278,      0.0042,     -0.0044,\n",
       "             -0.0168,     -0.0236,     -0.0299,     -0.0232,      0.0070,\n",
       "              0.0262,      0.0239,     -0.0015,      0.0033,     -0.0314,\n",
       "              0.0311,     -0.0260,     -0.0024,     -0.0022,      0.0136,\n",
       "              0.0096,      0.0116,      0.0089,     -0.0276,      0.0273,\n",
       "             -0.0094,      0.0270,     -0.0253,     -0.0052,     -0.0005,\n",
       "             -0.0008,      0.0084,     -0.0165,     -0.0130,      0.0138,\n",
       "              0.0062,      0.0279,     -0.0008,     -0.0287,     -0.0211,\n",
       "             -0.0322,      0.0004,     -0.0089,      0.0343,      0.0003,\n",
       "              0.0345,      0.0325,     -0.0356,     -0.0147,      0.0244,\n",
       "             -0.0130,     -0.0225,      0.0110,     -0.0321,      0.0154,\n",
       "              0.0197,      0.0127,      0.0182,     -0.0181,     -0.0083,\n",
       "              0.0050,     -0.0225,     -0.0329,     -0.0153,      0.0306,\n",
       "             -0.0356,      0.0085,      0.0306,      0.0335,     -0.0062,\n",
       "             -0.0148,      0.0165,      0.0218,     -0.0017,      0.0257,\n",
       "             -0.0160,     -0.0286,     -0.0185,      0.0005,      0.0009,\n",
       "              0.0101,      0.0030,      0.0017,     -0.0213,      0.0285,\n",
       "              0.0123,     -0.0333,      0.0201,     -0.0354,      0.0359,\n",
       "              0.0032,     -0.0159,     -0.0337,     -0.0230,     -0.0118,\n",
       "              0.0068,     -0.0305,     -0.0091,     -0.0111,     -0.0233,\n",
       "              0.0149,     -0.0324,      0.0292,     -0.0120,     -0.0028,\n",
       "              0.0027,      0.0344,     -0.0261,     -0.0156,      0.0284,\n",
       "             -0.0160,     -0.0198,     -0.0237,      0.0351,     -0.0160,\n",
       "             -0.0259,      0.0139,      0.0318,     -0.0080,      0.0004,\n",
       "              0.0190,     -0.0306,      0.0043,     -0.0061,      0.0255,\n",
       "             -0.0219,     -0.0161,     -0.0148,     -0.0339,     -0.0338,\n",
       "             -0.0312,      0.0303,      0.0259,     -0.0354,     -0.0247,\n",
       "             -0.0180,      0.0330,      0.0156,     -0.0144,      0.0070,\n",
       "              0.0154,      0.0095,      0.0088,      0.0154,     -0.0190,\n",
       "             -0.0143,     -0.0141,     -0.0118,      0.0039,     -0.0008,\n",
       "              0.0254,      0.0085,      0.0145,      0.0347,     -0.0249,\n",
       "             -0.0062,     -0.0176,     -0.0041,     -0.0082,      0.0166,\n",
       "             -0.0082,      0.0027,      0.0054,      0.0209,     -0.0178,\n",
       "              0.0277,      0.0108,     -0.0319,     -0.0035,      0.0017,\n",
       "              0.0147,      0.0287,     -0.0202,      0.0011,     -0.0271,\n",
       "             -0.0258,     -0.0091,      0.0196,     -0.0006,      0.0159,\n",
       "              0.0064,     -0.0158,     -0.0151,     -0.0115,      0.0229,\n",
       "             -0.0245,      0.0044,     -0.0006,     -0.0303,     -0.0112,\n",
       "             -0.0100,      0.0083,     -0.0079,      0.0164,      0.0033,\n",
       "              0.0043,      0.0282,     -0.0219,      0.0315,     -0.0151,\n",
       "              0.0101,      0.0038,      0.0210,     -0.0015,      0.0263,\n",
       "              0.0287,      0.0182,     -0.0077,     -0.0313,      0.0196,\n",
       "             -0.0309,     -0.0105,     -0.0360,     -0.0068,      0.0301,\n",
       "              0.0285,      0.0093,      0.0237,     -0.0058,      0.0321,\n",
       "             -0.0104,     -0.0266,     -0.0091,     -0.0156,      0.0028,\n",
       "              0.0306,     -0.0083,     -0.0131,     -0.0350,      0.0303,\n",
       "              0.0173,     -0.0037,      0.0226,      0.0324,     -0.0287,\n",
       "              0.0306,      0.0075,     -0.0079,      0.0040,     -0.0150,\n",
       "             -0.0288,     -0.0224,      0.0300,     -0.0117,     -0.0304,\n",
       "              0.0216,     -0.0184,      0.0274,      0.0027,     -0.0033,\n",
       "             -0.0270,     -0.0149,      0.0068,      0.0046,      0.0128,\n",
       "             -0.0186,      0.0214,      0.0109,      0.0188,      0.0201,\n",
       "             -0.0325,      0.0160,      0.0126,      0.0264,      0.0060,\n",
       "             -0.0093,     -0.0306,      0.0325,      0.0014,      0.0248,\n",
       "              0.0314,     -0.0303,      0.0334,      0.0008,      0.0082,\n",
       "              0.0159,     -0.0231,     -0.0094,      0.0239,     -0.0347,\n",
       "             -0.0083,      0.0183,     -0.0232,      0.0235,     -0.0152,\n",
       "             -0.0035,     -0.0171,     -0.0249,     -0.0160,      0.0207,\n",
       "             -0.0196,      0.0153,     -0.0039,     -0.0323,      0.0036,\n",
       "              0.0060,      0.0073,      0.0206,     -0.0115,     -0.0165,\n",
       "             -0.0290,      0.0035,      0.0335,      0.0166,      0.0162,\n",
       "              0.0166,     -0.0124,      0.0291,     -0.0281,     -0.0154,\n",
       "              0.0052,      0.0171,      0.0324,     -0.0138,      0.0043,\n",
       "             -0.0087,      0.0189,     -0.0210,      0.0296,     -0.0303,\n",
       "              0.0359,     -0.0355,      0.0137,      0.0081,      0.0149,\n",
       "             -0.0080,     -0.0002,      0.0307,     -0.0184,      0.0300,\n",
       "              0.0045,      0.0111,      0.0277,     -0.0209,     -0.0200,\n",
       "             -0.0042,      0.0115,      0.0260,      0.0265,      0.0053,\n",
       "             -0.0079,      0.0028,     -0.0198,      0.0252,     -0.0143,\n",
       "             -0.0238,      0.0252,     -0.0240,     -0.0094,     -0.0002,\n",
       "              0.0047,     -0.0055,     -0.0290,     -0.0131,     -0.0279,\n",
       "              0.0251,      0.0062,     -0.0118,      0.0148,     -0.0310,\n",
       "              0.0174,     -0.0231,      0.0005,      0.0135,     -0.0052,\n",
       "             -0.0055,      0.0094,      0.0088,     -0.0306,     -0.0224,\n",
       "              0.0190,      0.0318,      0.0269,      0.0209,     -0.0097,\n",
       "              0.0133,     -0.0187,     -0.0113,      0.0129,      0.0210,\n",
       "              0.0282,      0.0183,      0.0276,     -0.0292,      0.0299,\n",
       "              0.0187,      0.0360,     -0.0212,     -0.0225,      0.0346,\n",
       "             -0.0280,     -0.0081,      0.0160,     -0.0108,     -0.0106,\n",
       "             -0.0292,      0.0245,     -0.0134,     -0.0157,     -0.0244,\n",
       "             -0.0177,      0.0309,      0.0008,     -0.0265,      0.0270,\n",
       "             -0.0177,     -0.0151,      0.0356,      0.0305,      0.0305,\n",
       "              0.0179,     -0.0130,      0.0266], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[     0.0192,     -0.0202,     -0.0064,  ...,      0.0179,\n",
       "               0.0169,     -0.0113],\n",
       "         [    -0.0059,      0.0130,      0.0340,  ...,      0.0174,\n",
       "              -0.0352,     -0.0227],\n",
       "         [     0.0200,      0.0144,     -0.0329,  ...,      0.0162,\n",
       "               0.0279,     -0.0116],\n",
       "         ...,\n",
       "         [     0.0248,      0.0173,     -0.0149,  ...,      0.0149,\n",
       "               0.0066,     -0.0085],\n",
       "         [    -0.0265,      0.0331,      0.0075,  ...,      0.0227,\n",
       "               0.0001,     -0.0308],\n",
       "         [    -0.0042,     -0.0227,      0.0352,  ...,      0.0196,\n",
       "              -0.0040,     -0.0173]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.0168,  0.0295,  0.0201,  ..., -0.0190,  0.0052,  0.0035],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0168,  0.0063,  0.0019,  ..., -0.0109,  0.0104, -0.0033],\n",
       "         [ 0.0018,  0.0123,  0.0175,  ...,  0.0099, -0.0052, -0.0069],\n",
       "         [ 0.0042,  0.0144,  0.0109,  ...,  0.0106,  0.0153, -0.0088],\n",
       "         ...,\n",
       "         [ 0.0149, -0.0111, -0.0142,  ...,  0.0032,  0.0177,  0.0046],\n",
       "         [ 0.0089, -0.0012, -0.0100,  ..., -0.0032,  0.0116, -0.0128],\n",
       "         [-0.0085,  0.0171, -0.0164,  ..., -0.0117, -0.0050,  0.0004]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([    -0.0148,      0.0158,      0.0060,     -0.0122,     -0.0001,\n",
       "             -0.0018,      0.0118,      0.0086,      0.0110,     -0.0130,\n",
       "             -0.0071,     -0.0081,      0.0113,     -0.0175,      0.0130,\n",
       "             -0.0079,     -0.0063,     -0.0031,      0.0164,     -0.0077,\n",
       "              0.0002,      0.0102,      0.0143,     -0.0147,     -0.0019,\n",
       "              0.0084,      0.0077,      0.0172,     -0.0140,      0.0115,\n",
       "             -0.0108,     -0.0140,      0.0094,      0.0086,     -0.0149,\n",
       "              0.0062,      0.0013,      0.0069,     -0.0022,     -0.0123,\n",
       "              0.0094,      0.0166,     -0.0106,      0.0123,     -0.0089,\n",
       "             -0.0037,      0.0078,      0.0178,     -0.0068,     -0.0060,\n",
       "             -0.0085,      0.0095,      0.0082,     -0.0157,      0.0057,\n",
       "             -0.0050,      0.0103,     -0.0126,     -0.0105,     -0.0168,\n",
       "              0.0118,     -0.0107,     -0.0085,     -0.0075,     -0.0067,\n",
       "              0.0016,     -0.0150,     -0.0009,      0.0112,     -0.0053,\n",
       "              0.0139,     -0.0159,     -0.0009,     -0.0012,     -0.0067,\n",
       "              0.0116,     -0.0076,      0.0132,     -0.0064,      0.0154,\n",
       "              0.0146,     -0.0066,     -0.0018,      0.0136,      0.0044,\n",
       "             -0.0046,      0.0118,     -0.0126,      0.0107,     -0.0120,\n",
       "             -0.0157,     -0.0126,      0.0102,     -0.0097,      0.0168,\n",
       "             -0.0093,      0.0012,     -0.0109,      0.0071,     -0.0139,\n",
       "             -0.0107,     -0.0140,      0.0119,      0.0137,     -0.0135,\n",
       "             -0.0082,      0.0004,      0.0033,     -0.0096,     -0.0041,\n",
       "              0.0033,     -0.0015,     -0.0112,     -0.0142,     -0.0052,\n",
       "             -0.0169,      0.0116,     -0.0180,      0.0148,      0.0004,\n",
       "             -0.0025,     -0.0045,      0.0119,      0.0037,      0.0023,\n",
       "             -0.0151,      0.0033,     -0.0079,      0.0050,      0.0178,\n",
       "              0.0047,      0.0102,      0.0075,     -0.0092,     -0.0145,\n",
       "              0.0086,      0.0049,      0.0069,     -0.0170,     -0.0162,\n",
       "             -0.0056,     -0.0061,      0.0002,     -0.0157,     -0.0041,\n",
       "             -0.0019,      0.0025,      0.0026,      0.0134,      0.0064,\n",
       "             -0.0113,      0.0105,      0.0018,     -0.0068,     -0.0069,\n",
       "              0.0072,      0.0152,      0.0064,      0.0023,      0.0038,\n",
       "             -0.0087,     -0.0151,      0.0027,     -0.0160,      0.0136,\n",
       "             -0.0050,      0.0103,     -0.0128,      0.0056,      0.0015,\n",
       "              0.0121,     -0.0031,     -0.0078,      0.0040,     -0.0065,\n",
       "             -0.0144,      0.0009,     -0.0002,      0.0093,      0.0093,\n",
       "              0.0097,     -0.0117,     -0.0101,      0.0160,      0.0010,\n",
       "             -0.0029,     -0.0019,     -0.0143,      0.0035,     -0.0172,\n",
       "             -0.0038,     -0.0059,      0.0148,     -0.0054,      0.0078,\n",
       "             -0.0132,     -0.0019,      0.0011,     -0.0054,     -0.0144,\n",
       "              0.0164,     -0.0091,      0.0127,     -0.0057,     -0.0012,\n",
       "              0.0011,      0.0023,      0.0049,      0.0076,      0.0121,\n",
       "              0.0026,     -0.0115,      0.0022,     -0.0075,     -0.0143,\n",
       "             -0.0123,      0.0021,     -0.0148,     -0.0137,     -0.0148,\n",
       "             -0.0064,     -0.0093,      0.0090,     -0.0083,      0.0003,\n",
       "              0.0107,     -0.0146,     -0.0027,      0.0047,      0.0025,\n",
       "             -0.0112,     -0.0005,      0.0016,      0.0144,     -0.0120,\n",
       "              0.0141,     -0.0076,     -0.0136,     -0.0158,     -0.0051,\n",
       "              0.0167,      0.0005,     -0.0137,     -0.0047,     -0.0061,\n",
       "             -0.0011,     -0.0087,     -0.0163,     -0.0047,     -0.0015,\n",
       "              0.0008,     -0.0096,      0.0030,      0.0082,     -0.0106,\n",
       "             -0.0009,     -0.0153,      0.0160,     -0.0088,      0.0053,\n",
       "             -0.0067,     -0.0036,     -0.0137,     -0.0131,     -0.0177,\n",
       "             -0.0146,      0.0145,      0.0168,     -0.0117,      0.0135,\n",
       "              0.0094,     -0.0137,     -0.0001,      0.0164,     -0.0164,\n",
       "             -0.0004,      0.0083,     -0.0172,     -0.0027,     -0.0110,\n",
       "             -0.0042,      0.0017,     -0.0044,     -0.0112,      0.0073,\n",
       "              0.0146,      0.0041,     -0.0052,     -0.0029,      0.0125,\n",
       "              0.0105,     -0.0018,      0.0125,     -0.0028,      0.0028,\n",
       "              0.0097,      0.0060,     -0.0082,      0.0129,     -0.0065,\n",
       "              0.0070,     -0.0131,      0.0024,      0.0089,      0.0106,\n",
       "             -0.0115,      0.0042,     -0.0087,      0.0111,      0.0092,\n",
       "              0.0083,     -0.0112,      0.0043,      0.0005,      0.0180,\n",
       "             -0.0100,     -0.0056,      0.0037,     -0.0123,     -0.0077,\n",
       "             -0.0155,     -0.0130,     -0.0145,     -0.0047,      0.0117,\n",
       "              0.0003,      0.0088,      0.0082,     -0.0010,     -0.0001,\n",
       "             -0.0010,      0.0132,      0.0079,     -0.0093,      0.0075,\n",
       "              0.0049,      0.0034,     -0.0104,     -0.0004,     -0.0025,\n",
       "             -0.0174,     -0.0128,     -0.0044,     -0.0114,      0.0117,\n",
       "              0.0030,      0.0026,      0.0154,      0.0064,      0.0098,\n",
       "              0.0152,      0.0040,     -0.0068,      0.0146,     -0.0077,\n",
       "              0.0148,     -0.0049,     -0.0118,      0.0144,      0.0141,\n",
       "              0.0041,     -0.0113,     -0.0128,     -0.0108,     -0.0126,\n",
       "              0.0044,     -0.0015,     -0.0038,     -0.0165,      0.0161,\n",
       "             -0.0059,      0.0070,     -0.0107,      0.0004,     -0.0002,\n",
       "             -0.0056,     -0.0091,      0.0130,     -0.0038,     -0.0089,\n",
       "              0.0083,     -0.0022,      0.0123,     -0.0046,     -0.0122,\n",
       "              0.0154,      0.0009,      0.0005,      0.0065,     -0.0089,\n",
       "             -0.0125,     -0.0010,     -0.0040,     -0.0010,     -0.0026,\n",
       "              0.0019,      0.0025,     -0.0036,     -0.0009,      0.0167,\n",
       "             -0.0146,     -0.0095,      0.0119,      0.0051,      0.0034,\n",
       "             -0.0172,      0.0107,     -0.0097,      0.0110,     -0.0107,\n",
       "              0.0086,     -0.0084,     -0.0147,     -0.0019,     -0.0042,\n",
       "             -0.0180,     -0.0146,      0.0057,     -0.0130,      0.0168,\n",
       "             -0.0113,      0.0167,     -0.0130,     -0.0136,      0.0062,\n",
       "              0.0104,     -0.0152,      0.0078,     -0.0082,      0.0042,\n",
       "              0.0039,     -0.0050,      0.0115,      0.0067,     -0.0094,\n",
       "              0.0141,      0.0128,      0.0025,      0.0123,      0.0098,\n",
       "             -0.0108,      0.0045,      0.0043,      0.0033,     -0.0009,\n",
       "             -0.0025,     -0.0148,      0.0146,     -0.0070,      0.0012,\n",
       "             -0.0081,      0.0173,     -0.0019,     -0.0046,     -0.0054,\n",
       "             -0.0004,     -0.0119,     -0.0126,      0.0124,      0.0030,\n",
       "             -0.0142,     -0.0124,     -0.0166,      0.0002,      0.0002,\n",
       "             -0.0006,      0.0010,      0.0088,     -0.0004,      0.0107,\n",
       "             -0.0152,      0.0010,      0.0145,     -0.0099,      0.0047,\n",
       "             -0.0157,     -0.0070,     -0.0052,      0.0009,      0.0101,\n",
       "              0.0008,      0.0148,     -0.0083,     -0.0077,      0.0041,\n",
       "             -0.0102,      0.0039,     -0.0135,      0.0102,      0.0113,\n",
       "             -0.0119,     -0.0158,      0.0066,      0.0045,     -0.0003,\n",
       "              0.0157,     -0.0168,      0.0172,      0.0096,      0.0137,\n",
       "              0.0069,     -0.0017,     -0.0131,     -0.0056,     -0.0157,\n",
       "             -0.0125,     -0.0172,     -0.0137,     -0.0154,      0.0027,\n",
       "              0.0113,     -0.0160,     -0.0124,      0.0163,     -0.0156,\n",
       "              0.0137,      0.0005,     -0.0133,      0.0157,     -0.0112,\n",
       "              0.0138,      0.0118,      0.0128,      0.0013,     -0.0105,\n",
       "              0.0160,     -0.0027,      0.0051,      0.0084,     -0.0099,\n",
       "             -0.0015,      0.0044,      0.0074,      0.0070,      0.0073,\n",
       "              0.0039,      0.0041,     -0.0154,     -0.0042,      0.0133,\n",
       "             -0.0058,      0.0001,     -0.0006,      0.0133,      0.0007,\n",
       "             -0.0080,      0.0064,      0.0102,      0.0029,      0.0128,\n",
       "             -0.0164,     -0.0102,      0.0096,     -0.0107,      0.0144,\n",
       "             -0.0020,     -0.0007,      0.0103,     -0.0121,      0.0078,\n",
       "              0.0001,      0.0173,     -0.0076,     -0.0001,     -0.0029,\n",
       "              0.0027,     -0.0024,     -0.0011,     -0.0131,     -0.0121,\n",
       "              0.0124,     -0.0093,      0.0097,     -0.0150,     -0.0029,\n",
       "             -0.0134,     -0.0131,     -0.0011,      0.0083,     -0.0102,\n",
       "             -0.0014,      0.0158,      0.0110,     -0.0006,      0.0159,\n",
       "             -0.0177,     -0.0118,     -0.0102,      0.0038,     -0.0065,\n",
       "              0.0033,      0.0062,     -0.0007,      0.0129,     -0.0085,\n",
       "              0.0141,     -0.0008,     -0.0035,      0.0150,     -0.0019,\n",
       "              0.0039,     -0.0123,      0.0095,      0.0032,     -0.0091,\n",
       "             -0.0055,     -0.0127,      0.0004,     -0.0022,     -0.0140,\n",
       "             -0.0110,      0.0059,     -0.0042,      0.0131,     -0.0083,\n",
       "             -0.0008,      0.0159,      0.0016,      0.0132,     -0.0157,\n",
       "              0.0116,     -0.0014,      0.0045,      0.0164,     -0.0013,\n",
       "              0.0116,     -0.0134,      0.0114,     -0.0058,      0.0151,\n",
       "             -0.0125,     -0.0134,     -0.0114,      0.0054,      0.0109,\n",
       "              0.0099,     -0.0071,     -0.0151,     -0.0019,      0.0010,\n",
       "             -0.0157,      0.0133,     -0.0124,      0.0100,     -0.0096,\n",
       "             -0.0170,      0.0102,      0.0069,      0.0043,      0.0141,\n",
       "              0.0168,      0.0150,     -0.0027,      0.0120,      0.0063,\n",
       "             -0.0161,      0.0004,     -0.0073,     -0.0026,      0.0113,\n",
       "              0.0136,     -0.0004,     -0.0136,      0.0114,     -0.0116,\n",
       "             -0.0086,     -0.0071,     -0.0103,      0.0062,     -0.0157,\n",
       "             -0.0043,      0.0019,     -0.0026,      0.0115,      0.0082,\n",
       "             -0.0104,     -0.0145,     -0.0017,      0.0021,      0.0023,\n",
       "              0.0018,      0.0039,     -0.0063,      0.0038,      0.0013,\n",
       "             -0.0153,     -0.0033,     -0.0169,     -0.0043,      0.0136,\n",
       "              0.0119,     -0.0119,     -0.0175,     -0.0110,      0.0145,\n",
       "              0.0106,      0.0128,      0.0147,     -0.0058,     -0.0005,\n",
       "             -0.0032,     -0.0179,     -0.0079,     -0.0059,      0.0100,\n",
       "             -0.0168,     -0.0146,     -0.0105,     -0.0138,     -0.0099,\n",
       "             -0.0033,      0.0155,     -0.0169,     -0.0086,      0.0164,\n",
       "             -0.0163,     -0.0124,      0.0035,     -0.0015,      0.0001,\n",
       "             -0.0100,     -0.0067,      0.0079,     -0.0072,     -0.0004,\n",
       "             -0.0018,      0.0130,      0.0026,      0.0168,      0.0064,\n",
       "              0.0176,     -0.0079,      0.0040,      0.0057,      0.0048,\n",
       "             -0.0031,     -0.0155,      0.0021,      0.0118,      0.0115,\n",
       "              0.0064,      0.0064,      0.0172,      0.0107,     -0.0102,\n",
       "              0.0031,     -0.0118,      0.0059,      0.0048,      0.0106,\n",
       "              0.0154,      0.0049,     -0.0075,     -0.0173,      0.0150,\n",
       "              0.0011,     -0.0153,      0.0102,     -0.0004,     -0.0006,\n",
       "              0.0133,     -0.0064,     -0.0145,     -0.0030,     -0.0145,\n",
       "             -0.0017,     -0.0036,     -0.0127], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0044, -0.0314,  0.0292,  ..., -0.0063, -0.0111, -0.0145],\n",
       "         [ 0.0242,  0.0219,  0.0168,  ...,  0.0037, -0.0240, -0.0287],\n",
       "         [ 0.0357, -0.0302,  0.0177,  ...,  0.0089, -0.0107, -0.0244],\n",
       "         ...,\n",
       "         [ 0.0281,  0.0338,  0.0307,  ..., -0.0213,  0.0273,  0.0057],\n",
       "         [ 0.0329, -0.0282, -0.0177,  ..., -0.0041,  0.0294, -0.0121],\n",
       "         [ 0.0220, -0.0095,  0.0359,  ...,  0.0172,  0.0178, -0.0108]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0173,  0.0241,  0.0262,  ...,  0.0306,  0.0186,  0.0309],\n",
       "         [ 0.0283, -0.0234,  0.0063,  ..., -0.0345, -0.0200,  0.0111],\n",
       "         [-0.0153,  0.0078, -0.0298,  ..., -0.0094,  0.0249, -0.0025],\n",
       "         ...,\n",
       "         [-0.0015,  0.0205, -0.0077,  ...,  0.0327, -0.0243, -0.0235],\n",
       "         [-0.0013,  0.0282, -0.0335,  ..., -0.0295,  0.0190, -0.0300],\n",
       "         [-0.0334, -0.0347,  0.0321,  ..., -0.0024, -0.0325,  0.0321]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0178,  0.0136, -0.0270,  ...,  0.0209, -0.0082,  0.0216],\n",
       "         [ 0.0218, -0.0172,  0.0330,  ...,  0.0281,  0.0224, -0.0286],\n",
       "         [ 0.0092,  0.0069, -0.0326,  ...,  0.0301, -0.0019, -0.0243],\n",
       "         ...,\n",
       "         [-0.0205,  0.0133, -0.0130,  ...,  0.0034,  0.0223,  0.0086],\n",
       "         [ 0.0222,  0.0214,  0.0308,  ..., -0.0229, -0.0119,  0.0265],\n",
       "         [ 0.0188, -0.0309, -0.0154,  ...,  0.0179,  0.0256, -0.0023]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0055,  0.0117, -0.0259,  ..., -0.0130, -0.0220, -0.0102],\n",
       "         [ 0.0340,  0.0185,  0.0150,  ..., -0.0357, -0.0038, -0.0032],\n",
       "         [-0.0050, -0.0349,  0.0025,  ..., -0.0175, -0.0331, -0.0230],\n",
       "         ...,\n",
       "         [-0.0082,  0.0027,  0.0272,  ..., -0.0142, -0.0309, -0.0083],\n",
       "         [ 0.0117, -0.0133,  0.0321,  ..., -0.0153, -0.0219,  0.0170],\n",
       "         [-0.0351,  0.0334, -0.0260,  ...,  0.0261, -0.0085,  0.0272]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([     0.0265,     -0.0261,      0.0342,     -0.0223,      0.0031,\n",
       "             -0.0293,      0.0150,      0.0114,     -0.0078,      0.0055,\n",
       "             -0.0101,      0.0038,      0.0011,      0.0069,     -0.0176,\n",
       "             -0.0196,     -0.0081,      0.0293,     -0.0006,     -0.0095,\n",
       "              0.0172,     -0.0218,      0.0035,      0.0116,     -0.0201,\n",
       "             -0.0089,     -0.0091,     -0.0184,      0.0314,      0.0157,\n",
       "             -0.0072,     -0.0327,     -0.0184,     -0.0299,     -0.0183,\n",
       "             -0.0025,     -0.0084,      0.0075,     -0.0104,     -0.0140,\n",
       "              0.0061,     -0.0324,      0.0056,     -0.0248,     -0.0222,\n",
       "             -0.0133,     -0.0016,      0.0062,     -0.0130,     -0.0085,\n",
       "              0.0290,      0.0227,     -0.0122,     -0.0154,      0.0360,\n",
       "              0.0218,     -0.0047,      0.0229,     -0.0276,     -0.0282,\n",
       "             -0.0173,      0.0333,      0.0053,     -0.0257,     -0.0098,\n",
       "              0.0210,      0.0231,     -0.0353,     -0.0091,      0.0354,\n",
       "             -0.0347,     -0.0054,      0.0202,      0.0256,      0.0009,\n",
       "              0.0222,     -0.0115,     -0.0176,     -0.0238,      0.0015,\n",
       "              0.0351,      0.0009,     -0.0274,     -0.0056,      0.0049,\n",
       "              0.0135,     -0.0114,      0.0305,      0.0267,     -0.0103,\n",
       "             -0.0232,      0.0295,      0.0101,      0.0114,      0.0029,\n",
       "             -0.0252,      0.0253,     -0.0073,      0.0167,     -0.0315,\n",
       "             -0.0263,     -0.0049,      0.0089,      0.0318,     -0.0059,\n",
       "              0.0279,     -0.0359,     -0.0270,     -0.0238,      0.0219,\n",
       "             -0.0349,      0.0219,     -0.0007,     -0.0062,     -0.0189,\n",
       "             -0.0142,      0.0291,     -0.0157,     -0.0022,      0.0238,\n",
       "             -0.0046,     -0.0333,      0.0252,      0.0289,      0.0036,\n",
       "             -0.0182,      0.0349,      0.0206,      0.0302,      0.0294,\n",
       "              0.0118,      0.0208,      0.0197,     -0.0193,      0.0075,\n",
       "             -0.0233,     -0.0241,      0.0323,     -0.0224,      0.0125,\n",
       "             -0.0085,      0.0322,      0.0185,     -0.0279,      0.0119,\n",
       "              0.0069,      0.0202,     -0.0250,      0.0006,     -0.0130,\n",
       "              0.0350,      0.0342,      0.0224,      0.0082,      0.0285,\n",
       "             -0.0082,     -0.0067,      0.0263,      0.0229,     -0.0105,\n",
       "             -0.0357,     -0.0132,      0.0220,     -0.0290,      0.0152,\n",
       "              0.0139,      0.0263,     -0.0068,     -0.0306,     -0.0119,\n",
       "              0.0221,      0.0347,      0.0156,      0.0247,      0.0278,\n",
       "             -0.0217,     -0.0269,      0.0006,     -0.0160,      0.0022,\n",
       "             -0.0247,      0.0004,      0.0109,     -0.0241,      0.0039,\n",
       "              0.0154,      0.0100,     -0.0174,     -0.0052,      0.0322,\n",
       "              0.0061,     -0.0100,     -0.0188,      0.0307,      0.0254,\n",
       "             -0.0133,      0.0162,     -0.0165,      0.0045,      0.0074,\n",
       "             -0.0080,      0.0212,     -0.0209,      0.0153,      0.0028,\n",
       "             -0.0306,      0.0264,     -0.0008,     -0.0339,      0.0148,\n",
       "             -0.0127,      0.0046,      0.0129,     -0.0322,     -0.0010,\n",
       "             -0.0311,     -0.0162,     -0.0224,      0.0342,      0.0067,\n",
       "              0.0329,     -0.0283,     -0.0144,     -0.0091,      0.0140,\n",
       "              0.0184,     -0.0063,      0.0170,      0.0120,     -0.0058,\n",
       "              0.0285,      0.0106,     -0.0315,     -0.0195,     -0.0220,\n",
       "             -0.0058,      0.0126,      0.0133,     -0.0039,      0.0159,\n",
       "             -0.0255,     -0.0299,      0.0043,      0.0286,      0.0090,\n",
       "             -0.0248,      0.0244,     -0.0194,      0.0357,      0.0275,\n",
       "             -0.0353,      0.0338,      0.0301,      0.0358,     -0.0202,\n",
       "             -0.0166,     -0.0014,      0.0169,      0.0242,     -0.0332,\n",
       "              0.0081,      0.0207,     -0.0097,     -0.0210,      0.0261,\n",
       "              0.0166,     -0.0214,      0.0038,     -0.0355,      0.0120,\n",
       "             -0.0178,     -0.0275,      0.0250,     -0.0130,     -0.0251,\n",
       "              0.0172,      0.0258,      0.0331,     -0.0266,     -0.0300,\n",
       "             -0.0002,     -0.0329,      0.0158,      0.0181,     -0.0302,\n",
       "             -0.0352,      0.0021,      0.0150,      0.0261,     -0.0323,\n",
       "             -0.0018,      0.0332,      0.0040,     -0.0021,     -0.0359,\n",
       "             -0.0199,      0.0267,     -0.0358,     -0.0313,      0.0325,\n",
       "              0.0009,     -0.0161,      0.0003,      0.0161,      0.0114,\n",
       "             -0.0009,      0.0271,      0.0131,      0.0045,     -0.0218,\n",
       "             -0.0200,     -0.0001,      0.0137,     -0.0127,      0.0352,\n",
       "              0.0191,      0.0182,      0.0325,      0.0111,     -0.0232,\n",
       "             -0.0345,     -0.0068,     -0.0327,      0.0360,     -0.0333,\n",
       "              0.0188,     -0.0292,     -0.0169,     -0.0260,     -0.0104,\n",
       "              0.0044,     -0.0059,      0.0108,      0.0024,      0.0098,\n",
       "             -0.0081,      0.0041,      0.0053,     -0.0348,      0.0229,\n",
       "             -0.0282,      0.0328,     -0.0280,     -0.0087,     -0.0178,\n",
       "             -0.0135,      0.0267,     -0.0279,     -0.0199,     -0.0293,\n",
       "             -0.0208,      0.0015,     -0.0300,      0.0152,      0.0137,\n",
       "             -0.0146,     -0.0026,      0.0250,     -0.0356,      0.0150,\n",
       "              0.0275,      0.0046,      0.0201,     -0.0292,      0.0057,\n",
       "             -0.0251,      0.0081,     -0.0048,     -0.0189,      0.0065,\n",
       "              0.0034,     -0.0138,     -0.0202,      0.0060,      0.0024,\n",
       "              0.0277,      0.0035,     -0.0238,     -0.0314,     -0.0201,\n",
       "              0.0292,     -0.0225,     -0.0233,     -0.0185,     -0.0285,\n",
       "              0.0020,     -0.0256,     -0.0303,      0.0084,      0.0155,\n",
       "             -0.0324,      0.0178,     -0.0349,      0.0088,      0.0250,\n",
       "             -0.0026,      0.0354,     -0.0130,     -0.0176,      0.0321,\n",
       "              0.0309,     -0.0326,     -0.0274,     -0.0115,      0.0175,\n",
       "              0.0301,      0.0074,      0.0184,      0.0315,     -0.0168,\n",
       "              0.0019,     -0.0222,      0.0115,     -0.0345,      0.0224,\n",
       "             -0.0276,     -0.0003,     -0.0255,      0.0358,      0.0245,\n",
       "              0.0286,     -0.0028,      0.0212,     -0.0240,     -0.0212,\n",
       "             -0.0300,     -0.0192,     -0.0221,      0.0015,     -0.0030,\n",
       "             -0.0297,      0.0044,      0.0299,     -0.0251,      0.0195,\n",
       "             -0.0069,      0.0197,      0.0329,     -0.0269,     -0.0167,\n",
       "             -0.0210,     -0.0184,      0.0232,     -0.0041,     -0.0139,\n",
       "              0.0208,     -0.0040,      0.0036,      0.0284,     -0.0224,\n",
       "              0.0347,      0.0194,      0.0039,      0.0257,      0.0144,\n",
       "             -0.0116,     -0.0178,     -0.0280,     -0.0334,      0.0172,\n",
       "             -0.0084,     -0.0287,     -0.0254,      0.0256,      0.0130,\n",
       "              0.0339,     -0.0198,      0.0327,      0.0240,     -0.0161,\n",
       "              0.0080,      0.0184,     -0.0173,      0.0246,     -0.0328,\n",
       "             -0.0102,      0.0203,     -0.0310,     -0.0250,      0.0334,\n",
       "              0.0335,     -0.0263,     -0.0062,     -0.0308,     -0.0239,\n",
       "             -0.0021,     -0.0153,     -0.0310,      0.0308,      0.0196,\n",
       "             -0.0071,      0.0189,      0.0025,      0.0157,     -0.0259,\n",
       "             -0.0079,      0.0250,      0.0345,      0.0284,      0.0105,\n",
       "              0.0015,     -0.0328,      0.0172,     -0.0161,      0.0063,\n",
       "             -0.0337,     -0.0159,      0.0286,     -0.0027,      0.0213,\n",
       "              0.0145,     -0.0006,     -0.0087,     -0.0171,     -0.0325,\n",
       "             -0.0357,      0.0148,      0.0343,     -0.0275,     -0.0137,\n",
       "             -0.0097,      0.0346,      0.0076,      0.0160,      0.0342,\n",
       "              0.0218,      0.0002,      0.0251,     -0.0005,      0.0100,\n",
       "              0.0116,     -0.0342,      0.0217,      0.0027,     -0.0225,\n",
       "             -0.0342,      0.0247,     -0.0028,     -0.0247,     -0.0242,\n",
       "             -0.0050,      0.0290,     -0.0120,     -0.0090,     -0.0195,\n",
       "              0.0319,     -0.0084,     -0.0151,      0.0320,     -0.0348,\n",
       "              0.0107,     -0.0302,      0.0078,      0.0233,     -0.0169,\n",
       "             -0.0059,     -0.0139,     -0.0337,     -0.0283,      0.0095,\n",
       "              0.0026,     -0.0247,      0.0326,      0.0237,      0.0147,\n",
       "             -0.0119,     -0.0359,     -0.0045,      0.0080,      0.0123,\n",
       "              0.0361,      0.0097,     -0.0136,      0.0230,      0.0304,\n",
       "              0.0291,      0.0217,     -0.0267,     -0.0214,      0.0291,\n",
       "              0.0201,     -0.0311,     -0.0292,      0.0317,      0.0066,\n",
       "              0.0124,      0.0012,     -0.0099,     -0.0040,      0.0238,\n",
       "             -0.0247,     -0.0058,     -0.0067,      0.0223,     -0.0175,\n",
       "             -0.0318,     -0.0123,      0.0059,      0.0149,      0.0292,\n",
       "              0.0011,      0.0211,     -0.0142,     -0.0036,     -0.0345,\n",
       "             -0.0028,      0.0249,      0.0171,      0.0072,     -0.0206,\n",
       "              0.0276,     -0.0332,      0.0157,      0.0282,      0.0357,\n",
       "              0.0123,     -0.0303,     -0.0338,      0.0168,     -0.0005,\n",
       "             -0.0172,     -0.0143,     -0.0322,     -0.0029,     -0.0002,\n",
       "              0.0217,      0.0061,     -0.0039,     -0.0166,      0.0289,\n",
       "             -0.0360,      0.0047,      0.0094,      0.0250,     -0.0283,\n",
       "             -0.0198,      0.0027,      0.0211,      0.0268,      0.0113,\n",
       "              0.0184,     -0.0060,     -0.0264,     -0.0147,      0.0131,\n",
       "             -0.0124,     -0.0282,      0.0273,     -0.0249,      0.0278,\n",
       "              0.0141,      0.0008,      0.0262,      0.0084,      0.0303,\n",
       "             -0.0199,     -0.0032,     -0.0005,      0.0044,      0.0204,\n",
       "             -0.0292,      0.0066,     -0.0223,      0.0213,      0.0052,\n",
       "              0.0099,     -0.0220,     -0.0204,      0.0298,     -0.0134,\n",
       "              0.0240,     -0.0088,      0.0079,      0.0105,     -0.0040,\n",
       "             -0.0002,     -0.0212,     -0.0278,      0.0064,      0.0317,\n",
       "             -0.0355,      0.0034,      0.0174,     -0.0159,     -0.0300,\n",
       "             -0.0046,      0.0130,     -0.0127,     -0.0019,     -0.0100,\n",
       "             -0.0176,     -0.0044,     -0.0077,     -0.0060,      0.0093,\n",
       "              0.0345,     -0.0339,      0.0293,      0.0084,     -0.0196,\n",
       "              0.0212,      0.0309,      0.0260,      0.0104,      0.0253,\n",
       "              0.0257,      0.0078,      0.0024,     -0.0225,     -0.0204,\n",
       "             -0.0072,     -0.0254,     -0.0103,      0.0306,      0.0190,\n",
       "             -0.0153,     -0.0103,      0.0167,      0.0049,     -0.0341,\n",
       "             -0.0165,      0.0113,     -0.0173,     -0.0091,     -0.0081,\n",
       "             -0.0168,      0.0291,     -0.0023,      0.0110,     -0.0122,\n",
       "              0.0024,     -0.0020,      0.0194,     -0.0068,      0.0167,\n",
       "              0.0171,     -0.0139,      0.0311,      0.0192,     -0.0162,\n",
       "              0.0089,     -0.0300,      0.0017,      0.0261,     -0.0145,\n",
       "             -0.0315,     -0.0125,     -0.0350,      0.0240,     -0.0328,\n",
       "              0.0347,     -0.0113,      0.0107,     -0.0221,     -0.0342,\n",
       "              0.0355,     -0.0192,      0.0165,      0.0293,     -0.0027,\n",
       "              0.0036,      0.0184,      0.0337,     -0.0031,      0.0313,\n",
       "              0.0049,      0.0035,     -0.0075], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0185, -0.0215,  0.0130,  ..., -0.0207, -0.0050,  0.0275],\n",
       "         [-0.0029, -0.0172, -0.0281,  ..., -0.0209, -0.0265, -0.0154],\n",
       "         [-0.0358,  0.0009,  0.0131,  ...,  0.0240, -0.0255, -0.0267],\n",
       "         ...,\n",
       "         [-0.0102,  0.0181, -0.0107,  ...,  0.0025,  0.0054,  0.0231],\n",
       "         [ 0.0196, -0.0060, -0.0011,  ..., -0.0321, -0.0198, -0.0113],\n",
       "         [ 0.0271, -0.0200,  0.0232,  ..., -0.0103,  0.0342,  0.0333]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([    -0.0172,     -0.0001,      0.0159,  ...,     -0.0020,\n",
       "              0.0274,     -0.0016], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0066,  0.0007, -0.0121,  ..., -0.0014,  0.0177,  0.0170],\n",
       "         [ 0.0168,  0.0034, -0.0067,  ..., -0.0032,  0.0129,  0.0161],\n",
       "         [-0.0098, -0.0028,  0.0153,  ..., -0.0068,  0.0166, -0.0070],\n",
       "         ...,\n",
       "         [ 0.0054,  0.0023,  0.0175,  ...,  0.0160, -0.0132, -0.0022],\n",
       "         [ 0.0161,  0.0048, -0.0145,  ..., -0.0031,  0.0014, -0.0026],\n",
       "         [ 0.0025,  0.0180,  0.0179,  ..., -0.0003, -0.0063, -0.0131]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([     0.0060,      0.0022,      0.0075,      0.0105,     -0.0046,\n",
       "              0.0130,     -0.0177,     -0.0048,      0.0073,     -0.0032,\n",
       "              0.0068,      0.0021,      0.0176,     -0.0129,     -0.0077,\n",
       "              0.0058,      0.0122,     -0.0150,      0.0049,     -0.0029,\n",
       "             -0.0134,     -0.0159,     -0.0066,     -0.0158,      0.0043,\n",
       "              0.0031,     -0.0147,     -0.0179,     -0.0067,      0.0076,\n",
       "              0.0149,     -0.0060,      0.0119,      0.0178,      0.0101,\n",
       "             -0.0033,      0.0017,     -0.0053,      0.0009,     -0.0175,\n",
       "              0.0165,      0.0004,      0.0052,     -0.0117,     -0.0083,\n",
       "             -0.0115,     -0.0099,     -0.0037,      0.0052,      0.0104,\n",
       "             -0.0057,     -0.0178,     -0.0068,      0.0065,     -0.0150,\n",
       "              0.0143,     -0.0072,      0.0166,      0.0128,     -0.0025,\n",
       "             -0.0037,      0.0156,     -0.0097,     -0.0113,      0.0002,\n",
       "             -0.0087,      0.0046,      0.0012,      0.0055,     -0.0071,\n",
       "             -0.0134,     -0.0107,      0.0156,     -0.0159,      0.0158,\n",
       "              0.0113,      0.0030,      0.0171,     -0.0123,      0.0145,\n",
       "             -0.0060,      0.0180,      0.0143,     -0.0073,      0.0094,\n",
       "             -0.0027,     -0.0105,      0.0049,     -0.0169,      0.0102,\n",
       "              0.0039,     -0.0140,     -0.0096,      0.0111,     -0.0180,\n",
       "             -0.0085,      0.0072,     -0.0073,      0.0046,     -0.0130,\n",
       "             -0.0065,     -0.0031,      0.0167,      0.0139,      0.0016,\n",
       "             -0.0014,      0.0143,      0.0073,     -0.0153,      0.0151,\n",
       "             -0.0165,     -0.0148,     -0.0018,     -0.0123,      0.0154,\n",
       "              0.0043,     -0.0024,     -0.0031,      0.0035,     -0.0001,\n",
       "              0.0020,     -0.0143,     -0.0091,      0.0048,      0.0087,\n",
       "             -0.0042,     -0.0097,     -0.0143,     -0.0021,     -0.0005,\n",
       "             -0.0179,      0.0079,     -0.0031,     -0.0143,      0.0064,\n",
       "             -0.0177,     -0.0066,      0.0056,      0.0157,      0.0177,\n",
       "              0.0105,     -0.0173,     -0.0179,      0.0126,      0.0072,\n",
       "              0.0176,      0.0135,      0.0051,      0.0120,      0.0149,\n",
       "             -0.0089,     -0.0107,     -0.0057,     -0.0034,     -0.0112,\n",
       "              0.0118,      0.0028,      0.0137,      0.0099,      0.0161,\n",
       "              0.0028,      0.0029,     -0.0017,     -0.0167,     -0.0129,\n",
       "             -0.0121,     -0.0171,      0.0061,      0.0098,     -0.0086,\n",
       "             -0.0013,     -0.0121,     -0.0148,     -0.0060,      0.0138,\n",
       "              0.0153,      0.0142,      0.0113,     -0.0030,      0.0016,\n",
       "              0.0017,      0.0079,     -0.0023,     -0.0136,      0.0038,\n",
       "              0.0105,      0.0071,     -0.0114,     -0.0137,      0.0169,\n",
       "             -0.0173,      0.0160,     -0.0148,      0.0136,     -0.0022,\n",
       "              0.0023,     -0.0141,     -0.0111,      0.0146,      0.0155,\n",
       "              0.0048,     -0.0179,      0.0054,      0.0119,     -0.0101,\n",
       "              0.0055,     -0.0106,     -0.0174,      0.0068,      0.0070,\n",
       "             -0.0044,     -0.0134,      0.0072,     -0.0095,     -0.0153,\n",
       "             -0.0129,     -0.0016,     -0.0128,     -0.0126,     -0.0018,\n",
       "             -0.0105,     -0.0166,      0.0074,      0.0077,     -0.0180,\n",
       "             -0.0174,     -0.0139,      0.0100,     -0.0004,      0.0010,\n",
       "             -0.0056,     -0.0054,      0.0086,      0.0101,      0.0067,\n",
       "             -0.0109,      0.0008,     -0.0113,      0.0066,      0.0060,\n",
       "             -0.0068,      0.0150,      0.0021,     -0.0165,      0.0120,\n",
       "              0.0073,      0.0081,     -0.0172,     -0.0125,      0.0003,\n",
       "              0.0154,      0.0042,      0.0094,     -0.0092,     -0.0074,\n",
       "             -0.0071,      0.0015,      0.0085,      0.0097,      0.0177,\n",
       "              0.0165,     -0.0094,      0.0079,      0.0144,      0.0157,\n",
       "             -0.0156,      0.0122,      0.0133,     -0.0136,      0.0080,\n",
       "              0.0035,      0.0173,      0.0070,      0.0053,      0.0106,\n",
       "             -0.0180,     -0.0144,      0.0056,     -0.0009,     -0.0109,\n",
       "              0.0133,     -0.0049,     -0.0007,      0.0144,     -0.0034,\n",
       "              0.0080,     -0.0045,     -0.0107,      0.0129,     -0.0128,\n",
       "              0.0117,     -0.0052,      0.0168,      0.0176,      0.0122,\n",
       "              0.0013,     -0.0138,     -0.0022,     -0.0139,      0.0076,\n",
       "             -0.0119,      0.0040,     -0.0142,     -0.0013,      0.0089,\n",
       "              0.0179,      0.0099,     -0.0079,     -0.0100,      0.0062,\n",
       "              0.0165,     -0.0179,     -0.0147,      0.0053,      0.0024,\n",
       "             -0.0069,     -0.0030,     -0.0021,     -0.0016,      0.0138,\n",
       "              0.0121,      0.0115,      0.0156,     -0.0073,     -0.0050,\n",
       "             -0.0038,      0.0156,     -0.0079,     -0.0052,     -0.0180,\n",
       "              0.0162,     -0.0117,     -0.0148,      0.0066,     -0.0116,\n",
       "             -0.0174,      0.0120,     -0.0131,     -0.0033,      0.0075,\n",
       "             -0.0013,      0.0031,     -0.0148,     -0.0118,     -0.0060,\n",
       "             -0.0058,      0.0067,      0.0113,      0.0148,     -0.0101,\n",
       "             -0.0110,     -0.0108,      0.0089,     -0.0170,     -0.0067,\n",
       "             -0.0015,     -0.0056,     -0.0052,      0.0167,      0.0056,\n",
       "              0.0149,     -0.0017,     -0.0089,     -0.0177,      0.0130,\n",
       "              0.0180,      0.0059,     -0.0052,     -0.0108,     -0.0008,\n",
       "             -0.0152,     -0.0151,     -0.0056,     -0.0000,     -0.0170,\n",
       "              0.0111,     -0.0085,     -0.0079,      0.0069,      0.0124,\n",
       "             -0.0125,      0.0096,     -0.0095,      0.0047,     -0.0171,\n",
       "             -0.0118,     -0.0030,     -0.0148,      0.0123,     -0.0076,\n",
       "             -0.0131,      0.0098,      0.0056,     -0.0026,      0.0096,\n",
       "             -0.0047,      0.0020,      0.0146,     -0.0129,      0.0174,\n",
       "             -0.0108,     -0.0018,     -0.0171,     -0.0162,      0.0108,\n",
       "              0.0013,      0.0143,      0.0034,     -0.0022,     -0.0068,\n",
       "              0.0081,     -0.0100,     -0.0109,      0.0122,     -0.0081,\n",
       "              0.0118,      0.0103,      0.0027,      0.0049,      0.0011,\n",
       "              0.0053,     -0.0098,     -0.0135,     -0.0160,     -0.0119,\n",
       "             -0.0093,      0.0056,      0.0102,     -0.0047,      0.0058,\n",
       "              0.0125,     -0.0148,      0.0173,      0.0078,      0.0064,\n",
       "             -0.0114,      0.0130,      0.0158,      0.0113,     -0.0066,\n",
       "              0.0094,     -0.0045,      0.0124,      0.0173,     -0.0114,\n",
       "              0.0103,     -0.0047,      0.0108,     -0.0104,     -0.0082,\n",
       "             -0.0018,     -0.0039,     -0.0025,      0.0140,      0.0018,\n",
       "             -0.0060,      0.0114,     -0.0124,     -0.0170,     -0.0143,\n",
       "              0.0106,      0.0095,     -0.0113,     -0.0153,     -0.0045,\n",
       "             -0.0077,      0.0009,      0.0138,     -0.0053,     -0.0044,\n",
       "              0.0029,     -0.0071,     -0.0100,      0.0048,     -0.0096,\n",
       "             -0.0115,     -0.0173,      0.0054,      0.0099,      0.0172,\n",
       "             -0.0076,      0.0137,     -0.0073,     -0.0015,     -0.0049,\n",
       "              0.0013,     -0.0126,     -0.0012,     -0.0138,      0.0169,\n",
       "              0.0161,      0.0057,     -0.0006,      0.0038,     -0.0116,\n",
       "             -0.0078,     -0.0032,     -0.0022,     -0.0103,      0.0027,\n",
       "              0.0179,     -0.0158,      0.0003,      0.0085,     -0.0151,\n",
       "             -0.0097,     -0.0070,      0.0099,     -0.0139,     -0.0132,\n",
       "             -0.0065,     -0.0152,     -0.0109,      0.0006,      0.0161,\n",
       "             -0.0165,     -0.0063,      0.0095,     -0.0165,     -0.0139,\n",
       "              0.0081,     -0.0042,      0.0102,      0.0165,     -0.0071,\n",
       "              0.0115,     -0.0175,     -0.0119,      0.0052,     -0.0067,\n",
       "              0.0108,      0.0163,     -0.0106,      0.0058,      0.0020,\n",
       "              0.0058,     -0.0180,      0.0163,     -0.0136,      0.0147,\n",
       "             -0.0136,     -0.0124,      0.0135,      0.0152,      0.0044,\n",
       "             -0.0111,     -0.0175,      0.0094,      0.0098,      0.0015,\n",
       "             -0.0035,     -0.0084,     -0.0151,     -0.0083,     -0.0153,\n",
       "             -0.0063,     -0.0030,      0.0008,      0.0140,      0.0151,\n",
       "              0.0136,      0.0106,      0.0141,      0.0179,     -0.0161,\n",
       "             -0.0147,     -0.0115,      0.0128,      0.0046,      0.0038,\n",
       "             -0.0144,      0.0082,     -0.0022,      0.0148,      0.0031,\n",
       "             -0.0102,     -0.0067,     -0.0025,      0.0067,     -0.0049,\n",
       "              0.0121,     -0.0068,      0.0143,      0.0077,     -0.0114,\n",
       "              0.0014,     -0.0076,     -0.0057,      0.0007,      0.0136,\n",
       "             -0.0059,      0.0132,     -0.0109,      0.0063,      0.0052,\n",
       "             -0.0014,      0.0178,      0.0133,      0.0164,      0.0118,\n",
       "              0.0175,      0.0051,     -0.0095,     -0.0022,     -0.0042,\n",
       "             -0.0122,     -0.0073,      0.0135,      0.0125,     -0.0064,\n",
       "              0.0113,     -0.0105,     -0.0051,      0.0156,     -0.0147,\n",
       "             -0.0117,     -0.0063,     -0.0136,     -0.0051,     -0.0152,\n",
       "             -0.0078,      0.0117,     -0.0097,      0.0109,      0.0130,\n",
       "              0.0016,      0.0170,      0.0167,     -0.0113,      0.0043,\n",
       "              0.0043,      0.0177,     -0.0025,      0.0030,      0.0074,\n",
       "             -0.0129,     -0.0160,      0.0106,     -0.0087,      0.0109,\n",
       "             -0.0055,      0.0133,     -0.0156,      0.0064,      0.0109,\n",
       "             -0.0153,     -0.0020,      0.0144,     -0.0008,      0.0167,\n",
       "              0.0105,     -0.0141,      0.0075,      0.0045,      0.0060,\n",
       "              0.0086,     -0.0006,     -0.0053,      0.0114,      0.0052,\n",
       "              0.0075,      0.0150,     -0.0078,     -0.0125,     -0.0132,\n",
       "             -0.0173,      0.0017,     -0.0016,      0.0156,      0.0030,\n",
       "              0.0156,     -0.0074,      0.0135,     -0.0121,      0.0170,\n",
       "             -0.0139,     -0.0168,      0.0108,      0.0048,     -0.0085,\n",
       "              0.0097,     -0.0003,      0.0099,     -0.0174,     -0.0082,\n",
       "              0.0130,      0.0077,     -0.0111,     -0.0150,      0.0052,\n",
       "             -0.0004,      0.0180,      0.0082,     -0.0116,      0.0087,\n",
       "              0.0090,     -0.0025,     -0.0069,     -0.0083,      0.0073,\n",
       "             -0.0072,     -0.0123,      0.0129,     -0.0144,     -0.0177,\n",
       "             -0.0180,     -0.0121,      0.0074,      0.0098,     -0.0158,\n",
       "              0.0076,     -0.0082,     -0.0085,     -0.0128,     -0.0177,\n",
       "             -0.0176,     -0.0060,     -0.0030,      0.0120,     -0.0090,\n",
       "             -0.0140,     -0.0026,      0.0023,      0.0180,      0.0176,\n",
       "             -0.0091,     -0.0089,     -0.0177,      0.0077,      0.0125,\n",
       "             -0.0120,     -0.0131,      0.0149,     -0.0074,      0.0139,\n",
       "             -0.0153,      0.0113,      0.0081,     -0.0067,      0.0048,\n",
       "             -0.0180,      0.0026,      0.0145,     -0.0145,     -0.0072,\n",
       "             -0.0064,     -0.0132,      0.0092,     -0.0164,     -0.0172,\n",
       "             -0.0090,     -0.0084,      0.0155,      0.0012,     -0.0150,\n",
       "             -0.0023,      0.0106,     -0.0102,      0.0124,     -0.0078,\n",
       "              0.0069,     -0.0170,      0.0132,     -0.0037,      0.0151,\n",
       "             -0.0148,     -0.0059,     -0.0178], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0213,  0.0027,  0.0172,  ...,  0.0014, -0.0066,  0.0109],\n",
       "         [ 0.0081,  0.0340, -0.0361,  ...,  0.0279,  0.0065, -0.0247],\n",
       "         [-0.0271,  0.0315,  0.0349,  ..., -0.0207, -0.0067, -0.0296],\n",
       "         ...,\n",
       "         [-0.0264, -0.0256,  0.0302,  ..., -0.0252, -0.0011,  0.0333],\n",
       "         [-0.0122, -0.0239,  0.0169,  ...,  0.0251, -0.0029,  0.0355],\n",
       "         [-0.0003, -0.0270, -0.0290,  ..., -0.0308, -0.0188, -0.0052]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0325,  0.0330,  0.0041,  ..., -0.0066, -0.0020, -0.0258],\n",
       "         [-0.0232, -0.0032,  0.0340,  ...,  0.0127,  0.0109, -0.0010],\n",
       "         [ 0.0008,  0.0049, -0.0192,  ..., -0.0187,  0.0321, -0.0335],\n",
       "         ...,\n",
       "         [ 0.0355, -0.0141, -0.0288,  ...,  0.0162, -0.0104,  0.0008],\n",
       "         [ 0.0134,  0.0264,  0.0326,  ..., -0.0338, -0.0252,  0.0339],\n",
       "         [ 0.0254,  0.0200,  0.0316,  ..., -0.0011,  0.0264, -0.0136]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0207, -0.0129,  0.0185,  ..., -0.0328, -0.0153, -0.0070],\n",
       "         [-0.0215,  0.0010, -0.0284,  ..., -0.0277, -0.0094, -0.0026],\n",
       "         [-0.0223, -0.0333, -0.0243,  ...,  0.0141,  0.0289,  0.0163],\n",
       "         ...,\n",
       "         [-0.0144,  0.0097, -0.0038,  ..., -0.0083, -0.0139,  0.0299],\n",
       "         [-0.0188, -0.0092,  0.0335,  ..., -0.0305,  0.0108,  0.0261],\n",
       "         [ 0.0353,  0.0178, -0.0082,  ...,  0.0151,  0.0115, -0.0162]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0010, -0.0057,  0.0169,  ..., -0.0299,  0.0297, -0.0210],\n",
       "         [ 0.0032, -0.0109,  0.0035,  ...,  0.0075,  0.0216,  0.0146],\n",
       "         [ 0.0312, -0.0219, -0.0158,  ...,  0.0359, -0.0185, -0.0101],\n",
       "         ...,\n",
       "         [-0.0275,  0.0338, -0.0111,  ...,  0.0063, -0.0157, -0.0243],\n",
       "         [-0.0239, -0.0120, -0.0153,  ..., -0.0236,  0.0038,  0.0037],\n",
       "         [-0.0305, -0.0315,  0.0303,  ..., -0.0137, -0.0048, -0.0314]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([    -0.0115,      0.0316,     -0.0080,     -0.0045,      0.0075,\n",
       "              0.0195,     -0.0255,      0.0245,      0.0034,     -0.0269,\n",
       "             -0.0051,      0.0039,      0.0225,     -0.0345,      0.0340,\n",
       "              0.0023,     -0.0125,      0.0278,     -0.0153,      0.0164,\n",
       "              0.0255,     -0.0264,     -0.0211,     -0.0079,      0.0337,\n",
       "              0.0034,     -0.0229,      0.0012,      0.0171,     -0.0118,\n",
       "              0.0210,      0.0120,     -0.0050,      0.0087,      0.0258,\n",
       "             -0.0222,     -0.0282,     -0.0028,     -0.0186,      0.0019,\n",
       "              0.0351,     -0.0355,     -0.0297,      0.0254,      0.0123,\n",
       "             -0.0002,     -0.0260,     -0.0030,      0.0217,     -0.0083,\n",
       "             -0.0157,      0.0077,      0.0074,     -0.0295,     -0.0342,\n",
       "              0.0315,      0.0212,     -0.0266,     -0.0017,     -0.0258,\n",
       "              0.0180,     -0.0274,     -0.0311,      0.0225,     -0.0151,\n",
       "             -0.0170,      0.0152,     -0.0284,     -0.0124,     -0.0022,\n",
       "              0.0113,      0.0120,      0.0248,      0.0053,     -0.0001,\n",
       "             -0.0273,     -0.0270,      0.0297,     -0.0237,      0.0121,\n",
       "             -0.0034,     -0.0090,     -0.0101,     -0.0208,     -0.0315,\n",
       "             -0.0066,      0.0209,     -0.0161,     -0.0331,      0.0107,\n",
       "             -0.0004,     -0.0109,      0.0135,      0.0189,      0.0180,\n",
       "             -0.0094,      0.0058,      0.0032,     -0.0156,      0.0292,\n",
       "             -0.0221,      0.0335,     -0.0168,     -0.0110,      0.0203,\n",
       "              0.0313,      0.0257,      0.0280,      0.0076,     -0.0072,\n",
       "             -0.0075,      0.0044,      0.0352,     -0.0340,     -0.0216,\n",
       "             -0.0235,     -0.0093,      0.0244,     -0.0253,     -0.0189,\n",
       "              0.0052,     -0.0175,      0.0278,      0.0088,      0.0130,\n",
       "              0.0354,      0.0197,      0.0122,     -0.0247,     -0.0161,\n",
       "              0.0340,     -0.0011,     -0.0217,      0.0105,     -0.0103,\n",
       "              0.0107,      0.0323,      0.0244,     -0.0221,     -0.0039,\n",
       "              0.0062,     -0.0157,     -0.0271,      0.0342,      0.0053,\n",
       "              0.0177,      0.0299,      0.0353,      0.0348,     -0.0142,\n",
       "              0.0238,     -0.0353,      0.0285,     -0.0300,     -0.0346,\n",
       "             -0.0138,     -0.0166,     -0.0312,     -0.0333,     -0.0143,\n",
       "             -0.0119,      0.0163,      0.0110,      0.0229,      0.0349,\n",
       "              0.0082,     -0.0236,     -0.0212,     -0.0233,      0.0100,\n",
       "             -0.0285,      0.0045,      0.0045,      0.0318,     -0.0141,\n",
       "             -0.0223,      0.0008,      0.0113,      0.0228,      0.0238,\n",
       "              0.0187,     -0.0320,     -0.0352,      0.0119,     -0.0302,\n",
       "              0.0062,      0.0219,      0.0026,     -0.0292,     -0.0327,\n",
       "             -0.0217,     -0.0208,     -0.0313,      0.0278,     -0.0127,\n",
       "             -0.0174,     -0.0034,      0.0348,      0.0066,      0.0129,\n",
       "              0.0164,      0.0205,     -0.0095,     -0.0066,     -0.0316,\n",
       "             -0.0225,      0.0337,     -0.0145,     -0.0125,      0.0049,\n",
       "             -0.0085,     -0.0084,     -0.0268,     -0.0034,      0.0110,\n",
       "              0.0120,     -0.0181,      0.0251,      0.0072,     -0.0317,\n",
       "              0.0298,      0.0218,     -0.0294,     -0.0168,      0.0198,\n",
       "              0.0144,      0.0358,      0.0361,      0.0064,     -0.0012,\n",
       "              0.0189,      0.0043,     -0.0115,     -0.0016,      0.0305,\n",
       "              0.0312,      0.0315,     -0.0217,      0.0093,      0.0117,\n",
       "              0.0127,     -0.0037,      0.0230,     -0.0078,     -0.0218,\n",
       "             -0.0328,      0.0160,      0.0167,      0.0047,     -0.0189,\n",
       "             -0.0182,      0.0226,      0.0337,      0.0166,     -0.0234,\n",
       "              0.0023,      0.0221,     -0.0268,      0.0204,      0.0029,\n",
       "              0.0074,     -0.0114,     -0.0066,      0.0119,     -0.0277,\n",
       "             -0.0163,      0.0006,     -0.0260,     -0.0139,     -0.0219,\n",
       "              0.0066,     -0.0205,     -0.0316,     -0.0320,      0.0092,\n",
       "             -0.0276,      0.0165,      0.0102,      0.0198,     -0.0186,\n",
       "              0.0308,     -0.0086,     -0.0293,      0.0297,     -0.0076,\n",
       "             -0.0301,     -0.0128,      0.0258,     -0.0036,      0.0175,\n",
       "             -0.0319,      0.0085,     -0.0097,     -0.0061,     -0.0341,\n",
       "              0.0043,     -0.0019,     -0.0301,      0.0179,      0.0280,\n",
       "             -0.0070,     -0.0193,      0.0152,     -0.0145,     -0.0032,\n",
       "             -0.0078,      0.0081,      0.0268,     -0.0029,      0.0060,\n",
       "             -0.0246,     -0.0094,     -0.0310,      0.0228,     -0.0141,\n",
       "             -0.0121,     -0.0262,     -0.0316,     -0.0174,      0.0198,\n",
       "             -0.0058,      0.0308,      0.0262,      0.0336,     -0.0293,\n",
       "             -0.0150,     -0.0220,      0.0247,     -0.0195,      0.0353,\n",
       "             -0.0157,     -0.0141,     -0.0218,     -0.0142,     -0.0022,\n",
       "             -0.0295,     -0.0256,     -0.0302,      0.0295,     -0.0039,\n",
       "             -0.0306,     -0.0104,      0.0076,      0.0180,      0.0170,\n",
       "              0.0225,     -0.0119,      0.0130,     -0.0103,      0.0254,\n",
       "              0.0119,     -0.0347,      0.0152,     -0.0260,      0.0064,\n",
       "             -0.0286,      0.0155,      0.0335,     -0.0083,     -0.0315,\n",
       "              0.0226,      0.0119,     -0.0087,      0.0090,     -0.0162,\n",
       "              0.0290,      0.0292,     -0.0307,     -0.0214,     -0.0132,\n",
       "              0.0036,     -0.0205,      0.0104,      0.0066,      0.0208,\n",
       "              0.0056,      0.0103,     -0.0123,      0.0269,     -0.0158,\n",
       "             -0.0219,      0.0313,     -0.0200,     -0.0246,      0.0179,\n",
       "             -0.0233,      0.0313,     -0.0085,     -0.0088,     -0.0320,\n",
       "              0.0073,     -0.0289,      0.0304,      0.0114,     -0.0273,\n",
       "             -0.0289,     -0.0070,     -0.0038,     -0.0316,      0.0192,\n",
       "             -0.0094,      0.0136,     -0.0095,     -0.0129,      0.0175,\n",
       "              0.0196,     -0.0073,     -0.0006,     -0.0318,      0.0150,\n",
       "             -0.0293,      0.0271,     -0.0344,      0.0073,      0.0118,\n",
       "             -0.0331,     -0.0249,      0.0022,     -0.0343,     -0.0263,\n",
       "              0.0074,     -0.0246,      0.0098,     -0.0226,      0.0062,\n",
       "             -0.0276,     -0.0310,     -0.0045,     -0.0305,     -0.0350,\n",
       "             -0.0155,     -0.0019,     -0.0116,      0.0018,     -0.0216,\n",
       "             -0.0083,      0.0356,      0.0340,      0.0196,     -0.0130,\n",
       "              0.0349,     -0.0090,      0.0265,     -0.0202,      0.0069,\n",
       "             -0.0216,      0.0136,      0.0010,      0.0082,      0.0170,\n",
       "              0.0187,      0.0323,      0.0113,     -0.0092,     -0.0177,\n",
       "             -0.0281,     -0.0042,     -0.0074,      0.0129,     -0.0176,\n",
       "              0.0353,      0.0294,     -0.0354,      0.0287,      0.0127,\n",
       "             -0.0346,      0.0208,     -0.0099,      0.0238,      0.0328,\n",
       "             -0.0040,     -0.0069,      0.0188,     -0.0051,     -0.0298,\n",
       "              0.0216,      0.0181,      0.0057,     -0.0179,     -0.0195,\n",
       "              0.0167,      0.0082,     -0.0008,      0.0020,     -0.0008,\n",
       "              0.0130,     -0.0082,     -0.0315,      0.0291,      0.0043,\n",
       "             -0.0102,      0.0175,     -0.0189,     -0.0064,      0.0118,\n",
       "              0.0298,      0.0269,     -0.0011,      0.0195,     -0.0133,\n",
       "             -0.0089,     -0.0206,     -0.0168,     -0.0339,     -0.0023,\n",
       "             -0.0233,      0.0005,      0.0112,      0.0047,      0.0236,\n",
       "              0.0169,     -0.0195,     -0.0110,     -0.0330,      0.0316,\n",
       "             -0.0325,      0.0071,     -0.0072,      0.0258,     -0.0155,\n",
       "              0.0014,      0.0242,      0.0082,      0.0011,     -0.0219,\n",
       "             -0.0354,     -0.0349,     -0.0328,      0.0163,     -0.0016,\n",
       "             -0.0168,      0.0230,     -0.0220,     -0.0291,     -0.0150,\n",
       "             -0.0129,     -0.0053,     -0.0050,     -0.0307,      0.0312,\n",
       "              0.0023,     -0.0325,      0.0047,      0.0060,     -0.0172,\n",
       "             -0.0120,     -0.0098,      0.0069,     -0.0320,      0.0111,\n",
       "              0.0230,     -0.0285,     -0.0341,      0.0113,      0.0031,\n",
       "              0.0356,     -0.0272,     -0.0096,     -0.0230,     -0.0018,\n",
       "             -0.0352,      0.0187,      0.0313,      0.0324,     -0.0287,\n",
       "             -0.0301,     -0.0126,     -0.0221,     -0.0220,      0.0208,\n",
       "              0.0251,     -0.0141,     -0.0240,     -0.0235,      0.0129,\n",
       "              0.0234,     -0.0127,     -0.0128,     -0.0080,     -0.0335,\n",
       "              0.0159,     -0.0161,      0.0124,      0.0140,      0.0239,\n",
       "              0.0264,      0.0235,      0.0229,      0.0322,     -0.0141,\n",
       "             -0.0135,     -0.0033,      0.0331,     -0.0042,     -0.0144,\n",
       "              0.0128,      0.0324,      0.0026,     -0.0337,     -0.0100,\n",
       "              0.0100,      0.0259,      0.0227,      0.0360,     -0.0146,\n",
       "              0.0155,      0.0314,     -0.0351,     -0.0039,     -0.0237,\n",
       "              0.0102,      0.0192,     -0.0341,      0.0271,      0.0103,\n",
       "              0.0016,     -0.0033,      0.0241,      0.0187,     -0.0140,\n",
       "             -0.0146,      0.0141,      0.0197,      0.0034,      0.0320,\n",
       "             -0.0304,     -0.0332,      0.0190,     -0.0049,     -0.0278,\n",
       "              0.0036,     -0.0033,     -0.0043,      0.0011,     -0.0156,\n",
       "             -0.0009,      0.0226,      0.0157,     -0.0041,      0.0350,\n",
       "             -0.0225,      0.0174,      0.0045,      0.0222,     -0.0122,\n",
       "              0.0090,      0.0211,     -0.0302,     -0.0217,      0.0316,\n",
       "             -0.0210,      0.0164,     -0.0152,      0.0036,     -0.0096,\n",
       "              0.0319,      0.0141,     -0.0315,     -0.0157,     -0.0027,\n",
       "             -0.0025,      0.0312,     -0.0049,     -0.0161,      0.0342,\n",
       "             -0.0118,      0.0277,     -0.0321,     -0.0096,     -0.0279,\n",
       "              0.0047,      0.0119,     -0.0233,     -0.0133,     -0.0153,\n",
       "             -0.0066,     -0.0207,     -0.0133,     -0.0113,     -0.0287,\n",
       "             -0.0127,     -0.0263,     -0.0342,      0.0157,      0.0264,\n",
       "             -0.0005,     -0.0160,      0.0253,     -0.0228,      0.0040,\n",
       "             -0.0030,     -0.0004,     -0.0299,     -0.0061,     -0.0147,\n",
       "              0.0338,      0.0266,     -0.0168,      0.0183,      0.0212,\n",
       "             -0.0188,     -0.0030,     -0.0274,     -0.0281,     -0.0333,\n",
       "              0.0157,     -0.0009,     -0.0230,     -0.0302,      0.0172,\n",
       "              0.0170,      0.0082,     -0.0347,      0.0205,      0.0034,\n",
       "              0.0139,      0.0056,      0.0041,      0.0182,     -0.0062,\n",
       "              0.0158,      0.0271,      0.0050,      0.0011,      0.0005,\n",
       "             -0.0265,      0.0148,     -0.0018,      0.0052,     -0.0126,\n",
       "             -0.0188,     -0.0183,     -0.0150,      0.0141,     -0.0257,\n",
       "              0.0089,     -0.0089,     -0.0057,     -0.0321,     -0.0006,\n",
       "             -0.0277,     -0.0345,      0.0221,     -0.0054,      0.0045,\n",
       "             -0.0165,      0.0231,     -0.0165,     -0.0158,      0.0110,\n",
       "             -0.0021,     -0.0198,      0.0178,      0.0043,     -0.0073,\n",
       "             -0.0360,     -0.0201,      0.0252,     -0.0162,      0.0255,\n",
       "              0.0300,     -0.0250,     -0.0016,      0.0252,     -0.0169,\n",
       "              0.0019,      0.0343,     -0.0231], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0247,  0.0340, -0.0347,  ...,  0.0326, -0.0042, -0.0165],\n",
       "         [ 0.0314, -0.0122,  0.0102,  ...,  0.0333, -0.0310, -0.0167],\n",
       "         [-0.0016,  0.0296, -0.0205,  ...,  0.0085, -0.0236,  0.0039],\n",
       "         ...,\n",
       "         [ 0.0030,  0.0302, -0.0112,  ..., -0.0174, -0.0244, -0.0165],\n",
       "         [ 0.0168, -0.0350, -0.0305,  ...,  0.0121, -0.0277, -0.0027],\n",
       "         [ 0.0273, -0.0178, -0.0227,  ...,  0.0290,  0.0136, -0.0029]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0030,  0.0198, -0.0112,  ...,  0.0034, -0.0330,  0.0187],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0118,  0.0020,  0.0058,  ...,  0.0123, -0.0051, -0.0076],\n",
       "         [ 0.0160,  0.0046,  0.0104,  ..., -0.0038, -0.0144,  0.0091],\n",
       "         [-0.0118,  0.0145, -0.0136,  ...,  0.0011, -0.0175,  0.0033],\n",
       "         ...,\n",
       "         [ 0.0012,  0.0074,  0.0097,  ..., -0.0063,  0.0023,  0.0093],\n",
       "         [-0.0020, -0.0127,  0.0154,  ...,  0.0176, -0.0095, -0.0099],\n",
       "         [-0.0117, -0.0175,  0.0141,  ...,  0.0107,  0.0152, -0.0159]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([     0.0036,     -0.0141,     -0.0134,     -0.0011,     -0.0045,\n",
       "             -0.0099,      0.0001,      0.0042,      0.0123,     -0.0097,\n",
       "              0.0045,     -0.0042,     -0.0173,      0.0076,      0.0100,\n",
       "              0.0084,     -0.0046,     -0.0071,      0.0013,     -0.0148,\n",
       "              0.0078,      0.0090,      0.0082,     -0.0154,     -0.0060,\n",
       "             -0.0079,     -0.0036,     -0.0171,      0.0136,      0.0049,\n",
       "             -0.0067,      0.0145,      0.0110,     -0.0061,      0.0052,\n",
       "              0.0098,     -0.0154,      0.0024,     -0.0111,     -0.0130,\n",
       "              0.0048,      0.0097,     -0.0042,      0.0151,      0.0009,\n",
       "             -0.0099,      0.0109,      0.0108,     -0.0143,      0.0054,\n",
       "             -0.0177,      0.0035,      0.0028,      0.0138,     -0.0001,\n",
       "             -0.0039,     -0.0027,     -0.0075,      0.0145,     -0.0135,\n",
       "              0.0169,     -0.0013,     -0.0098,      0.0041,      0.0075,\n",
       "              0.0119,      0.0155,      0.0003,     -0.0044,      0.0119,\n",
       "             -0.0147,      0.0159,      0.0080,      0.0073,     -0.0051,\n",
       "              0.0053,     -0.0014,     -0.0003,      0.0038,      0.0155,\n",
       "              0.0105,     -0.0133,     -0.0038,      0.0088,     -0.0174,\n",
       "              0.0169,      0.0067,      0.0157,     -0.0030,      0.0075,\n",
       "              0.0108,     -0.0031,     -0.0157,      0.0059,      0.0090,\n",
       "             -0.0039,      0.0114,      0.0043,     -0.0147,     -0.0126,\n",
       "              0.0083,      0.0049,      0.0103,      0.0047,      0.0062,\n",
       "             -0.0002,     -0.0035,     -0.0123,     -0.0040,      0.0082,\n",
       "             -0.0036,      0.0002,      0.0067,      0.0088,      0.0172,\n",
       "             -0.0091,      0.0109,      0.0024,      0.0178,      0.0136,\n",
       "             -0.0060,     -0.0162,     -0.0156,     -0.0022,     -0.0142,\n",
       "             -0.0007,      0.0068,     -0.0091,     -0.0166,      0.0154,\n",
       "              0.0055,     -0.0066,     -0.0087,      0.0166,     -0.0133,\n",
       "             -0.0003,      0.0068,      0.0051,     -0.0112,     -0.0064,\n",
       "             -0.0014,      0.0064,      0.0033,     -0.0092,     -0.0004,\n",
       "              0.0093,      0.0153,     -0.0081,     -0.0007,     -0.0066,\n",
       "             -0.0151,      0.0026,      0.0067,      0.0005,     -0.0073,\n",
       "              0.0170,      0.0089,      0.0174,     -0.0040,      0.0071,\n",
       "              0.0018,      0.0071,      0.0105,      0.0056,     -0.0103,\n",
       "             -0.0095,     -0.0070,      0.0046,     -0.0147,      0.0094,\n",
       "             -0.0063,      0.0048,      0.0114,      0.0028,      0.0010,\n",
       "              0.0039,      0.0093,     -0.0070,     -0.0004,      0.0030,\n",
       "              0.0044,     -0.0073,     -0.0175,      0.0180,      0.0170,\n",
       "              0.0069,     -0.0057,      0.0157,     -0.0046,     -0.0134,\n",
       "              0.0078,     -0.0154,      0.0004,     -0.0120,     -0.0019,\n",
       "             -0.0061,     -0.0164,     -0.0036,      0.0117,      0.0176,\n",
       "             -0.0039,     -0.0055,      0.0024,     -0.0142,      0.0125,\n",
       "             -0.0031,     -0.0070,      0.0083,     -0.0033,     -0.0157,\n",
       "              0.0177,     -0.0046,     -0.0058,      0.0131,      0.0149,\n",
       "              0.0016,     -0.0149,     -0.0148,      0.0159,      0.0105,\n",
       "              0.0165,     -0.0179,      0.0047,     -0.0106,     -0.0066,\n",
       "             -0.0159,      0.0117,     -0.0023,     -0.0113,      0.0029,\n",
       "             -0.0112,     -0.0049,      0.0121,      0.0167,     -0.0094,\n",
       "              0.0003,      0.0133,     -0.0011,     -0.0169,     -0.0140,\n",
       "             -0.0034,      0.0156,     -0.0033,      0.0074,     -0.0069,\n",
       "              0.0104,     -0.0123,     -0.0105,      0.0029,      0.0077,\n",
       "              0.0075,     -0.0048,     -0.0151,     -0.0143,      0.0039,\n",
       "             -0.0026,      0.0090,      0.0118,     -0.0050,     -0.0077,\n",
       "              0.0148,     -0.0054,      0.0074,      0.0028,      0.0008,\n",
       "             -0.0007,      0.0100,     -0.0003,      0.0138,     -0.0117,\n",
       "             -0.0079,      0.0025,     -0.0126,     -0.0113,     -0.0006,\n",
       "              0.0078,     -0.0156,     -0.0026,     -0.0067,      0.0106,\n",
       "              0.0140,     -0.0102,     -0.0132,      0.0058,     -0.0076,\n",
       "              0.0149,      0.0044,     -0.0139,      0.0063,     -0.0152,\n",
       "             -0.0013,      0.0002,     -0.0129,     -0.0064,     -0.0008,\n",
       "             -0.0112,      0.0169,      0.0103,      0.0173,      0.0133,\n",
       "              0.0178,      0.0057,     -0.0144,      0.0025,     -0.0130,\n",
       "              0.0024,     -0.0076,      0.0094,     -0.0066,     -0.0064,\n",
       "             -0.0031,      0.0071,      0.0153,      0.0118,     -0.0080,\n",
       "              0.0168,      0.0002,     -0.0090,     -0.0164,      0.0010,\n",
       "              0.0059,     -0.0173,     -0.0015,      0.0102,     -0.0025,\n",
       "              0.0129,      0.0039,      0.0021,     -0.0178,      0.0128,\n",
       "             -0.0070,      0.0114,     -0.0045,     -0.0019,      0.0121,\n",
       "             -0.0107,      0.0073,      0.0110,      0.0147,     -0.0071,\n",
       "              0.0165,     -0.0095,     -0.0166,      0.0156,     -0.0128,\n",
       "             -0.0097,     -0.0078,     -0.0153,      0.0174,     -0.0154,\n",
       "             -0.0153,      0.0117,     -0.0085,     -0.0096,      0.0061,\n",
       "              0.0122,     -0.0151,     -0.0143,      0.0097,      0.0119,\n",
       "              0.0048,     -0.0102,      0.0129,      0.0123,      0.0135,\n",
       "             -0.0106,     -0.0179,      0.0000,     -0.0158,     -0.0049,\n",
       "              0.0072,      0.0031,      0.0073,     -0.0154,     -0.0071,\n",
       "              0.0047,     -0.0096,     -0.0042,      0.0018,      0.0104,\n",
       "             -0.0105,     -0.0179,      0.0130,     -0.0097,      0.0087,\n",
       "              0.0095,     -0.0009,     -0.0071,     -0.0127,     -0.0104,\n",
       "              0.0096,      0.0162,      0.0014,      0.0011,     -0.0147,\n",
       "             -0.0119,     -0.0136,      0.0002,      0.0064,     -0.0008,\n",
       "              0.0044,      0.0017,      0.0069,     -0.0088,      0.0102,\n",
       "              0.0093,     -0.0168,     -0.0008,     -0.0136,      0.0076,\n",
       "             -0.0023,      0.0005,     -0.0062,      0.0079,     -0.0035,\n",
       "              0.0036,      0.0169,     -0.0105,      0.0120,      0.0028,\n",
       "             -0.0054,      0.0006,     -0.0025,      0.0085,     -0.0060,\n",
       "              0.0014,     -0.0110,     -0.0091,      0.0164,      0.0133,\n",
       "              0.0138,      0.0017,     -0.0030,     -0.0047,      0.0032,\n",
       "              0.0166,      0.0106,     -0.0141,     -0.0013,     -0.0079,\n",
       "             -0.0106,      0.0077,     -0.0097,     -0.0095,      0.0144,\n",
       "              0.0032,     -0.0150,     -0.0017,     -0.0044,      0.0118,\n",
       "              0.0118,      0.0006,      0.0029,      0.0050,      0.0047,\n",
       "             -0.0112,     -0.0057,     -0.0042,     -0.0027,      0.0177,\n",
       "              0.0146,     -0.0133,      0.0035,     -0.0129,      0.0023,\n",
       "             -0.0003,     -0.0079,     -0.0091,      0.0103,      0.0045,\n",
       "              0.0050,     -0.0036,      0.0111,      0.0095,      0.0050,\n",
       "             -0.0028,     -0.0101,     -0.0068,     -0.0009,     -0.0111,\n",
       "              0.0050,     -0.0037,      0.0108,     -0.0027,     -0.0047,\n",
       "             -0.0004,      0.0130,     -0.0098,     -0.0173,      0.0008,\n",
       "              0.0125,     -0.0105,     -0.0017,     -0.0032,      0.0054,\n",
       "             -0.0140,      0.0011,     -0.0070,     -0.0079,     -0.0138,\n",
       "              0.0100,      0.0085,     -0.0056,      0.0056,     -0.0125,\n",
       "              0.0086,     -0.0164,      0.0093,      0.0021,     -0.0028,\n",
       "             -0.0042,     -0.0044,      0.0175,      0.0021,      0.0180,\n",
       "              0.0171,     -0.0147,     -0.0059,     -0.0009,      0.0111,\n",
       "              0.0093,      0.0037,     -0.0087,      0.0024,      0.0051,\n",
       "              0.0006,      0.0147,     -0.0107,     -0.0143,      0.0029,\n",
       "             -0.0137,      0.0041,      0.0159,      0.0112,     -0.0069,\n",
       "              0.0046,     -0.0175,      0.0100,      0.0099,      0.0039,\n",
       "              0.0077,      0.0039,     -0.0105,      0.0054,      0.0037,\n",
       "             -0.0008,      0.0151,     -0.0177,      0.0034,     -0.0121,\n",
       "             -0.0142,      0.0028,     -0.0177,      0.0133,     -0.0078,\n",
       "             -0.0104,      0.0013,     -0.0035,      0.0064,      0.0028,\n",
       "             -0.0046,      0.0063,      0.0140,     -0.0105,     -0.0111,\n",
       "             -0.0135,      0.0052,      0.0009,     -0.0033,      0.0074,\n",
       "              0.0008,      0.0046,     -0.0099,      0.0091,     -0.0079,\n",
       "              0.0045,      0.0087,      0.0148,      0.0081,     -0.0095,\n",
       "             -0.0168,      0.0033,     -0.0162,      0.0011,     -0.0156,\n",
       "             -0.0046,     -0.0079,     -0.0028,     -0.0067,     -0.0148,\n",
       "              0.0128,      0.0081,     -0.0175,      0.0105,     -0.0127,\n",
       "              0.0082,     -0.0104,     -0.0128,      0.0156,      0.0138,\n",
       "             -0.0062,      0.0022,     -0.0027,      0.0074,      0.0114,\n",
       "              0.0083,     -0.0162,     -0.0107,     -0.0111,     -0.0016,\n",
       "             -0.0176,     -0.0142,     -0.0103,     -0.0122,      0.0110,\n",
       "             -0.0004,     -0.0034,     -0.0156,     -0.0149,      0.0125,\n",
       "              0.0112,      0.0085,     -0.0050,     -0.0171,     -0.0117,\n",
       "             -0.0155,     -0.0071,     -0.0180,     -0.0053,      0.0127,\n",
       "             -0.0087,     -0.0094,     -0.0024,      0.0152,      0.0000,\n",
       "             -0.0146,      0.0172,     -0.0056,      0.0137,     -0.0101,\n",
       "             -0.0036,     -0.0008,     -0.0064,      0.0001,     -0.0042,\n",
       "             -0.0109,      0.0034,      0.0128,      0.0017,     -0.0034,\n",
       "             -0.0132,     -0.0063,      0.0110,      0.0144,      0.0167,\n",
       "              0.0054,     -0.0102,     -0.0027,     -0.0061,     -0.0014,\n",
       "             -0.0151,      0.0147,      0.0103,      0.0072,     -0.0020,\n",
       "             -0.0013,     -0.0006,     -0.0005,      0.0064,     -0.0025,\n",
       "             -0.0124,     -0.0124,     -0.0085,      0.0113,      0.0087,\n",
       "              0.0162,      0.0072,     -0.0141,     -0.0052,     -0.0163,\n",
       "             -0.0035,      0.0163,     -0.0005,      0.0077,     -0.0112,\n",
       "              0.0083,      0.0069,     -0.0132,      0.0111,      0.0062,\n",
       "              0.0171,     -0.0043,      0.0002,      0.0134,     -0.0172,\n",
       "              0.0019,      0.0109,      0.0020,     -0.0179,      0.0179,\n",
       "              0.0008,     -0.0060,     -0.0010,     -0.0079,     -0.0170,\n",
       "             -0.0171,      0.0037,     -0.0156,      0.0022,      0.0015,\n",
       "              0.0068,      0.0063,     -0.0117,     -0.0078,     -0.0098,\n",
       "             -0.0175,     -0.0065,     -0.0026,     -0.0078,      0.0077,\n",
       "              0.0049,     -0.0100,      0.0023,      0.0002,     -0.0139,\n",
       "             -0.0118,      0.0014,      0.0163,      0.0042,      0.0106,\n",
       "              0.0004,      0.0090,      0.0174,      0.0104,      0.0121,\n",
       "              0.0123,      0.0072,     -0.0024,      0.0108,      0.0087,\n",
       "              0.0141,      0.0065,      0.0017,      0.0116,      0.0090,\n",
       "              0.0113,      0.0094,     -0.0035,      0.0015,      0.0180,\n",
       "              0.0137,      0.0124,     -0.0097,      0.0118,     -0.0031,\n",
       "              0.0083,     -0.0019,      0.0088,      0.0084,     -0.0010,\n",
       "              0.0025,     -0.0118,     -0.0006,      0.0052,     -0.0128,\n",
       "             -0.0130,     -0.0114,     -0.0136], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0190, -0.0193, -0.0233,  ...,  0.0253,  0.0098, -0.0309],\n",
       "         [-0.0252,  0.0194, -0.0096,  ...,  0.0095,  0.0345, -0.0299],\n",
       "         [-0.0291,  0.0097,  0.0154,  ...,  0.0138,  0.0003, -0.0248],\n",
       "         ...,\n",
       "         [ 0.0222,  0.0164, -0.0010,  ..., -0.0102, -0.0110,  0.0323],\n",
       "         [-0.0170,  0.0222, -0.0201,  ...,  0.0042,  0.0037, -0.0028],\n",
       "         [-0.0015, -0.0276, -0.0253,  ...,  0.0151,  0.0032, -0.0002]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0296,  0.0335, -0.0281,  ..., -0.0228,  0.0355, -0.0332],\n",
       "         [-0.0250, -0.0164, -0.0123,  ...,  0.0192,  0.0241, -0.0191],\n",
       "         [ 0.0146,  0.0319,  0.0089,  ..., -0.0091,  0.0321,  0.0150],\n",
       "         ...,\n",
       "         [-0.0237,  0.0295,  0.0036,  ...,  0.0104, -0.0277, -0.0122],\n",
       "         [-0.0278, -0.0081,  0.0326,  ...,  0.0319, -0.0008, -0.0055],\n",
       "         [-0.0104, -0.0319,  0.0327,  ...,  0.0220,  0.0036,  0.0009]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0011, -0.0254,  0.0096,  ..., -0.0057,  0.0033, -0.0025],\n",
       "         [ 0.0136, -0.0162, -0.0133,  ...,  0.0213, -0.0019,  0.0080],\n",
       "         [-0.0192, -0.0134,  0.0069,  ...,  0.0020, -0.0263,  0.0114],\n",
       "         ...,\n",
       "         [-0.0359, -0.0139, -0.0262,  ..., -0.0284, -0.0178, -0.0296],\n",
       "         [-0.0288,  0.0111, -0.0151,  ...,  0.0076, -0.0158, -0.0034],\n",
       "         [-0.0271, -0.0255, -0.0182,  ...,  0.0087,  0.0207,  0.0213]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0343, -0.0092, -0.0183,  ..., -0.0137, -0.0231, -0.0046],\n",
       "         [ 0.0128, -0.0352, -0.0110,  ..., -0.0336, -0.0274, -0.0047],\n",
       "         [ 0.0091, -0.0357, -0.0153,  ..., -0.0161, -0.0177,  0.0179],\n",
       "         ...,\n",
       "         [ 0.0294, -0.0147, -0.0246,  ..., -0.0004,  0.0289, -0.0224],\n",
       "         [ 0.0028, -0.0222, -0.0099,  ...,  0.0099,  0.0169, -0.0124],\n",
       "         [-0.0159,  0.0072,  0.0295,  ..., -0.0127,  0.0297, -0.0124]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([     0.0004,     -0.0277,     -0.0273,      0.0035,      0.0062,\n",
       "             -0.0288,      0.0086,     -0.0042,      0.0021,      0.0116,\n",
       "              0.0294,      0.0157,     -0.0167,     -0.0220,     -0.0321,\n",
       "             -0.0013,      0.0082,     -0.0213,     -0.0110,      0.0353,\n",
       "             -0.0187,      0.0130,      0.0162,      0.0022,      0.0304,\n",
       "              0.0137,     -0.0168,     -0.0026,     -0.0337,      0.0123,\n",
       "              0.0209,     -0.0031,      0.0038,     -0.0201,     -0.0102,\n",
       "              0.0264,      0.0105,      0.0230,      0.0263,      0.0314,\n",
       "             -0.0238,      0.0176,      0.0004,     -0.0033,     -0.0137,\n",
       "             -0.0030,     -0.0255,     -0.0176,     -0.0068,      0.0137,\n",
       "             -0.0066,      0.0128,      0.0321,     -0.0072,     -0.0278,\n",
       "              0.0171,      0.0087,     -0.0044,      0.0006,      0.0177,\n",
       "              0.0188,      0.0185,      0.0138,      0.0073,      0.0106,\n",
       "             -0.0271,      0.0205,     -0.0223,     -0.0360,      0.0027,\n",
       "             -0.0133,      0.0029,     -0.0305,      0.0088,     -0.0276,\n",
       "             -0.0028,     -0.0191,      0.0084,      0.0248,      0.0168,\n",
       "              0.0178,      0.0082,     -0.0314,      0.0228,      0.0122,\n",
       "              0.0359,     -0.0043,      0.0170,     -0.0345,     -0.0032,\n",
       "             -0.0214,     -0.0230,     -0.0001,     -0.0029,     -0.0010,\n",
       "              0.0262,      0.0119,      0.0331,     -0.0096,     -0.0006,\n",
       "             -0.0177,      0.0032,      0.0038,      0.0266,     -0.0179,\n",
       "              0.0179,      0.0212,     -0.0334,      0.0123,      0.0165,\n",
       "              0.0359,      0.0231,      0.0261,      0.0344,      0.0199,\n",
       "              0.0060,      0.0217,      0.0092,      0.0323,     -0.0264,\n",
       "              0.0340,     -0.0287,     -0.0226,      0.0215,     -0.0015,\n",
       "             -0.0329,     -0.0239,      0.0341,      0.0023,      0.0071,\n",
       "             -0.0173,     -0.0237,      0.0288,      0.0150,     -0.0177,\n",
       "             -0.0202,     -0.0286,      0.0035,      0.0353,     -0.0253,\n",
       "              0.0287,     -0.0038,     -0.0236,      0.0109,     -0.0347,\n",
       "             -0.0209,     -0.0252,      0.0261,      0.0274,     -0.0232,\n",
       "              0.0040,      0.0084,     -0.0214,     -0.0010,      0.0327,\n",
       "             -0.0339,      0.0339,     -0.0195,      0.0035,      0.0206,\n",
       "              0.0306,     -0.0346,     -0.0086,     -0.0056,     -0.0206,\n",
       "              0.0192,     -0.0088,     -0.0027,     -0.0031,      0.0230,\n",
       "              0.0277,     -0.0274,     -0.0023,      0.0189,      0.0027,\n",
       "              0.0068,      0.0330,      0.0182,     -0.0203,     -0.0139,\n",
       "             -0.0111,     -0.0304,     -0.0153,     -0.0331,      0.0017,\n",
       "             -0.0244,      0.0337,      0.0203,      0.0227,      0.0062,\n",
       "              0.0208,     -0.0237,     -0.0235,      0.0193,      0.0108,\n",
       "              0.0138,      0.0060,      0.0112,      0.0101,     -0.0202,\n",
       "             -0.0200,     -0.0032,     -0.0322,      0.0235,     -0.0208,\n",
       "             -0.0259,      0.0143,      0.0308,      0.0123,      0.0281,\n",
       "              0.0333,     -0.0234,      0.0101,     -0.0223,      0.0208,\n",
       "              0.0034,     -0.0253,      0.0212,     -0.0253,     -0.0086,\n",
       "             -0.0161,     -0.0125,      0.0093,     -0.0113,     -0.0223,\n",
       "              0.0075,      0.0028,      0.0030,     -0.0058,     -0.0262,\n",
       "             -0.0246,     -0.0002,      0.0040,     -0.0281,      0.0180,\n",
       "              0.0237,     -0.0199,     -0.0185,      0.0324,     -0.0131,\n",
       "              0.0084,     -0.0222,     -0.0248,      0.0019,     -0.0138,\n",
       "              0.0111,      0.0267,     -0.0186,     -0.0236,      0.0128,\n",
       "              0.0111,      0.0336,     -0.0001,      0.0075,      0.0263,\n",
       "             -0.0070,     -0.0177,      0.0296,      0.0356,     -0.0196,\n",
       "             -0.0251,     -0.0068,      0.0229,      0.0093,      0.0053,\n",
       "             -0.0314,     -0.0320,     -0.0205,      0.0227,     -0.0252,\n",
       "             -0.0013,      0.0160,      0.0228,      0.0337,      0.0030,\n",
       "             -0.0245,     -0.0344,     -0.0349,      0.0129,      0.0150,\n",
       "             -0.0124,     -0.0005,      0.0254,      0.0101,      0.0229,\n",
       "             -0.0154,     -0.0331,      0.0348,     -0.0310,     -0.0319,\n",
       "             -0.0093,     -0.0334,     -0.0046,      0.0199,     -0.0062,\n",
       "              0.0236,     -0.0012,      0.0229,     -0.0262,     -0.0153,\n",
       "              0.0213,     -0.0258,      0.0274,     -0.0275,     -0.0141,\n",
       "             -0.0273,      0.0141,     -0.0336,     -0.0339,      0.0327,\n",
       "              0.0204,      0.0300,     -0.0180,     -0.0139,      0.0115,\n",
       "              0.0293,      0.0033,      0.0141,     -0.0306,      0.0211,\n",
       "             -0.0264,     -0.0288,     -0.0198,      0.0101,      0.0291,\n",
       "              0.0162,      0.0168,     -0.0048,      0.0325,      0.0006,\n",
       "             -0.0081,     -0.0114,      0.0021,      0.0172,     -0.0344,\n",
       "              0.0326,     -0.0187,      0.0102,      0.0102,     -0.0230,\n",
       "             -0.0201,      0.0100,     -0.0071,     -0.0336,      0.0172,\n",
       "              0.0199,      0.0074,     -0.0095,      0.0118,     -0.0171,\n",
       "              0.0046,      0.0174,      0.0342,     -0.0095,      0.0081,\n",
       "              0.0034,      0.0272,      0.0062,     -0.0011,      0.0184,\n",
       "              0.0272,     -0.0139,     -0.0214,      0.0333,      0.0271,\n",
       "              0.0116,     -0.0149,     -0.0063,      0.0044,      0.0172,\n",
       "             -0.0234,      0.0314,      0.0040,      0.0327,     -0.0155,\n",
       "              0.0317,     -0.0263,     -0.0080,      0.0243,      0.0347,\n",
       "             -0.0269,      0.0136,     -0.0210,     -0.0059,     -0.0359,\n",
       "             -0.0187,      0.0120,      0.0290,      0.0251,     -0.0051,\n",
       "             -0.0360,     -0.0174,     -0.0179,     -0.0151,     -0.0131,\n",
       "              0.0058,      0.0028,      0.0038,      0.0070,     -0.0070,\n",
       "             -0.0255,     -0.0200,      0.0286,     -0.0058,      0.0190,\n",
       "              0.0297,     -0.0145,      0.0278,     -0.0163,     -0.0350,\n",
       "              0.0241,      0.0026,     -0.0120,     -0.0051,      0.0167,\n",
       "             -0.0269,     -0.0261,     -0.0215,     -0.0093,      0.0247,\n",
       "              0.0169,     -0.0001,     -0.0194,      0.0297,      0.0262,\n",
       "              0.0101,     -0.0219,     -0.0025,     -0.0321,      0.0286,\n",
       "             -0.0077,      0.0268,      0.0255,     -0.0268,      0.0267,\n",
       "              0.0011,      0.0144,     -0.0299,     -0.0328,     -0.0170,\n",
       "              0.0244,     -0.0030,      0.0169,     -0.0255,     -0.0293,\n",
       "             -0.0241,      0.0005,     -0.0045,     -0.0345,     -0.0087,\n",
       "             -0.0163,     -0.0076,      0.0278,     -0.0108,     -0.0301,\n",
       "              0.0030,      0.0176,      0.0093,     -0.0150,      0.0021,\n",
       "              0.0249,      0.0133,      0.0287,      0.0035,      0.0117,\n",
       "              0.0043,      0.0204,      0.0095,     -0.0226,      0.0152,\n",
       "             -0.0190,      0.0269,      0.0311,      0.0047,      0.0094,\n",
       "             -0.0014,      0.0184,      0.0075,      0.0124,     -0.0036,\n",
       "              0.0124,     -0.0167,     -0.0111,     -0.0338,     -0.0172,\n",
       "              0.0331,      0.0261,      0.0014,     -0.0073,      0.0099,\n",
       "              0.0279,     -0.0331,     -0.0353,     -0.0308,     -0.0138,\n",
       "             -0.0252,      0.0208,      0.0087,     -0.0052,     -0.0005,\n",
       "              0.0254,      0.0229,     -0.0306,     -0.0286,     -0.0312,\n",
       "             -0.0352,     -0.0270,      0.0058,     -0.0015,      0.0200,\n",
       "              0.0101,     -0.0315,      0.0336,     -0.0257,     -0.0181,\n",
       "              0.0339,      0.0158,      0.0228,     -0.0318,     -0.0347,\n",
       "              0.0063,     -0.0281,      0.0070,     -0.0239,     -0.0055,\n",
       "             -0.0308,      0.0337,     -0.0353,      0.0082,      0.0353,\n",
       "             -0.0308,     -0.0173,      0.0214,      0.0143,      0.0237,\n",
       "             -0.0342,     -0.0220,     -0.0100,      0.0240,     -0.0099,\n",
       "              0.0107,     -0.0157,      0.0356,      0.0349,     -0.0168,\n",
       "             -0.0323,      0.0043,     -0.0039,     -0.0202,      0.0349,\n",
       "              0.0047,     -0.0162,     -0.0184,     -0.0078,      0.0313,\n",
       "              0.0113,      0.0232,      0.0156,     -0.0039,      0.0015,\n",
       "              0.0005,      0.0324,     -0.0208,      0.0179,      0.0019,\n",
       "              0.0253,      0.0320,      0.0195,     -0.0136,      0.0080,\n",
       "              0.0163,      0.0148,      0.0002,      0.0272,     -0.0216,\n",
       "             -0.0116,      0.0284,      0.0129,     -0.0244,      0.0195,\n",
       "              0.0359,     -0.0106,     -0.0295,     -0.0068,      0.0218,\n",
       "              0.0065,     -0.0175,     -0.0186,      0.0075,     -0.0205,\n",
       "              0.0004,     -0.0340,      0.0118,      0.0338,      0.0254,\n",
       "              0.0221,      0.0074,      0.0333,      0.0356,      0.0280,\n",
       "              0.0134,      0.0079,     -0.0174,      0.0224,     -0.0258,\n",
       "             -0.0337,     -0.0143,      0.0043,     -0.0003,      0.0070,\n",
       "             -0.0329,      0.0103,     -0.0120,     -0.0314,      0.0302,\n",
       "              0.0099,      0.0295,      0.0135,     -0.0087,     -0.0162,\n",
       "              0.0278,      0.0123,      0.0153,     -0.0178,      0.0319,\n",
       "              0.0227,      0.0264,      0.0120,     -0.0314,     -0.0110,\n",
       "             -0.0040,     -0.0067,      0.0173,      0.0213,     -0.0163,\n",
       "             -0.0241,     -0.0203,     -0.0359,      0.0020,     -0.0050,\n",
       "              0.0090,     -0.0046,     -0.0153,     -0.0217,     -0.0083,\n",
       "              0.0076,      0.0177,     -0.0144,     -0.0111,      0.0183,\n",
       "             -0.0187,     -0.0232,      0.0174,      0.0119,     -0.0295,\n",
       "              0.0231,     -0.0234,      0.0354,     -0.0322,      0.0057,\n",
       "             -0.0185,      0.0328,     -0.0291,     -0.0343,      0.0186,\n",
       "             -0.0019,      0.0181,     -0.0118,      0.0022,      0.0253,\n",
       "             -0.0192,      0.0019,      0.0027,      0.0310,      0.0058,\n",
       "             -0.0132,      0.0086,     -0.0240,     -0.0292,      0.0275,\n",
       "              0.0349,     -0.0202,      0.0284,      0.0138,      0.0024,\n",
       "              0.0234,     -0.0218,      0.0185,     -0.0310,     -0.0136,\n",
       "              0.0164,      0.0126,     -0.0334,     -0.0092,      0.0146,\n",
       "             -0.0150,      0.0002,     -0.0027,     -0.0079,      0.0324,\n",
       "             -0.0252,      0.0215,      0.0218,     -0.0221,      0.0357,\n",
       "             -0.0237,     -0.0070,      0.0255,      0.0001,      0.0034,\n",
       "              0.0296,      0.0095,     -0.0060,      0.0353,     -0.0023,\n",
       "              0.0186,      0.0278,     -0.0059,      0.0357,     -0.0170,\n",
       "             -0.0160,      0.0043,      0.0221,      0.0027,     -0.0179,\n",
       "              0.0359,      0.0082,     -0.0142,      0.0077,     -0.0093,\n",
       "             -0.0168,      0.0075,      0.0122,      0.0314,     -0.0204,\n",
       "             -0.0019,     -0.0120,     -0.0003,     -0.0172,     -0.0318,\n",
       "             -0.0051,      0.0221,     -0.0247,     -0.0266,      0.0129,\n",
       "              0.0040,      0.0065,     -0.0257,     -0.0309,      0.0185,\n",
       "             -0.0045,      0.0171,     -0.0264,      0.0087,     -0.0215,\n",
       "              0.0222,     -0.0314,      0.0019,      0.0289,     -0.0064,\n",
       "             -0.0081,     -0.0255,      0.0162,     -0.0136,      0.0289,\n",
       "              0.0103,      0.0292,      0.0131], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0346, -0.0129,  0.0034,  ...,  0.0226, -0.0225, -0.0256],\n",
       "         [ 0.0134, -0.0125, -0.0107,  ..., -0.0028, -0.0278,  0.0181],\n",
       "         [-0.0161, -0.0262,  0.0280,  ...,  0.0355, -0.0355, -0.0082],\n",
       "         ...,\n",
       "         [-0.0158, -0.0230,  0.0009,  ..., -0.0103, -0.0331,  0.0282],\n",
       "         [-0.0277,  0.0100,  0.0019,  ..., -0.0260, -0.0207, -0.0295],\n",
       "         [ 0.0055,  0.0085,  0.0002,  ...,  0.0307, -0.0093,  0.0150]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.0290,  0.0002,  0.0174,  ..., -0.0042,  0.0162, -0.0269],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[     0.0111,     -0.0028,     -0.0124,  ...,      0.0069,\n",
       "              -0.0155,     -0.0121],\n",
       "         [     0.0109,      0.0122,      0.0022,  ...,     -0.0005,\n",
       "               0.0118,      0.0008],\n",
       "         [    -0.0044,     -0.0040,     -0.0114,  ...,     -0.0111,\n",
       "              -0.0086,     -0.0127],\n",
       "         ...,\n",
       "         [    -0.0132,      0.0108,     -0.0066,  ...,     -0.0132,\n",
       "              -0.0124,     -0.0058],\n",
       "         [     0.0075,      0.0019,      0.0102,  ...,     -0.0130,\n",
       "               0.0000,     -0.0133],\n",
       "         [     0.0136,     -0.0105,      0.0129,  ...,      0.0081,\n",
       "              -0.0108,      0.0051]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([     0.0129,      0.0177,      0.0121,     -0.0010,      0.0110,\n",
       "              0.0072,      0.0015,      0.0042,      0.0110,      0.0060,\n",
       "             -0.0035,      0.0115,      0.0151,      0.0019,     -0.0155,\n",
       "             -0.0069,     -0.0119,     -0.0051,      0.0160,      0.0176,\n",
       "             -0.0053,      0.0137,      0.0143,     -0.0053,     -0.0027,\n",
       "             -0.0044,      0.0083,      0.0013,     -0.0150,      0.0026,\n",
       "              0.0052,      0.0052,     -0.0013,      0.0100,      0.0168,\n",
       "             -0.0078,     -0.0018,      0.0139,     -0.0005,      0.0085,\n",
       "              0.0150,      0.0086,      0.0045,      0.0068,     -0.0054,\n",
       "             -0.0045,     -0.0117,     -0.0082,     -0.0007,     -0.0062,\n",
       "             -0.0100,      0.0064,     -0.0142,     -0.0024,      0.0142,\n",
       "              0.0021,      0.0122,     -0.0086,     -0.0092,      0.0042,\n",
       "             -0.0012,     -0.0051,     -0.0018,     -0.0131,     -0.0148,\n",
       "              0.0025,      0.0126,      0.0013,     -0.0099,     -0.0037,\n",
       "             -0.0124,      0.0012,      0.0179,     -0.0097,      0.0034,\n",
       "              0.0094,     -0.0031,      0.0061,     -0.0134,      0.0173,\n",
       "             -0.0029,     -0.0030,      0.0049,      0.0180,      0.0103,\n",
       "             -0.0051,     -0.0120,      0.0148,     -0.0122,     -0.0032,\n",
       "              0.0001,      0.0165,     -0.0007,      0.0154,      0.0042,\n",
       "             -0.0093,      0.0128,      0.0043,      0.0125,      0.0149,\n",
       "              0.0066,      0.0120,     -0.0016,     -0.0116,      0.0086,\n",
       "             -0.0090,     -0.0161,      0.0054,     -0.0004,     -0.0121,\n",
       "             -0.0172,     -0.0055,     -0.0163,      0.0023,      0.0152,\n",
       "             -0.0099,     -0.0151,      0.0154,      0.0077,      0.0050,\n",
       "             -0.0024,      0.0008,     -0.0033,      0.0160,     -0.0037,\n",
       "             -0.0057,      0.0164,      0.0119,      0.0180,     -0.0048,\n",
       "              0.0052,     -0.0068,      0.0099,     -0.0045,     -0.0152,\n",
       "              0.0066,      0.0174,     -0.0170,      0.0166,     -0.0083,\n",
       "              0.0028,     -0.0158,     -0.0004,      0.0121,     -0.0170,\n",
       "              0.0085,     -0.0055,     -0.0170,      0.0000,      0.0164,\n",
       "             -0.0070,     -0.0041,      0.0002,     -0.0047,      0.0077,\n",
       "              0.0115,      0.0036,      0.0131,     -0.0015,      0.0124,\n",
       "              0.0133,     -0.0020,      0.0127,      0.0115,      0.0146,\n",
       "             -0.0175,     -0.0117,     -0.0064,      0.0041,     -0.0095,\n",
       "              0.0006,      0.0089,      0.0061,      0.0007,     -0.0148,\n",
       "              0.0134,      0.0107,      0.0173,      0.0090,      0.0046,\n",
       "             -0.0011,     -0.0062,     -0.0026,     -0.0148,      0.0064,\n",
       "             -0.0083,      0.0145,     -0.0144,      0.0031,      0.0073,\n",
       "             -0.0049,      0.0086,     -0.0034,      0.0078,     -0.0104,\n",
       "              0.0003,     -0.0041,     -0.0062,      0.0156,      0.0041,\n",
       "              0.0143,     -0.0023,      0.0104,      0.0053,     -0.0002,\n",
       "              0.0173,      0.0177,      0.0062,      0.0002,     -0.0166,\n",
       "              0.0083,      0.0014,     -0.0055,      0.0063,     -0.0070,\n",
       "             -0.0085,      0.0109,      0.0014,      0.0001,      0.0076,\n",
       "             -0.0046,      0.0046,     -0.0061,     -0.0015,      0.0068,\n",
       "              0.0108,      0.0150,      0.0059,      0.0061,      0.0160,\n",
       "              0.0170,     -0.0028,      0.0139,     -0.0073,     -0.0065,\n",
       "              0.0031,     -0.0056,      0.0039,     -0.0120,      0.0145,\n",
       "              0.0034,      0.0004,      0.0130,      0.0039,      0.0037,\n",
       "              0.0179,      0.0158,      0.0006,      0.0176,      0.0065,\n",
       "              0.0115,      0.0107,     -0.0009,     -0.0050,      0.0096,\n",
       "             -0.0094,     -0.0000,      0.0145,      0.0053,     -0.0073,\n",
       "              0.0023,     -0.0075,     -0.0122,     -0.0026,     -0.0127,\n",
       "             -0.0059,      0.0137,      0.0099,     -0.0109,     -0.0141,\n",
       "              0.0146,      0.0043,     -0.0038,     -0.0168,     -0.0029,\n",
       "             -0.0004,     -0.0017,      0.0093,     -0.0057,      0.0154,\n",
       "              0.0054,      0.0066,     -0.0008,     -0.0171,     -0.0125,\n",
       "              0.0096,      0.0042,      0.0097,     -0.0133,      0.0020,\n",
       "              0.0105,     -0.0041,     -0.0144,     -0.0146,     -0.0169,\n",
       "             -0.0180,      0.0030,     -0.0100,      0.0150,      0.0087,\n",
       "              0.0177,     -0.0110,      0.0086,      0.0073,      0.0050,\n",
       "             -0.0160,      0.0143,     -0.0170,      0.0037,     -0.0009,\n",
       "              0.0022,      0.0034,     -0.0078,     -0.0159,     -0.0174,\n",
       "              0.0141,      0.0100,     -0.0145,     -0.0171,     -0.0173,\n",
       "             -0.0012,     -0.0001,      0.0014,      0.0138,     -0.0098,\n",
       "              0.0088,      0.0023,     -0.0085,      0.0087,     -0.0068,\n",
       "             -0.0048,     -0.0004,      0.0069,      0.0029,      0.0046,\n",
       "              0.0033,      0.0103,      0.0002,      0.0143,      0.0129,\n",
       "             -0.0077,     -0.0169,      0.0152,     -0.0098,      0.0112,\n",
       "             -0.0012,      0.0086,      0.0062,     -0.0076,     -0.0168,\n",
       "             -0.0138,      0.0007,     -0.0175,     -0.0135,     -0.0037,\n",
       "             -0.0086,     -0.0078,      0.0037,      0.0079,     -0.0009,\n",
       "              0.0019,      0.0007,     -0.0146,     -0.0114,     -0.0140,\n",
       "             -0.0079,      0.0163,     -0.0101,     -0.0135,     -0.0101,\n",
       "              0.0121,      0.0071,      0.0118,     -0.0090,      0.0128,\n",
       "              0.0155,      0.0103,      0.0054,     -0.0059,     -0.0026,\n",
       "              0.0031,      0.0117,      0.0098,      0.0117,     -0.0064,\n",
       "             -0.0135,      0.0170,     -0.0177,     -0.0059,      0.0116,\n",
       "              0.0170,      0.0104,      0.0122,     -0.0103,     -0.0162,\n",
       "              0.0108,     -0.0174,     -0.0048,     -0.0120,     -0.0168,\n",
       "              0.0087,      0.0089,      0.0085,      0.0005,      0.0007,\n",
       "              0.0052,     -0.0065,      0.0142,      0.0022,     -0.0051,\n",
       "             -0.0141,      0.0123,      0.0019,      0.0007,     -0.0098,\n",
       "              0.0131,     -0.0101,     -0.0126,      0.0158,      0.0064,\n",
       "              0.0135,     -0.0066,     -0.0130,     -0.0052,      0.0032,\n",
       "             -0.0140,      0.0057,     -0.0045,     -0.0102,      0.0035,\n",
       "             -0.0176,      0.0112,      0.0070,      0.0002,      0.0100,\n",
       "             -0.0052,      0.0148,      0.0086,     -0.0022,      0.0147,\n",
       "             -0.0150,     -0.0068,     -0.0034,      0.0026,      0.0074,\n",
       "             -0.0152,     -0.0175,      0.0169,      0.0132,     -0.0001,\n",
       "              0.0099,     -0.0112,     -0.0107,      0.0084,     -0.0142,\n",
       "              0.0123,      0.0180,      0.0152,      0.0129,      0.0127,\n",
       "              0.0010,     -0.0098,      0.0179,      0.0128,      0.0137,\n",
       "              0.0085,      0.0028,     -0.0058,     -0.0151,     -0.0174,\n",
       "              0.0095,      0.0135,      0.0106,      0.0020,      0.0129,\n",
       "             -0.0035,     -0.0137,      0.0110,      0.0010,     -0.0048,\n",
       "              0.0027,      0.0078,     -0.0118,     -0.0070,      0.0102,\n",
       "             -0.0091,     -0.0089,     -0.0150,     -0.0177,     -0.0050,\n",
       "              0.0088,     -0.0139,     -0.0036,      0.0096,      0.0085,\n",
       "             -0.0118,      0.0091,     -0.0094,     -0.0158,      0.0164,\n",
       "              0.0093,      0.0052,      0.0124,     -0.0157,     -0.0076,\n",
       "              0.0071,      0.0054,      0.0083,     -0.0101,     -0.0020,\n",
       "              0.0040,     -0.0175,      0.0125,     -0.0120,      0.0089,\n",
       "              0.0109,     -0.0061,      0.0147,      0.0035,      0.0022,\n",
       "             -0.0144,     -0.0127,     -0.0072,     -0.0114,      0.0028,\n",
       "             -0.0139,      0.0131,      0.0148,      0.0155,     -0.0063,\n",
       "             -0.0019,      0.0173,      0.0162,     -0.0150,     -0.0125,\n",
       "              0.0018,      0.0165,     -0.0097,     -0.0042,     -0.0002,\n",
       "              0.0174,      0.0043,     -0.0165,      0.0013,     -0.0146,\n",
       "              0.0075,     -0.0104,     -0.0120,      0.0152,      0.0130,\n",
       "              0.0098,      0.0139,     -0.0072,      0.0038,      0.0088,\n",
       "             -0.0170,      0.0020,      0.0148,     -0.0127,     -0.0017,\n",
       "             -0.0155,     -0.0056,     -0.0062,     -0.0142,      0.0178,\n",
       "             -0.0070,      0.0075,     -0.0172,      0.0008,     -0.0096,\n",
       "              0.0065,     -0.0055,     -0.0070,      0.0014,     -0.0155,\n",
       "              0.0037,     -0.0161,      0.0119,      0.0061,      0.0089,\n",
       "              0.0080,     -0.0014,     -0.0087,      0.0069,      0.0109,\n",
       "              0.0064,      0.0003,      0.0033,      0.0023,      0.0080,\n",
       "             -0.0014,     -0.0093,     -0.0108,     -0.0054,      0.0136,\n",
       "              0.0077,     -0.0096,      0.0133,      0.0064,     -0.0138,\n",
       "             -0.0053,     -0.0173,      0.0072,      0.0180,     -0.0167,\n",
       "             -0.0160,      0.0165,     -0.0014,      0.0140,      0.0132,\n",
       "              0.0126,      0.0090,     -0.0020,     -0.0103,     -0.0151,\n",
       "             -0.0114,     -0.0134,     -0.0161,      0.0023,     -0.0050,\n",
       "              0.0048,      0.0025,      0.0101,     -0.0080,     -0.0180,\n",
       "             -0.0018,     -0.0048,     -0.0002,     -0.0109,      0.0127,\n",
       "              0.0061,      0.0017,      0.0015,     -0.0155,     -0.0129,\n",
       "              0.0045,      0.0107,      0.0002,      0.0032,     -0.0048,\n",
       "             -0.0158,     -0.0044,      0.0040,      0.0157,      0.0153,\n",
       "              0.0141,      0.0180,     -0.0173,      0.0138,     -0.0117,\n",
       "              0.0162,     -0.0145,      0.0065,      0.0159,      0.0016,\n",
       "              0.0004,      0.0074,      0.0093,      0.0151,     -0.0034,\n",
       "             -0.0044,     -0.0077,     -0.0035,     -0.0051,      0.0105,\n",
       "             -0.0087,     -0.0089,      0.0124,     -0.0102,     -0.0027,\n",
       "              0.0176,     -0.0137,      0.0008,     -0.0155,      0.0133,\n",
       "             -0.0118,      0.0123,      0.0065,      0.0007,      0.0157,\n",
       "              0.0003,      0.0026,      0.0164,      0.0118,     -0.0100,\n",
       "              0.0062,      0.0114,      0.0010,      0.0108,     -0.0023,\n",
       "             -0.0036,     -0.0105,     -0.0166,      0.0057,      0.0146,\n",
       "             -0.0081,     -0.0121,      0.0017,     -0.0073,      0.0007,\n",
       "             -0.0176,     -0.0013,      0.0160,     -0.0012,     -0.0033,\n",
       "              0.0111,      0.0039,      0.0100,     -0.0041,     -0.0076,\n",
       "              0.0001,      0.0017,     -0.0004,      0.0009,      0.0114,\n",
       "             -0.0173,     -0.0158,     -0.0096,     -0.0058,      0.0086,\n",
       "             -0.0106,     -0.0020,     -0.0030,     -0.0092,     -0.0167,\n",
       "              0.0077,      0.0036,      0.0082,     -0.0159,      0.0149,\n",
       "             -0.0160,     -0.0165,     -0.0017,      0.0023,      0.0139,\n",
       "              0.0018,      0.0143,      0.0017,      0.0077,     -0.0057,\n",
       "             -0.0100,      0.0103,     -0.0166,      0.0047,     -0.0122,\n",
       "              0.0010,      0.0031,      0.0132,     -0.0119,      0.0011,\n",
       "             -0.0180,      0.0151,     -0.0111,     -0.0082,      0.0039,\n",
       "              0.0045,     -0.0180,     -0.0145,      0.0107,      0.0178,\n",
       "              0.0002,     -0.0178,      0.0144,      0.0178,     -0.0171,\n",
       "             -0.0108,     -0.0001,      0.0069], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0334,  0.0053,  0.0175,  ..., -0.0358,  0.0010,  0.0068],\n",
       "         [-0.0107,  0.0270, -0.0217,  ...,  0.0223, -0.0012, -0.0164],\n",
       "         [-0.0006,  0.0299,  0.0271,  ..., -0.0305,  0.0250, -0.0199],\n",
       "         ...,\n",
       "         [ 0.0059, -0.0063, -0.0081,  ...,  0.0292, -0.0292,  0.0086],\n",
       "         [ 0.0336,  0.0309, -0.0335,  ...,  0.0207,  0.0331,  0.0328],\n",
       "         [-0.0265,  0.0153, -0.0168,  ...,  0.0131,  0.0068, -0.0010]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0088, -0.0208, -0.0203,  ...,  0.0114,  0.0106, -0.0249],\n",
       "         [ 0.0049,  0.0218,  0.0298,  ..., -0.0294, -0.0107,  0.0165],\n",
       "         [-0.0187, -0.0107, -0.0320,  ...,  0.0230, -0.0292, -0.0100],\n",
       "         ...,\n",
       "         [-0.0075,  0.0205, -0.0034,  ...,  0.0191,  0.0282,  0.0337],\n",
       "         [ 0.0318, -0.0294,  0.0014,  ...,  0.0009,  0.0092,  0.0343],\n",
       "         [ 0.0191, -0.0251,  0.0043,  ..., -0.0189, -0.0301,  0.0008]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0195, -0.0354,  0.0070,  ..., -0.0155,  0.0049, -0.0356],\n",
       "         [ 0.0146,  0.0056, -0.0146,  ..., -0.0339,  0.0179,  0.0208],\n",
       "         [-0.0264, -0.0234,  0.0350,  ...,  0.0214,  0.0269,  0.0190],\n",
       "         ...,\n",
       "         [ 0.0261,  0.0112, -0.0296,  ..., -0.0138,  0.0052, -0.0241],\n",
       "         [-0.0304,  0.0264,  0.0323,  ..., -0.0348, -0.0092, -0.0075],\n",
       "         [-0.0267, -0.0094,  0.0010,  ...,  0.0185,  0.0089, -0.0103]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[     0.0074,      0.0267,      0.0206,  ...,     -0.0120,\n",
       "              -0.0178,      0.0338],\n",
       "         [    -0.0166,     -0.0103,     -0.0099,  ...,     -0.0117,\n",
       "               0.0286,     -0.0296],\n",
       "         [    -0.0300,     -0.0000,      0.0279,  ...,      0.0105,\n",
       "               0.0114,      0.0147],\n",
       "         ...,\n",
       "         [    -0.0142,      0.0247,      0.0001,  ...,     -0.0329,\n",
       "               0.0008,      0.0324],\n",
       "         [    -0.0078,     -0.0272,     -0.0005,  ...,     -0.0116,\n",
       "              -0.0193,     -0.0302],\n",
       "         [    -0.0082,      0.0309,      0.0079,  ...,     -0.0154,\n",
       "              -0.0246,      0.0053]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([    -0.0237,     -0.0008,      0.0169,      0.0002,      0.0090,\n",
       "              0.0018,      0.0112,     -0.0295,      0.0269,      0.0241,\n",
       "             -0.0071,      0.0045,     -0.0232,      0.0160,      0.0035,\n",
       "             -0.0164,      0.0112,     -0.0311,     -0.0157,     -0.0306,\n",
       "              0.0306,      0.0003,     -0.0248,     -0.0054,      0.0100,\n",
       "             -0.0099,      0.0058,      0.0208,     -0.0034,     -0.0065,\n",
       "             -0.0057,     -0.0057,     -0.0047,      0.0358,      0.0224,\n",
       "              0.0023,     -0.0237,      0.0257,     -0.0157,     -0.0223,\n",
       "              0.0108,      0.0086,     -0.0240,      0.0206,      0.0295,\n",
       "             -0.0252,      0.0145,      0.0160,     -0.0296,     -0.0310,\n",
       "              0.0282,     -0.0149,      0.0052,      0.0078,      0.0317,\n",
       "             -0.0102,     -0.0248,     -0.0266,      0.0191,     -0.0189,\n",
       "             -0.0016,     -0.0154,     -0.0204,      0.0138,     -0.0351,\n",
       "              0.0093,      0.0186,     -0.0324,     -0.0349,      0.0138,\n",
       "             -0.0032,      0.0241,     -0.0206,      0.0358,     -0.0031,\n",
       "             -0.0194,      0.0285,      0.0313,     -0.0101,     -0.0102,\n",
       "              0.0170,     -0.0032,      0.0215,      0.0231,     -0.0059,\n",
       "              0.0197,     -0.0155,     -0.0334,      0.0043,      0.0245,\n",
       "             -0.0049,     -0.0319,      0.0112,     -0.0213,      0.0250,\n",
       "             -0.0042,      0.0149,     -0.0170,      0.0309,      0.0085,\n",
       "              0.0126,      0.0045,      0.0080,      0.0084,      0.0349,\n",
       "             -0.0225,      0.0220,      0.0172,      0.0253,     -0.0013,\n",
       "             -0.0141,      0.0225,      0.0003,      0.0001,      0.0285,\n",
       "              0.0355,      0.0327,      0.0003,     -0.0298,     -0.0026,\n",
       "              0.0237,     -0.0196,     -0.0024,      0.0028,      0.0065,\n",
       "             -0.0184,     -0.0278,      0.0084,      0.0114,     -0.0278,\n",
       "             -0.0020,      0.0219,      0.0192,     -0.0265,      0.0207,\n",
       "             -0.0018,      0.0256,     -0.0061,      0.0303,      0.0108,\n",
       "             -0.0180,     -0.0308,     -0.0298,      0.0007,      0.0265,\n",
       "             -0.0094,     -0.0247,     -0.0159,      0.0045,     -0.0150,\n",
       "              0.0040,     -0.0219,      0.0172,     -0.0175,     -0.0272,\n",
       "             -0.0014,     -0.0218,     -0.0185,      0.0335,     -0.0197,\n",
       "             -0.0148,     -0.0203,     -0.0007,      0.0027,      0.0183,\n",
       "             -0.0236,      0.0003,     -0.0049,      0.0313,      0.0174,\n",
       "              0.0278,      0.0323,     -0.0045,     -0.0080,     -0.0341,\n",
       "             -0.0008,     -0.0252,      0.0256,      0.0332,      0.0245,\n",
       "             -0.0008,      0.0302,      0.0265,      0.0158,     -0.0153,\n",
       "              0.0135,      0.0141,     -0.0113,      0.0162,     -0.0168,\n",
       "              0.0284,      0.0152,     -0.0001,     -0.0226,     -0.0010,\n",
       "              0.0010,      0.0304,     -0.0216,     -0.0305,      0.0285,\n",
       "             -0.0007,     -0.0068,     -0.0244,     -0.0278,      0.0246,\n",
       "             -0.0177,     -0.0288,      0.0171,     -0.0115,     -0.0271,\n",
       "              0.0326,     -0.0249,     -0.0043,     -0.0311,     -0.0069,\n",
       "             -0.0358,      0.0098,      0.0038,     -0.0067,     -0.0186,\n",
       "             -0.0293,      0.0275,      0.0023,     -0.0161,     -0.0283,\n",
       "              0.0357,      0.0116,      0.0170,     -0.0136,      0.0154,\n",
       "             -0.0287,      0.0209,     -0.0279,      0.0353,      0.0100,\n",
       "              0.0034,     -0.0112,     -0.0078,      0.0021,     -0.0207,\n",
       "              0.0349,      0.0302,     -0.0056,      0.0145,      0.0092,\n",
       "             -0.0275,      0.0189,      0.0190,      0.0204,      0.0009,\n",
       "             -0.0212,     -0.0301,     -0.0130,      0.0104,      0.0117,\n",
       "             -0.0139,      0.0038,      0.0322,      0.0083,      0.0256,\n",
       "              0.0330,      0.0025,      0.0189,      0.0005,     -0.0281,\n",
       "              0.0310,      0.0254,      0.0294,     -0.0359,      0.0002,\n",
       "             -0.0033,     -0.0325,     -0.0296,      0.0161,     -0.0071,\n",
       "              0.0210,      0.0174,      0.0297,      0.0027,      0.0287,\n",
       "             -0.0144,     -0.0309,      0.0255,      0.0184,     -0.0323,\n",
       "             -0.0001,     -0.0087,      0.0355,     -0.0286,     -0.0138,\n",
       "              0.0112,     -0.0226,     -0.0271,      0.0048,     -0.0233,\n",
       "              0.0303,      0.0155,     -0.0240,     -0.0285,      0.0265,\n",
       "             -0.0204,      0.0277,      0.0073,     -0.0119,      0.0013,\n",
       "              0.0299,      0.0135,     -0.0009,      0.0277,     -0.0236,\n",
       "              0.0074,     -0.0109,     -0.0252,     -0.0016,      0.0202,\n",
       "              0.0271,     -0.0215,      0.0141,      0.0010,      0.0156,\n",
       "              0.0148,     -0.0026,      0.0139,     -0.0229,      0.0154,\n",
       "              0.0093,      0.0167,     -0.0360,     -0.0073,     -0.0029,\n",
       "             -0.0355,     -0.0148,     -0.0172,     -0.0074,      0.0328,\n",
       "             -0.0034,     -0.0276,     -0.0076,      0.0341,     -0.0198,\n",
       "              0.0153,     -0.0272,     -0.0225,      0.0350,     -0.0140,\n",
       "             -0.0349,      0.0109,     -0.0255,      0.0163,      0.0168,\n",
       "              0.0071,     -0.0280,     -0.0275,      0.0064,     -0.0188,\n",
       "              0.0190,     -0.0169,     -0.0151,      0.0196,     -0.0217,\n",
       "              0.0146,     -0.0269,      0.0051,      0.0041,      0.0158,\n",
       "             -0.0096,      0.0007,     -0.0233,      0.0062,      0.0145,\n",
       "             -0.0204,     -0.0010,     -0.0275,      0.0241,      0.0181,\n",
       "              0.0344,     -0.0064,      0.0054,      0.0137,     -0.0029,\n",
       "              0.0024,      0.0124,      0.0225,     -0.0062,     -0.0139,\n",
       "              0.0070,      0.0179,      0.0325,      0.0327,     -0.0234,\n",
       "              0.0170,     -0.0139,      0.0204,      0.0291,     -0.0200,\n",
       "             -0.0087,     -0.0196,      0.0096,      0.0172,     -0.0314,\n",
       "              0.0358,     -0.0155,      0.0069,     -0.0255,     -0.0213,\n",
       "              0.0333,      0.0044,     -0.0103,      0.0323,     -0.0145,\n",
       "             -0.0245,      0.0024,      0.0111,     -0.0289,      0.0320,\n",
       "             -0.0348,     -0.0220,      0.0347,      0.0114,     -0.0345,\n",
       "              0.0152,      0.0131,      0.0035,     -0.0309,     -0.0285,\n",
       "             -0.0309,      0.0349,     -0.0236,      0.0187,     -0.0357,\n",
       "              0.0048,     -0.0243,     -0.0191,     -0.0109,      0.0113,\n",
       "              0.0087,     -0.0121,     -0.0083,      0.0205,      0.0173,\n",
       "              0.0035,     -0.0226,      0.0033,      0.0257,      0.0313,\n",
       "              0.0150,     -0.0254,      0.0203,     -0.0345,     -0.0279,\n",
       "              0.0347,     -0.0129,     -0.0255,     -0.0041,     -0.0334,\n",
       "              0.0327,      0.0294,      0.0335,      0.0305,      0.0158,\n",
       "              0.0164,     -0.0243,     -0.0320,      0.0128,     -0.0062,\n",
       "             -0.0303,      0.0163,     -0.0086,     -0.0302,      0.0178,\n",
       "              0.0299,      0.0107,      0.0129,     -0.0015,      0.0256,\n",
       "              0.0288,     -0.0176,      0.0169,     -0.0212,      0.0298,\n",
       "              0.0093,     -0.0038,      0.0071,      0.0345,      0.0162,\n",
       "             -0.0266,     -0.0236,     -0.0048,      0.0111,     -0.0355,\n",
       "              0.0031,      0.0295,     -0.0153,      0.0218,      0.0194,\n",
       "              0.0052,      0.0320,      0.0116,      0.0102,      0.0130,\n",
       "              0.0306,      0.0068,     -0.0304,      0.0034,     -0.0360,\n",
       "             -0.0039,      0.0123,     -0.0009,     -0.0161,      0.0338,\n",
       "             -0.0315,      0.0171,     -0.0050,      0.0059,     -0.0224,\n",
       "              0.0076,      0.0116,      0.0069,     -0.0340,      0.0352,\n",
       "              0.0043,      0.0243,      0.0229,      0.0340,      0.0083,\n",
       "              0.0253,     -0.0194,      0.0313,     -0.0321,     -0.0353,\n",
       "             -0.0198,     -0.0240,      0.0245,      0.0217,     -0.0238,\n",
       "              0.0184,     -0.0096,      0.0167,      0.0360,     -0.0091,\n",
       "             -0.0172,     -0.0170,      0.0078,      0.0356,      0.0136,\n",
       "              0.0226,      0.0083,      0.0235,     -0.0085,     -0.0089,\n",
       "             -0.0126,      0.0121,      0.0172,      0.0051,     -0.0163,\n",
       "              0.0105,     -0.0061,     -0.0306,      0.0196,     -0.0026,\n",
       "             -0.0263,      0.0033,      0.0294,      0.0263,      0.0056,\n",
       "             -0.0127,      0.0076,      0.0214,      0.0341,     -0.0091,\n",
       "              0.0011,     -0.0239,     -0.0192,     -0.0357,      0.0333,\n",
       "              0.0173,     -0.0118,     -0.0227,      0.0142,     -0.0072,\n",
       "             -0.0260,     -0.0124,      0.0179,     -0.0353,     -0.0045,\n",
       "              0.0054,     -0.0324,     -0.0290,      0.0110,      0.0253,\n",
       "             -0.0272,     -0.0129,     -0.0077,     -0.0152,     -0.0273,\n",
       "              0.0248,      0.0186,      0.0139,      0.0189,     -0.0131,\n",
       "              0.0111,      0.0266,     -0.0293,      0.0103,      0.0264,\n",
       "             -0.0075,      0.0238,      0.0005,     -0.0335,     -0.0080,\n",
       "             -0.0153,     -0.0213,      0.0061,     -0.0292,     -0.0002,\n",
       "             -0.0254,      0.0033,     -0.0163,     -0.0161,     -0.0080,\n",
       "             -0.0055,     -0.0036,      0.0260,     -0.0271,      0.0149,\n",
       "              0.0285,      0.0100,      0.0209,      0.0282,     -0.0085,\n",
       "             -0.0298,     -0.0339,     -0.0210,     -0.0162,      0.0169,\n",
       "              0.0073,      0.0351,      0.0265,      0.0345,     -0.0055,\n",
       "             -0.0038,     -0.0213,      0.0265,      0.0190,      0.0147,\n",
       "             -0.0274,      0.0221,      0.0343,      0.0178,     -0.0327,\n",
       "             -0.0204,     -0.0355,     -0.0121,     -0.0094,     -0.0339,\n",
       "             -0.0329,      0.0142,      0.0080,     -0.0283,     -0.0226,\n",
       "              0.0159,      0.0209,      0.0323,      0.0223,      0.0302,\n",
       "             -0.0230,     -0.0041,     -0.0016,      0.0076,     -0.0162,\n",
       "             -0.0348,     -0.0333,      0.0067,      0.0128,      0.0110,\n",
       "              0.0274,     -0.0089,      0.0008,     -0.0160,      0.0050,\n",
       "             -0.0239,     -0.0152,     -0.0299,      0.0184,     -0.0006,\n",
       "             -0.0172,      0.0171,     -0.0334,      0.0226,      0.0027,\n",
       "             -0.0088,     -0.0276,     -0.0103,     -0.0322,      0.0123,\n",
       "             -0.0191,      0.0317,     -0.0010,      0.0104,     -0.0250,\n",
       "             -0.0073,     -0.0060,     -0.0161,     -0.0053,      0.0203,\n",
       "              0.0290,     -0.0357,      0.0333,     -0.0127,     -0.0211,\n",
       "             -0.0324,      0.0297,      0.0149,      0.0044,     -0.0280,\n",
       "              0.0013,      0.0226,      0.0360,     -0.0184,      0.0346,\n",
       "              0.0001,     -0.0112,      0.0217,      0.0324,     -0.0196,\n",
       "              0.0121,      0.0104,      0.0291,      0.0109,      0.0299,\n",
       "             -0.0082,      0.0127,     -0.0020,     -0.0104,     -0.0239,\n",
       "              0.0157,      0.0230,     -0.0286,     -0.0152,     -0.0284,\n",
       "              0.0240,     -0.0314,      0.0188,     -0.0117,      0.0358,\n",
       "              0.0236,     -0.0245,      0.0219,      0.0238,     -0.0272,\n",
       "             -0.0046,      0.0027,     -0.0199,      0.0127,      0.0290,\n",
       "             -0.0091,     -0.0106,     -0.0207,     -0.0031,     -0.0324,\n",
       "              0.0056,     -0.0159,      0.0191,     -0.0315,     -0.0190,\n",
       "              0.0338,      0.0214,     -0.0271], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0041,  0.0011,  0.0096,  ..., -0.0279, -0.0243,  0.0174],\n",
       "         [ 0.0003, -0.0154, -0.0017,  ...,  0.0025,  0.0207, -0.0273],\n",
       "         [ 0.0210,  0.0203,  0.0179,  ...,  0.0287,  0.0254, -0.0291],\n",
       "         ...,\n",
       "         [ 0.0221,  0.0145, -0.0201,  ...,  0.0289, -0.0227,  0.0230],\n",
       "         [-0.0066, -0.0014, -0.0206,  ..., -0.0157,  0.0284,  0.0276],\n",
       "         [-0.0262,  0.0339, -0.0178,  ...,  0.0269,  0.0352,  0.0254]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0341, -0.0105, -0.0086,  ..., -0.0077, -0.0293,  0.0175],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[    -0.0013,      0.0076,     -0.0064,  ...,      0.0126,\n",
       "               0.0165,     -0.0157],\n",
       "         [     0.0000,      0.0099,     -0.0043,  ...,      0.0040,\n",
       "               0.0028,      0.0123],\n",
       "         [    -0.0008,      0.0022,     -0.0024,  ...,     -0.0153,\n",
       "               0.0159,      0.0132],\n",
       "         ...,\n",
       "         [     0.0080,     -0.0140,     -0.0100,  ...,      0.0154,\n",
       "              -0.0175,     -0.0056],\n",
       "         [     0.0118,     -0.0081,     -0.0125,  ...,      0.0136,\n",
       "               0.0148,      0.0093],\n",
       "         [     0.0090,      0.0133,      0.0171,  ...,     -0.0085,\n",
       "               0.0177,     -0.0086]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([    -0.0024,     -0.0172,     -0.0004,     -0.0139,     -0.0158,\n",
       "              0.0052,      0.0019,      0.0085,     -0.0115,      0.0002,\n",
       "             -0.0024,     -0.0130,     -0.0109,     -0.0001,      0.0102,\n",
       "             -0.0017,      0.0148,      0.0166,      0.0155,     -0.0113,\n",
       "              0.0039,     -0.0041,      0.0031,      0.0052,     -0.0062,\n",
       "             -0.0074,     -0.0125,      0.0088,     -0.0066,     -0.0072,\n",
       "             -0.0086,      0.0156,      0.0156,     -0.0156,      0.0042,\n",
       "              0.0013,     -0.0177,      0.0040,      0.0009,     -0.0023,\n",
       "             -0.0020,      0.0102,     -0.0059,      0.0069,     -0.0047,\n",
       "             -0.0072,      0.0034,      0.0017,      0.0073,     -0.0162,\n",
       "             -0.0059,      0.0019,     -0.0136,     -0.0128,     -0.0070,\n",
       "             -0.0173,      0.0121,     -0.0165,      0.0146,      0.0123,\n",
       "             -0.0045,     -0.0121,     -0.0115,      0.0090,     -0.0126,\n",
       "             -0.0141,     -0.0090,     -0.0063,     -0.0167,     -0.0132,\n",
       "              0.0179,      0.0009,     -0.0180,      0.0130,     -0.0130,\n",
       "             -0.0030,      0.0002,      0.0132,      0.0081,     -0.0138,\n",
       "             -0.0056,     -0.0115,     -0.0100,     -0.0059,      0.0032,\n",
       "             -0.0089,     -0.0042,     -0.0172,      0.0130,      0.0006,\n",
       "             -0.0079,     -0.0158,      0.0070,      0.0107,      0.0019,\n",
       "              0.0180,      0.0177,     -0.0115,     -0.0101,     -0.0047,\n",
       "              0.0158,      0.0116,     -0.0035,      0.0039,     -0.0163,\n",
       "             -0.0050,     -0.0115,      0.0088,      0.0172,     -0.0047,\n",
       "              0.0018,      0.0029,     -0.0001,     -0.0099,      0.0097,\n",
       "             -0.0168,     -0.0150,     -0.0130,     -0.0050,      0.0119,\n",
       "             -0.0082,      0.0038,     -0.0086,      0.0160,     -0.0117,\n",
       "             -0.0070,     -0.0001,     -0.0167,     -0.0061,     -0.0054,\n",
       "             -0.0048,     -0.0069,     -0.0010,      0.0056,     -0.0135,\n",
       "              0.0078,     -0.0057,     -0.0141,     -0.0075,      0.0177,\n",
       "              0.0046,      0.0053,      0.0176,     -0.0082,      0.0107,\n",
       "             -0.0066,      0.0157,      0.0062,     -0.0127,      0.0160,\n",
       "              0.0102,      0.0030,     -0.0108,     -0.0159,      0.0137,\n",
       "             -0.0119,      0.0167,      0.0014,      0.0165,      0.0149,\n",
       "             -0.0163,     -0.0090,     -0.0046,      0.0017,     -0.0100,\n",
       "              0.0008,      0.0119,     -0.0021,      0.0082,      0.0097,\n",
       "              0.0091,      0.0081,     -0.0032,     -0.0112,      0.0050,\n",
       "             -0.0155,      0.0045,      0.0133,      0.0074,     -0.0013,\n",
       "             -0.0172,     -0.0167,     -0.0006,     -0.0006,     -0.0114,\n",
       "              0.0070,     -0.0068,     -0.0154,     -0.0141,      0.0134,\n",
       "              0.0166,      0.0019,      0.0135,      0.0167,     -0.0017,\n",
       "             -0.0027,     -0.0045,      0.0111,      0.0138,      0.0140,\n",
       "             -0.0044,     -0.0151,      0.0034,     -0.0084,     -0.0136,\n",
       "             -0.0024,      0.0040,     -0.0076,      0.0162,     -0.0166,\n",
       "             -0.0021,     -0.0003,     -0.0019,     -0.0147,     -0.0080,\n",
       "             -0.0107,      0.0116,     -0.0023,     -0.0171,      0.0084,\n",
       "             -0.0138,      0.0031,     -0.0141,     -0.0118,      0.0173,\n",
       "             -0.0071,      0.0105,     -0.0151,     -0.0166,     -0.0070,\n",
       "             -0.0149,     -0.0129,      0.0027,      0.0139,     -0.0177,\n",
       "              0.0103,     -0.0170,      0.0035,     -0.0051,      0.0169,\n",
       "             -0.0054,     -0.0089,     -0.0166,     -0.0080,      0.0097,\n",
       "             -0.0054,      0.0085,      0.0177,      0.0060,      0.0018,\n",
       "              0.0080,      0.0118,      0.0059,      0.0068,      0.0027,\n",
       "              0.0013,      0.0002,      0.0141,      0.0068,      0.0176,\n",
       "             -0.0129,      0.0071,      0.0016,      0.0022,     -0.0171,\n",
       "             -0.0134,      0.0047,     -0.0010,      0.0000,      0.0119,\n",
       "              0.0012,      0.0067,     -0.0105,     -0.0148,     -0.0067,\n",
       "             -0.0036,     -0.0010,      0.0052,     -0.0175,      0.0143,\n",
       "             -0.0043,      0.0038,      0.0095,      0.0125,     -0.0153,\n",
       "             -0.0080,      0.0105,      0.0054,      0.0158,      0.0115,\n",
       "              0.0010,      0.0043,      0.0043,      0.0166,     -0.0036,\n",
       "              0.0124,      0.0155,      0.0115,      0.0159,     -0.0162,\n",
       "              0.0032,     -0.0167,     -0.0092,      0.0055,      0.0129,\n",
       "             -0.0108,      0.0178,      0.0010,      0.0066,     -0.0040,\n",
       "              0.0025,     -0.0074,     -0.0009,      0.0025,     -0.0007,\n",
       "             -0.0112,      0.0113,      0.0059,      0.0049,     -0.0163,\n",
       "             -0.0123,     -0.0045,     -0.0123,     -0.0070,     -0.0166,\n",
       "              0.0037,     -0.0085,      0.0003,     -0.0177,      0.0032,\n",
       "              0.0164,      0.0017,      0.0050,      0.0088,     -0.0108,\n",
       "             -0.0057,     -0.0159,     -0.0104,     -0.0095,     -0.0153,\n",
       "              0.0156,      0.0145,     -0.0150,     -0.0162,     -0.0017,\n",
       "             -0.0053,     -0.0050,     -0.0020,      0.0045,     -0.0020,\n",
       "              0.0082,     -0.0021,      0.0041,     -0.0177,     -0.0086,\n",
       "             -0.0074,      0.0122,     -0.0158,      0.0113,      0.0087,\n",
       "              0.0015,     -0.0120,     -0.0087,      0.0170,     -0.0114,\n",
       "              0.0015,      0.0011,      0.0053,     -0.0109,     -0.0160,\n",
       "             -0.0009,      0.0077,     -0.0079,      0.0143,     -0.0171,\n",
       "             -0.0116,      0.0098,      0.0104,     -0.0038,     -0.0043,\n",
       "             -0.0122,     -0.0041,     -0.0008,      0.0028,     -0.0114,\n",
       "             -0.0078,     -0.0012,      0.0139,      0.0054,     -0.0118,\n",
       "             -0.0173,      0.0060,      0.0024,     -0.0003,     -0.0067,\n",
       "             -0.0152,     -0.0008,      0.0170,      0.0003,      0.0000,\n",
       "              0.0066,      0.0120,      0.0105,      0.0143,     -0.0090,\n",
       "              0.0031,     -0.0001,      0.0161,      0.0013,      0.0089,\n",
       "              0.0022,     -0.0149,     -0.0003,     -0.0095,      0.0151,\n",
       "              0.0005,      0.0037,      0.0101,     -0.0075,     -0.0025,\n",
       "              0.0150,     -0.0112,      0.0114,     -0.0016,     -0.0094,\n",
       "             -0.0087,     -0.0153,      0.0138,      0.0021,     -0.0024,\n",
       "             -0.0019,      0.0002,      0.0140,      0.0133,      0.0057,\n",
       "             -0.0025,     -0.0029,      0.0072,     -0.0148,      0.0066,\n",
       "             -0.0128,     -0.0022,     -0.0067,     -0.0016,      0.0072,\n",
       "             -0.0126,      0.0093,     -0.0062,      0.0038,     -0.0153,\n",
       "             -0.0068,     -0.0030,     -0.0158,      0.0164,     -0.0145,\n",
       "             -0.0111,      0.0127,      0.0038,      0.0128,     -0.0106,\n",
       "              0.0134,     -0.0038,      0.0160,      0.0138,     -0.0097,\n",
       "              0.0088,      0.0128,     -0.0058,      0.0104,      0.0086,\n",
       "             -0.0105,      0.0000,     -0.0095,      0.0100,     -0.0127,\n",
       "              0.0139,      0.0009,     -0.0103,      0.0089,      0.0033,\n",
       "             -0.0074,      0.0037,      0.0106,      0.0004,      0.0056,\n",
       "              0.0008,      0.0134,     -0.0162,     -0.0157,     -0.0048,\n",
       "              0.0152,     -0.0083,     -0.0014,      0.0177,     -0.0105,\n",
       "              0.0174,     -0.0179,     -0.0170,      0.0143,     -0.0073,\n",
       "              0.0066,     -0.0039,     -0.0033,     -0.0092,      0.0026,\n",
       "              0.0042,      0.0083,     -0.0085,      0.0032,      0.0035,\n",
       "             -0.0097,      0.0056,      0.0098,     -0.0108,      0.0057,\n",
       "             -0.0178,     -0.0020,      0.0056,     -0.0104,      0.0054,\n",
       "              0.0047,     -0.0101,     -0.0009,      0.0011,      0.0005,\n",
       "              0.0107,     -0.0177,     -0.0149,     -0.0162,      0.0074,\n",
       "             -0.0148,     -0.0125,      0.0141,     -0.0154,      0.0160,\n",
       "             -0.0021,     -0.0125,     -0.0053,     -0.0117,      0.0144,\n",
       "             -0.0141,      0.0155,      0.0029,      0.0170,     -0.0162,\n",
       "             -0.0025,     -0.0098,      0.0106,      0.0019,      0.0036,\n",
       "             -0.0021,     -0.0033,     -0.0123,      0.0031,      0.0129,\n",
       "             -0.0116,      0.0067,      0.0160,     -0.0137,     -0.0040,\n",
       "              0.0176,      0.0162,      0.0031,      0.0102,     -0.0039,\n",
       "              0.0171,     -0.0030,      0.0084,     -0.0096,     -0.0049,\n",
       "              0.0159,      0.0131,      0.0143,      0.0097,     -0.0136,\n",
       "             -0.0132,      0.0141,     -0.0059,     -0.0074,      0.0008,\n",
       "             -0.0140,      0.0058,     -0.0158,      0.0157,      0.0126,\n",
       "             -0.0019,      0.0132,      0.0020,      0.0103,     -0.0008,\n",
       "             -0.0170,      0.0036,      0.0011,      0.0146,     -0.0064,\n",
       "              0.0023,     -0.0011,     -0.0119,      0.0081,      0.0129,\n",
       "             -0.0065,      0.0055,      0.0110,     -0.0132,      0.0000,\n",
       "             -0.0108,     -0.0057,     -0.0029,      0.0149,     -0.0130,\n",
       "             -0.0125,     -0.0018,      0.0059,     -0.0152,      0.0143,\n",
       "              0.0170,     -0.0114,      0.0109,      0.0124,      0.0129,\n",
       "             -0.0111,      0.0042,      0.0116,     -0.0012,      0.0098,\n",
       "              0.0096,      0.0097,      0.0081,     -0.0024,      0.0179,\n",
       "             -0.0079,      0.0152,      0.0024,      0.0077,      0.0163,\n",
       "              0.0032,      0.0100,      0.0109,      0.0103,      0.0054,\n",
       "             -0.0147,     -0.0021,      0.0158,      0.0011,     -0.0069,\n",
       "             -0.0105,      0.0004,      0.0159,     -0.0049,     -0.0169,\n",
       "             -0.0098,     -0.0028,      0.0155,      0.0001,      0.0033,\n",
       "             -0.0168,      0.0064,     -0.0001,      0.0055,     -0.0076,\n",
       "              0.0042,     -0.0098,     -0.0035,     -0.0114,      0.0098,\n",
       "              0.0072,     -0.0122,     -0.0153,      0.0139,      0.0158,\n",
       "              0.0116,     -0.0027,     -0.0099,      0.0141,      0.0177,\n",
       "             -0.0057,     -0.0118,      0.0155,     -0.0073,      0.0030,\n",
       "             -0.0151,      0.0157,      0.0045,      0.0098,      0.0163,\n",
       "             -0.0151,     -0.0074,     -0.0160,      0.0038,      0.0099,\n",
       "             -0.0061,      0.0153,      0.0029,      0.0168,     -0.0030,\n",
       "             -0.0080,     -0.0089,     -0.0116,     -0.0096,      0.0168,\n",
       "              0.0077,      0.0026,      0.0079,      0.0038,      0.0009,\n",
       "             -0.0071,      0.0098,     -0.0116,      0.0038,      0.0030,\n",
       "              0.0083,     -0.0014,     -0.0179,      0.0062,      0.0110,\n",
       "             -0.0173,      0.0082,      0.0075,     -0.0140,      0.0081,\n",
       "             -0.0077,     -0.0135,     -0.0159,     -0.0064,      0.0069,\n",
       "             -0.0050,      0.0013,      0.0089,     -0.0065,      0.0068,\n",
       "              0.0179,     -0.0076,     -0.0165,      0.0165,     -0.0167,\n",
       "             -0.0137,      0.0012,      0.0109,      0.0112,     -0.0129,\n",
       "              0.0118,      0.0065,     -0.0006,      0.0001,      0.0021,\n",
       "             -0.0177,      0.0089,     -0.0057,      0.0148,      0.0071,\n",
       "              0.0041,      0.0128,     -0.0136,      0.0015,      0.0101,\n",
       "              0.0064,     -0.0149,     -0.0098,      0.0144,      0.0040,\n",
       "             -0.0118,      0.0104,     -0.0030,      0.0167,      0.0033,\n",
       "             -0.0089,      0.0100,     -0.0083], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0218,  0.0157, -0.0052,  ..., -0.0102, -0.0070,  0.0110],\n",
       "         [-0.0183,  0.0137, -0.0168,  ...,  0.0355, -0.0243,  0.0099],\n",
       "         [-0.0015,  0.0154, -0.0284,  ..., -0.0197,  0.0325,  0.0103],\n",
       "         ...,\n",
       "         [-0.0150,  0.0132, -0.0172,  ...,  0.0158,  0.0275, -0.0225],\n",
       "         [ 0.0229,  0.0195,  0.0049,  ..., -0.0276,  0.0347, -0.0327],\n",
       "         [ 0.0247, -0.0060, -0.0313,  ..., -0.0105, -0.0155, -0.0093]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0033, -0.0340,  0.0082,  ..., -0.0173,  0.0344,  0.0237],\n",
       "         [ 0.0181,  0.0151, -0.0202,  ..., -0.0068,  0.0197,  0.0260],\n",
       "         [ 0.0214,  0.0274, -0.0113,  ..., -0.0313, -0.0011, -0.0014],\n",
       "         ...,\n",
       "         [-0.0306, -0.0021,  0.0100,  ...,  0.0027,  0.0346,  0.0122],\n",
       "         [ 0.0038, -0.0057, -0.0311,  ...,  0.0237, -0.0116, -0.0252],\n",
       "         [-0.0309,  0.0040, -0.0204,  ...,  0.0053, -0.0109, -0.0350]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0159, -0.0228,  0.0114,  ...,  0.0277, -0.0017,  0.0192],\n",
       "         [-0.0252,  0.0179, -0.0030,  ..., -0.0236,  0.0088,  0.0267],\n",
       "         [-0.0359, -0.0337,  0.0062,  ..., -0.0358, -0.0073,  0.0054],\n",
       "         ...,\n",
       "         [ 0.0328,  0.0333,  0.0250,  ..., -0.0325, -0.0030,  0.0082],\n",
       "         [ 0.0045,  0.0086, -0.0351,  ...,  0.0306,  0.0137,  0.0110],\n",
       "         [-0.0301, -0.0192,  0.0042,  ...,  0.0277, -0.0067, -0.0361]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0074, -0.0011, -0.0329,  ..., -0.0311, -0.0188, -0.0204],\n",
       "         [ 0.0199, -0.0027, -0.0025,  ..., -0.0221, -0.0094, -0.0104],\n",
       "         [ 0.0068, -0.0313, -0.0132,  ..., -0.0304,  0.0314,  0.0091],\n",
       "         ...,\n",
       "         [ 0.0004, -0.0243,  0.0056,  ...,  0.0210,  0.0209, -0.0047],\n",
       "         [ 0.0208,  0.0066, -0.0353,  ..., -0.0258, -0.0111,  0.0320],\n",
       "         [ 0.0091,  0.0099, -0.0306,  ..., -0.0266,  0.0025, -0.0083]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([     0.0005,     -0.0110,     -0.0060,     -0.0155,     -0.0269,\n",
       "              0.0278,      0.0276,      0.0149,     -0.0249,     -0.0257,\n",
       "              0.0319,      0.0185,      0.0290,     -0.0327,     -0.0196,\n",
       "             -0.0087,     -0.0013,      0.0113,      0.0112,     -0.0191,\n",
       "             -0.0188,     -0.0156,     -0.0009,      0.0215,     -0.0150,\n",
       "              0.0256,      0.0230,     -0.0065,      0.0024,     -0.0236,\n",
       "             -0.0263,      0.0325,      0.0091,      0.0035,      0.0326,\n",
       "              0.0204,      0.0063,     -0.0264,     -0.0268,     -0.0346,\n",
       "              0.0316,      0.0236,      0.0052,     -0.0217,     -0.0285,\n",
       "             -0.0326,     -0.0084,      0.0128,      0.0080,     -0.0059,\n",
       "              0.0186,     -0.0059,      0.0178,      0.0245,      0.0006,\n",
       "             -0.0231,      0.0046,      0.0099,     -0.0283,      0.0220,\n",
       "             -0.0167,     -0.0035,      0.0121,      0.0299,      0.0099,\n",
       "             -0.0201,      0.0177,      0.0158,     -0.0059,     -0.0125,\n",
       "             -0.0168,     -0.0101,     -0.0032,     -0.0149,      0.0340,\n",
       "             -0.0328,     -0.0301,     -0.0225,     -0.0271,      0.0328,\n",
       "              0.0210,      0.0209,      0.0107,      0.0339,     -0.0060,\n",
       "             -0.0175,     -0.0239,     -0.0003,      0.0141,      0.0315,\n",
       "              0.0217,     -0.0073,     -0.0018,     -0.0270,     -0.0355,\n",
       "             -0.0302,      0.0345,      0.0317,      0.0244,      0.0243,\n",
       "              0.0346,     -0.0301,      0.0122,     -0.0012,     -0.0237,\n",
       "              0.0108,      0.0281,     -0.0291,     -0.0307,     -0.0109,\n",
       "              0.0231,     -0.0186,     -0.0246,      0.0081,      0.0238,\n",
       "              0.0030,      0.0231,      0.0217,      0.0213,     -0.0248,\n",
       "              0.0165,     -0.0063,     -0.0039,     -0.0262,      0.0125,\n",
       "              0.0197,     -0.0185,     -0.0275,     -0.0309,      0.0160,\n",
       "              0.0138,     -0.0251,     -0.0215,     -0.0124,     -0.0317,\n",
       "              0.0270,      0.0023,     -0.0345,      0.0339,      0.0157,\n",
       "             -0.0331,      0.0013,     -0.0348,     -0.0053,     -0.0202,\n",
       "             -0.0223,      0.0283,     -0.0278,     -0.0096,      0.0031,\n",
       "             -0.0168,     -0.0185,     -0.0062,     -0.0161,      0.0023,\n",
       "              0.0043,      0.0221,      0.0153,      0.0110,      0.0328,\n",
       "              0.0300,     -0.0102,      0.0251,      0.0112,      0.0128,\n",
       "             -0.0259,     -0.0308,      0.0202,     -0.0100,     -0.0256,\n",
       "             -0.0068,     -0.0053,      0.0039,     -0.0322,      0.0233,\n",
       "              0.0229,      0.0060,      0.0261,     -0.0270,     -0.0054,\n",
       "             -0.0162,     -0.0249,     -0.0203,     -0.0046,      0.0323,\n",
       "              0.0194,     -0.0281,      0.0212,      0.0220,     -0.0321,\n",
       "             -0.0018,      0.0125,      0.0252,     -0.0085,     -0.0247,\n",
       "              0.0064,      0.0053,      0.0319,     -0.0254,      0.0328,\n",
       "              0.0332,     -0.0083,     -0.0190,     -0.0037,     -0.0074,\n",
       "              0.0347,      0.0303,     -0.0119,      0.0084,      0.0209,\n",
       "             -0.0340,      0.0055,      0.0036,     -0.0266,      0.0057,\n",
       "              0.0100,     -0.0029,     -0.0194,     -0.0151,      0.0034,\n",
       "             -0.0213,     -0.0343,      0.0031,      0.0252,     -0.0341,\n",
       "              0.0344,     -0.0038,     -0.0301,      0.0283,      0.0051,\n",
       "              0.0055,     -0.0132,     -0.0243,      0.0066,     -0.0081,\n",
       "             -0.0351,     -0.0306,     -0.0106,     -0.0072,     -0.0130,\n",
       "             -0.0176,     -0.0244,     -0.0253,     -0.0216,      0.0236,\n",
       "             -0.0312,      0.0073,      0.0053,      0.0313,      0.0242,\n",
       "             -0.0274,     -0.0337,      0.0299,      0.0239,      0.0148,\n",
       "              0.0279,      0.0273,      0.0026,      0.0064,     -0.0092,\n",
       "              0.0173,     -0.0351,     -0.0231,     -0.0339,      0.0167,\n",
       "             -0.0221,     -0.0337,     -0.0141,      0.0032,     -0.0202,\n",
       "              0.0325,     -0.0209,      0.0174,      0.0077,      0.0238,\n",
       "              0.0268,     -0.0014,      0.0019,      0.0264,      0.0190,\n",
       "              0.0068,     -0.0218,      0.0343,      0.0219,     -0.0057,\n",
       "              0.0326,      0.0149,      0.0209,     -0.0135,      0.0239,\n",
       "              0.0056,     -0.0269,     -0.0346,     -0.0328,      0.0267,\n",
       "              0.0323,      0.0025,     -0.0193,      0.0270,     -0.0018,\n",
       "              0.0079,      0.0308,     -0.0197,      0.0002,     -0.0162,\n",
       "             -0.0354,      0.0332,     -0.0263,     -0.0114,      0.0182,\n",
       "              0.0028,     -0.0057,     -0.0149,      0.0087,     -0.0161,\n",
       "              0.0275,     -0.0084,     -0.0194,      0.0357,     -0.0065,\n",
       "             -0.0076,     -0.0116,     -0.0218,     -0.0070,      0.0350,\n",
       "             -0.0330,     -0.0008,      0.0090,     -0.0256,     -0.0109,\n",
       "             -0.0029,      0.0021,     -0.0109,      0.0125,      0.0040,\n",
       "              0.0112,      0.0094,      0.0265,      0.0067,      0.0116,\n",
       "             -0.0107,     -0.0206,     -0.0145,     -0.0046,      0.0253,\n",
       "             -0.0201,     -0.0268,     -0.0017,      0.0032,     -0.0336,\n",
       "              0.0019,     -0.0310,      0.0225,      0.0138,      0.0049,\n",
       "             -0.0178,     -0.0267,      0.0174,      0.0290,     -0.0318,\n",
       "             -0.0205,     -0.0066,      0.0059,      0.0148,      0.0105,\n",
       "              0.0123,     -0.0036,      0.0037,     -0.0015,      0.0026,\n",
       "             -0.0072,      0.0266,     -0.0252,     -0.0110,      0.0106,\n",
       "              0.0149,      0.0252,     -0.0004,     -0.0349,      0.0002,\n",
       "             -0.0155,      0.0347,     -0.0228,      0.0346,      0.0358,\n",
       "              0.0171,      0.0173,     -0.0122,     -0.0132,      0.0092,\n",
       "             -0.0115,      0.0054,      0.0198,     -0.0214,      0.0052,\n",
       "             -0.0055,      0.0235,      0.0291,      0.0038,     -0.0270,\n",
       "             -0.0263,     -0.0224,      0.0317,     -0.0077,     -0.0277,\n",
       "              0.0130,      0.0005,      0.0002,     -0.0273,     -0.0159,\n",
       "              0.0275,      0.0344,     -0.0304,      0.0336,     -0.0336,\n",
       "             -0.0048,      0.0295,      0.0022,     -0.0194,      0.0300,\n",
       "             -0.0230,      0.0042,      0.0199,      0.0043,     -0.0229,\n",
       "              0.0352,      0.0277,      0.0109,     -0.0339,     -0.0198,\n",
       "              0.0340,      0.0111,     -0.0214,     -0.0036,     -0.0340,\n",
       "              0.0078,     -0.0330,      0.0245,     -0.0004,     -0.0193,\n",
       "              0.0111,     -0.0015,     -0.0012,     -0.0222,      0.0102,\n",
       "              0.0069,     -0.0297,     -0.0107,      0.0360,     -0.0103,\n",
       "             -0.0121,     -0.0187,     -0.0014,     -0.0281,      0.0243,\n",
       "              0.0013,      0.0077,     -0.0251,      0.0055,      0.0128,\n",
       "             -0.0200,      0.0290,      0.0123,     -0.0313,     -0.0226,\n",
       "             -0.0301,     -0.0249,      0.0032,     -0.0312,     -0.0114,\n",
       "              0.0151,     -0.0282,     -0.0341,     -0.0043,     -0.0066,\n",
       "             -0.0296,      0.0138,     -0.0228,     -0.0214,      0.0310,\n",
       "              0.0127,     -0.0027,      0.0232,      0.0342,      0.0129,\n",
       "              0.0033,      0.0066,     -0.0049,     -0.0185,      0.0102,\n",
       "              0.0319,      0.0189,     -0.0075,      0.0241,      0.0166,\n",
       "              0.0179,      0.0156,     -0.0016,     -0.0293,     -0.0032,\n",
       "              0.0217,     -0.0273,      0.0173,      0.0069,      0.0085,\n",
       "             -0.0046,     -0.0002,     -0.0196,      0.0013,      0.0046,\n",
       "             -0.0004,     -0.0247,     -0.0293,      0.0232,      0.0357,\n",
       "              0.0343,      0.0143,      0.0326,     -0.0226,     -0.0285,\n",
       "              0.0275,      0.0221,     -0.0262,      0.0050,     -0.0163,\n",
       "             -0.0222,      0.0147,     -0.0184,     -0.0276,     -0.0163,\n",
       "             -0.0028,     -0.0249,      0.0333,      0.0330,      0.0322,\n",
       "              0.0126,      0.0065,     -0.0189,      0.0011,      0.0083,\n",
       "              0.0317,     -0.0092,      0.0325,     -0.0352,      0.0228,\n",
       "             -0.0305,     -0.0311,      0.0254,      0.0041,      0.0056,\n",
       "             -0.0280,      0.0347,      0.0007,      0.0176,      0.0237,\n",
       "             -0.0222,      0.0360,     -0.0341,     -0.0052,     -0.0010,\n",
       "             -0.0315,     -0.0341,      0.0116,      0.0287,      0.0179,\n",
       "             -0.0172,     -0.0153,     -0.0261,     -0.0132,     -0.0065,\n",
       "              0.0260,     -0.0067,     -0.0087,      0.0335,     -0.0168,\n",
       "             -0.0010,     -0.0148,     -0.0303,     -0.0298,      0.0032,\n",
       "             -0.0300,      0.0194,     -0.0058,     -0.0340,     -0.0209,\n",
       "             -0.0290,     -0.0210,     -0.0234,     -0.0040,      0.0187,\n",
       "             -0.0134,      0.0250,      0.0050,      0.0214,     -0.0002,\n",
       "             -0.0080,     -0.0226,      0.0257,     -0.0051,      0.0185,\n",
       "             -0.0254,      0.0350,     -0.0158,      0.0117,     -0.0167,\n",
       "             -0.0073,      0.0196,      0.0303,     -0.0191,     -0.0057,\n",
       "              0.0359,      0.0259,      0.0238,      0.0161,     -0.0047,\n",
       "              0.0019,     -0.0148,     -0.0042,      0.0305,     -0.0287,\n",
       "             -0.0246,     -0.0140,     -0.0307,     -0.0110,     -0.0101,\n",
       "             -0.0079,      0.0176,      0.0276,     -0.0050,     -0.0289,\n",
       "              0.0081,     -0.0000,      0.0327,      0.0241,     -0.0263,\n",
       "             -0.0309,      0.0247,     -0.0088,     -0.0017,      0.0078,\n",
       "              0.0343,      0.0086,      0.0026,      0.0066,      0.0180,\n",
       "              0.0019,     -0.0226,      0.0015,      0.0274,     -0.0040,\n",
       "              0.0238,     -0.0235,     -0.0171,      0.0207,     -0.0220,\n",
       "              0.0036,     -0.0136,     -0.0281,     -0.0219,     -0.0239,\n",
       "              0.0158,      0.0187,      0.0040,      0.0165,     -0.0138,\n",
       "              0.0235,     -0.0018,      0.0077,     -0.0059,     -0.0168,\n",
       "              0.0014,      0.0081,     -0.0207,     -0.0303,      0.0154,\n",
       "              0.0230,     -0.0191,      0.0033,      0.0268,     -0.0340,\n",
       "             -0.0354,     -0.0324,      0.0080,     -0.0180,      0.0262,\n",
       "              0.0045,      0.0105,     -0.0026,     -0.0265,     -0.0047,\n",
       "              0.0288,     -0.0347,      0.0341,     -0.0317,     -0.0047,\n",
       "              0.0105,      0.0141,      0.0086,     -0.0027,     -0.0078,\n",
       "              0.0186,     -0.0127,      0.0176,      0.0256,     -0.0120,\n",
       "             -0.0181,      0.0237,     -0.0330,     -0.0158,      0.0356,\n",
       "              0.0193,     -0.0235,      0.0149,      0.0106,      0.0346,\n",
       "             -0.0104,      0.0261,     -0.0041,     -0.0265,      0.0291,\n",
       "             -0.0094,      0.0197,     -0.0024,      0.0125,     -0.0267,\n",
       "              0.0039,     -0.0029,     -0.0269,     -0.0205,     -0.0138,\n",
       "             -0.0257,      0.0064,      0.0209,     -0.0058,     -0.0025,\n",
       "             -0.0224,      0.0291,     -0.0179,     -0.0147,     -0.0252,\n",
       "             -0.0264,      0.0303,     -0.0038,     -0.0194,      0.0099,\n",
       "             -0.0311,     -0.0337,      0.0282,      0.0150,     -0.0210,\n",
       "              0.0093,      0.0033,     -0.0157,      0.0046,     -0.0173,\n",
       "              0.0325,     -0.0015,     -0.0270,      0.0027,     -0.0260,\n",
       "              0.0306,     -0.0072,      0.0064,     -0.0175,      0.0211,\n",
       "             -0.0155,      0.0111,     -0.0270], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0236,  0.0329,  0.0056,  ..., -0.0239, -0.0155,  0.0090],\n",
       "         [-0.0269, -0.0242,  0.0339,  ..., -0.0207,  0.0123,  0.0086],\n",
       "         [-0.0327, -0.0184, -0.0063,  ...,  0.0202,  0.0024,  0.0073],\n",
       "         ...,\n",
       "         [-0.0050,  0.0224, -0.0290,  ..., -0.0230,  0.0152,  0.0150],\n",
       "         [-0.0216, -0.0329, -0.0166,  ..., -0.0223,  0.0340,  0.0194],\n",
       "         [-0.0090,  0.0087,  0.0226,  ..., -0.0185,  0.0204, -0.0006]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([     0.0237,      0.0146,      0.0001,  ...,     -0.0131,\n",
       "              0.0192,     -0.0241], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0040,  0.0016, -0.0104,  ...,  0.0059,  0.0136,  0.0098],\n",
       "         [-0.0174,  0.0092, -0.0096,  ..., -0.0154, -0.0019, -0.0101],\n",
       "         [ 0.0110, -0.0013,  0.0148,  ..., -0.0086,  0.0038, -0.0159],\n",
       "         ...,\n",
       "         [-0.0146,  0.0084,  0.0173,  ..., -0.0110,  0.0019,  0.0046],\n",
       "         [-0.0024, -0.0037,  0.0071,  ..., -0.0117,  0.0010, -0.0095],\n",
       "         [-0.0172, -0.0094,  0.0099,  ..., -0.0023,  0.0133,  0.0146]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([     0.0004,     -0.0114,      0.0047,      0.0152,     -0.0054,\n",
       "             -0.0003,      0.0079,     -0.0109,      0.0066,      0.0041,\n",
       "             -0.0149,      0.0065,     -0.0088,      0.0177,     -0.0002,\n",
       "             -0.0112,      0.0024,     -0.0008,      0.0069,      0.0000,\n",
       "             -0.0132,     -0.0121,      0.0075,      0.0036,     -0.0151,\n",
       "              0.0111,     -0.0110,      0.0026,      0.0081,      0.0102,\n",
       "              0.0170,     -0.0004,      0.0118,     -0.0119,     -0.0033,\n",
       "              0.0087,     -0.0106,     -0.0155,     -0.0089,      0.0040,\n",
       "             -0.0016,      0.0005,      0.0153,     -0.0047,      0.0006,\n",
       "              0.0108,     -0.0168,     -0.0140,     -0.0151,     -0.0042,\n",
       "             -0.0019,     -0.0151,     -0.0103,      0.0082,      0.0111,\n",
       "              0.0125,      0.0135,     -0.0111,     -0.0070,     -0.0150,\n",
       "              0.0145,     -0.0090,      0.0092,     -0.0117,      0.0099,\n",
       "             -0.0103,     -0.0180,      0.0071,      0.0131,      0.0142,\n",
       "             -0.0087,      0.0063,     -0.0147,     -0.0065,      0.0029,\n",
       "              0.0032,     -0.0007,     -0.0118,     -0.0143,     -0.0172,\n",
       "             -0.0173,      0.0053,     -0.0009,     -0.0099,     -0.0006,\n",
       "              0.0152,     -0.0174,      0.0155,      0.0132,     -0.0179,\n",
       "             -0.0021,      0.0138,      0.0166,      0.0017,     -0.0156,\n",
       "              0.0105,      0.0161,     -0.0140,     -0.0019,     -0.0116,\n",
       "             -0.0029,      0.0021,      0.0013,     -0.0148,     -0.0033,\n",
       "              0.0116,     -0.0100,     -0.0176,      0.0026,      0.0174,\n",
       "              0.0009,      0.0173,     -0.0134,     -0.0073,     -0.0176,\n",
       "             -0.0081,      0.0103,      0.0049,      0.0099,      0.0104,\n",
       "             -0.0088,     -0.0162,     -0.0122,      0.0108,      0.0166,\n",
       "             -0.0125,     -0.0020,      0.0003,      0.0150,     -0.0070,\n",
       "              0.0161,     -0.0123,      0.0031,     -0.0013,      0.0177,\n",
       "             -0.0070,      0.0104,      0.0104,     -0.0168,      0.0128,\n",
       "              0.0070,     -0.0024,      0.0147,      0.0168,     -0.0088,\n",
       "              0.0111,     -0.0130,     -0.0015,      0.0035,      0.0078,\n",
       "             -0.0147,      0.0094,     -0.0099,     -0.0094,     -0.0066,\n",
       "             -0.0049,     -0.0110,      0.0179,     -0.0114,      0.0125,\n",
       "             -0.0028,     -0.0176,     -0.0058,     -0.0139,     -0.0055,\n",
       "             -0.0120,     -0.0126,      0.0075,     -0.0066,      0.0032,\n",
       "              0.0056,     -0.0061,      0.0019,      0.0149,      0.0056,\n",
       "              0.0095,     -0.0081,      0.0114,      0.0158,     -0.0136,\n",
       "              0.0135,      0.0148,     -0.0055,      0.0110,     -0.0136,\n",
       "              0.0147,      0.0091,      0.0133,     -0.0082,     -0.0153,\n",
       "             -0.0023,      0.0073,     -0.0162,     -0.0067,     -0.0146,\n",
       "             -0.0031,      0.0117,     -0.0011,     -0.0179,     -0.0112,\n",
       "             -0.0094,     -0.0143,     -0.0176,      0.0047,      0.0115,\n",
       "             -0.0118,     -0.0026,     -0.0139,      0.0057,      0.0172,\n",
       "             -0.0116,      0.0176,      0.0026,      0.0103,     -0.0089,\n",
       "              0.0085,     -0.0016,      0.0048,      0.0176,     -0.0045,\n",
       "             -0.0050,     -0.0096,      0.0064,      0.0094,      0.0176,\n",
       "              0.0050,     -0.0105,     -0.0125,      0.0169,      0.0032,\n",
       "             -0.0001,     -0.0169,     -0.0011,      0.0163,      0.0143,\n",
       "             -0.0051,     -0.0145,      0.0131,      0.0095,      0.0104,\n",
       "             -0.0071,     -0.0106,     -0.0072,     -0.0128,      0.0038,\n",
       "             -0.0000,     -0.0089,     -0.0143,     -0.0067,      0.0040,\n",
       "             -0.0032,     -0.0137,      0.0101,     -0.0075,     -0.0074,\n",
       "             -0.0103,      0.0155,      0.0025,     -0.0021,     -0.0147,\n",
       "              0.0109,      0.0041,      0.0035,     -0.0121,     -0.0162,\n",
       "             -0.0007,     -0.0066,     -0.0093,     -0.0102,      0.0172,\n",
       "              0.0116,      0.0142,      0.0095,      0.0046,     -0.0022,\n",
       "             -0.0089,     -0.0053,      0.0161,     -0.0045,     -0.0031,\n",
       "              0.0051,     -0.0178,      0.0039,      0.0012,     -0.0092,\n",
       "              0.0116,      0.0012,      0.0120,      0.0099,     -0.0060,\n",
       "              0.0025,      0.0089,     -0.0074,     -0.0030,      0.0131,\n",
       "              0.0021,     -0.0073,     -0.0119,     -0.0176,     -0.0021,\n",
       "             -0.0122,      0.0118,     -0.0132,      0.0101,     -0.0070,\n",
       "              0.0077,      0.0103,      0.0124,     -0.0037,      0.0119,\n",
       "             -0.0127,     -0.0133,     -0.0019,     -0.0082,      0.0128,\n",
       "              0.0044,     -0.0160,     -0.0137,      0.0103,     -0.0041,\n",
       "             -0.0169,     -0.0118,      0.0025,      0.0129,      0.0109,\n",
       "              0.0064,     -0.0098,      0.0106,      0.0006,      0.0020,\n",
       "             -0.0060,      0.0158,     -0.0135,     -0.0176,     -0.0161,\n",
       "             -0.0175,     -0.0174,     -0.0093,     -0.0070,      0.0024,\n",
       "             -0.0117,     -0.0095,     -0.0147,     -0.0124,     -0.0072,\n",
       "              0.0167,     -0.0021,     -0.0166,      0.0156,      0.0145,\n",
       "             -0.0096,      0.0004,      0.0149,     -0.0147,      0.0098,\n",
       "             -0.0116,     -0.0104,     -0.0083,      0.0160,     -0.0036,\n",
       "              0.0068,     -0.0025,      0.0103,      0.0160,     -0.0039,\n",
       "              0.0067,      0.0175,      0.0117,      0.0036,     -0.0132,\n",
       "              0.0010,     -0.0031,     -0.0173,     -0.0066,      0.0109,\n",
       "              0.0075,     -0.0177,     -0.0140,     -0.0150,     -0.0106,\n",
       "             -0.0043,     -0.0072,     -0.0036,      0.0013,     -0.0062,\n",
       "              0.0179,     -0.0145,     -0.0045,     -0.0034,     -0.0150,\n",
       "             -0.0147,      0.0125,     -0.0081,      0.0140,      0.0099,\n",
       "              0.0173,     -0.0010,      0.0058,     -0.0164,      0.0167,\n",
       "              0.0009,     -0.0019,      0.0030,     -0.0032,     -0.0155,\n",
       "             -0.0060,      0.0007,      0.0159,     -0.0017,     -0.0058,\n",
       "             -0.0164,     -0.0092,      0.0099,     -0.0179,     -0.0156,\n",
       "             -0.0169,     -0.0105,      0.0149,      0.0055,     -0.0138,\n",
       "             -0.0123,      0.0025,     -0.0062,      0.0086,     -0.0053,\n",
       "              0.0124,      0.0164,     -0.0015,     -0.0153,      0.0055,\n",
       "             -0.0025,     -0.0080,     -0.0039,      0.0025,     -0.0024,\n",
       "             -0.0176,     -0.0105,      0.0068,     -0.0026,      0.0138,\n",
       "              0.0148,      0.0125,     -0.0141,      0.0037,      0.0012,\n",
       "             -0.0164,      0.0103,      0.0087,      0.0027,      0.0110,\n",
       "              0.0035,     -0.0050,      0.0080,      0.0000,     -0.0067,\n",
       "              0.0091,      0.0045,      0.0020,     -0.0115,      0.0143,\n",
       "              0.0096,      0.0099,     -0.0118,     -0.0016,      0.0082,\n",
       "              0.0064,     -0.0110,      0.0109,      0.0075,      0.0031,\n",
       "             -0.0097,     -0.0035,      0.0144,      0.0090,     -0.0107,\n",
       "              0.0073,     -0.0097,      0.0151,      0.0006,      0.0010,\n",
       "             -0.0175,     -0.0140,      0.0021,      0.0150,      0.0140,\n",
       "              0.0115,      0.0040,     -0.0057,      0.0062,      0.0043,\n",
       "             -0.0061,      0.0098,     -0.0054,     -0.0028,      0.0022,\n",
       "              0.0166,      0.0080,     -0.0100,      0.0157,     -0.0071,\n",
       "              0.0141,      0.0140,      0.0126,      0.0105,     -0.0141,\n",
       "             -0.0067,      0.0049,     -0.0071,     -0.0030,     -0.0159,\n",
       "             -0.0177,     -0.0063,      0.0150,      0.0026,     -0.0028,\n",
       "             -0.0096,      0.0117,     -0.0087,      0.0071,     -0.0012,\n",
       "              0.0179,     -0.0118,      0.0052,      0.0128,     -0.0034,\n",
       "              0.0052,      0.0148,     -0.0037,     -0.0096,      0.0053,\n",
       "              0.0067,      0.0138,     -0.0016,      0.0146,     -0.0058,\n",
       "              0.0060,     -0.0117,      0.0126,      0.0012,      0.0171,\n",
       "             -0.0145,      0.0153,      0.0078,      0.0063,      0.0065,\n",
       "             -0.0164,      0.0170,     -0.0006,     -0.0179,      0.0046,\n",
       "              0.0099,      0.0075,      0.0160,      0.0019,      0.0011,\n",
       "              0.0172,     -0.0160,     -0.0027,     -0.0023,     -0.0110,\n",
       "              0.0089,      0.0135,      0.0079,      0.0178,      0.0012,\n",
       "             -0.0049,      0.0116,      0.0066,     -0.0130,     -0.0122,\n",
       "              0.0123,     -0.0034,     -0.0154,     -0.0105,     -0.0096,\n",
       "             -0.0142,     -0.0052,     -0.0004,      0.0047,     -0.0107,\n",
       "             -0.0099,      0.0134,     -0.0108,     -0.0178,      0.0167,\n",
       "              0.0018,     -0.0172,     -0.0012,     -0.0082,      0.0024,\n",
       "              0.0132,      0.0059,     -0.0128,     -0.0124,     -0.0055,\n",
       "             -0.0043,     -0.0104,      0.0002,     -0.0108,      0.0083,\n",
       "              0.0029,      0.0036,      0.0003,      0.0140,     -0.0120,\n",
       "             -0.0064,     -0.0136,      0.0034,     -0.0148,     -0.0118,\n",
       "              0.0090,     -0.0091,      0.0037,      0.0098,      0.0076,\n",
       "             -0.0100,      0.0135,      0.0071,     -0.0126,     -0.0013,\n",
       "              0.0141,     -0.0023,      0.0171,      0.0077,     -0.0172,\n",
       "              0.0113,      0.0082,      0.0016,      0.0062,      0.0088,\n",
       "              0.0066,     -0.0119,      0.0040,      0.0119,     -0.0155,\n",
       "             -0.0015,      0.0123,      0.0084,     -0.0000,     -0.0014,\n",
       "              0.0134,     -0.0072,      0.0027,      0.0110,      0.0035,\n",
       "              0.0086,      0.0038,      0.0060,     -0.0091,      0.0054,\n",
       "              0.0179,     -0.0023,      0.0141,     -0.0143,      0.0002,\n",
       "              0.0053,     -0.0042,      0.0135,      0.0159,      0.0131,\n",
       "             -0.0048,     -0.0166,     -0.0004,      0.0125,      0.0161,\n",
       "             -0.0026,      0.0117,      0.0153,     -0.0100,     -0.0123,\n",
       "             -0.0135,     -0.0027,     -0.0148,     -0.0018,     -0.0080,\n",
       "             -0.0050,     -0.0069,     -0.0115,      0.0150,      0.0071,\n",
       "             -0.0146,     -0.0142,      0.0172,      0.0143,     -0.0094,\n",
       "              0.0068,      0.0070,      0.0019,     -0.0004,      0.0000,\n",
       "             -0.0016,      0.0035,      0.0119,      0.0031,     -0.0132,\n",
       "              0.0073,      0.0130,     -0.0097,      0.0039,      0.0058,\n",
       "             -0.0031,     -0.0094,     -0.0085,     -0.0126,      0.0125,\n",
       "             -0.0013,      0.0066,      0.0017,     -0.0117,     -0.0089,\n",
       "             -0.0039,      0.0032,     -0.0083,      0.0016,      0.0151,\n",
       "             -0.0174,      0.0082,      0.0136,      0.0067,     -0.0023,\n",
       "             -0.0028,      0.0076,      0.0099,      0.0080,      0.0036,\n",
       "             -0.0162,      0.0107,      0.0151,     -0.0004,      0.0068,\n",
       "             -0.0091,     -0.0024,     -0.0161,      0.0127,     -0.0028,\n",
       "              0.0047,     -0.0093,     -0.0057,     -0.0012,      0.0065,\n",
       "              0.0121,     -0.0108,     -0.0145,      0.0113,      0.0125,\n",
       "             -0.0036,      0.0033,      0.0082,     -0.0098,     -0.0153,\n",
       "              0.0119,     -0.0063,     -0.0029,      0.0103,      0.0046,\n",
       "              0.0058,     -0.0120,     -0.0172,     -0.0139,     -0.0001,\n",
       "              0.0108,      0.0067,     -0.0042,      0.0119,     -0.0015,\n",
       "             -0.0178,      0.0075,     -0.0096], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0346,  0.0253, -0.0056,  ...,  0.0084, -0.0346, -0.0319],\n",
       "         [ 0.0311,  0.0109,  0.0081,  ..., -0.0276, -0.0269,  0.0244],\n",
       "         [-0.0347,  0.0266,  0.0223,  ...,  0.0005,  0.0310, -0.0236],\n",
       "         ...,\n",
       "         [-0.0275,  0.0096,  0.0169,  ..., -0.0354,  0.0091, -0.0316],\n",
       "         [-0.0346,  0.0233, -0.0132,  ..., -0.0160, -0.0359,  0.0149],\n",
       "         [-0.0147,  0.0121, -0.0346,  ...,  0.0226, -0.0152, -0.0358]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0142,  0.0096, -0.0290,  ..., -0.0131, -0.0235,  0.0133],\n",
       "         [ 0.0135, -0.0167, -0.0347,  ...,  0.0005,  0.0301, -0.0335],\n",
       "         [ 0.0339,  0.0040, -0.0228,  ...,  0.0255, -0.0032, -0.0199],\n",
       "         ...,\n",
       "         [ 0.0298,  0.0222, -0.0342,  ..., -0.0193, -0.0161,  0.0183],\n",
       "         [-0.0079, -0.0198, -0.0243,  ...,  0.0118,  0.0032, -0.0105],\n",
       "         [ 0.0285, -0.0048,  0.0303,  ..., -0.0187, -0.0326,  0.0280]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0095,  0.0250,  0.0091,  ..., -0.0101, -0.0047, -0.0075],\n",
       "         [ 0.0204,  0.0088,  0.0360,  ...,  0.0338, -0.0221, -0.0105],\n",
       "         [ 0.0350,  0.0162,  0.0077,  ..., -0.0237,  0.0133, -0.0238],\n",
       "         ...,\n",
       "         [-0.0194, -0.0019, -0.0125,  ...,  0.0318,  0.0120,  0.0308],\n",
       "         [ 0.0037, -0.0058, -0.0021,  ..., -0.0273,  0.0116,  0.0055],\n",
       "         [-0.0142, -0.0267, -0.0324,  ...,  0.0268,  0.0009,  0.0216]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0061, -0.0199,  0.0160,  ..., -0.0286, -0.0234, -0.0203],\n",
       "         [ 0.0001,  0.0220, -0.0243,  ..., -0.0200,  0.0341,  0.0035],\n",
       "         [-0.0248, -0.0206,  0.0106,  ..., -0.0289,  0.0077,  0.0238],\n",
       "         ...,\n",
       "         [-0.0168,  0.0254,  0.0032,  ...,  0.0277, -0.0048,  0.0209],\n",
       "         [-0.0184, -0.0086,  0.0157,  ...,  0.0017, -0.0187,  0.0324],\n",
       "         [ 0.0219,  0.0012, -0.0297,  ...,  0.0300,  0.0070, -0.0014]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([    -0.0154,     -0.0246,      0.0118,      0.0100,     -0.0137,\n",
       "              0.0237,      0.0232,     -0.0179,      0.0151,     -0.0286,\n",
       "              0.0095,     -0.0260,     -0.0208,     -0.0172,      0.0030,\n",
       "              0.0282,      0.0284,     -0.0136,     -0.0250,     -0.0320,\n",
       "              0.0143,      0.0139,      0.0346,      0.0000,      0.0267,\n",
       "              0.0321,     -0.0105,      0.0055,     -0.0191,      0.0360,\n",
       "              0.0215,     -0.0038,     -0.0127,     -0.0214,     -0.0011,\n",
       "             -0.0222,      0.0006,     -0.0023,      0.0176,      0.0105,\n",
       "             -0.0199,      0.0197,      0.0336,      0.0276,     -0.0325,\n",
       "             -0.0035,      0.0198,     -0.0204,     -0.0295,      0.0052,\n",
       "             -0.0237,     -0.0198,      0.0021,      0.0156,      0.0037,\n",
       "             -0.0210,     -0.0344,     -0.0204,     -0.0099,     -0.0186,\n",
       "              0.0213,      0.0017,      0.0242,      0.0235,     -0.0080,\n",
       "             -0.0176,     -0.0160,      0.0128,      0.0213,      0.0248,\n",
       "              0.0045,      0.0077,      0.0228,     -0.0214,     -0.0340,\n",
       "              0.0024,      0.0347,      0.0301,      0.0293,      0.0016,\n",
       "              0.0117,      0.0166,     -0.0010,     -0.0070,      0.0360,\n",
       "              0.0319,      0.0162,     -0.0122,      0.0295,      0.0186,\n",
       "             -0.0249,     -0.0039,      0.0080,     -0.0063,     -0.0148,\n",
       "              0.0235,      0.0336,     -0.0021,     -0.0076,     -0.0141,\n",
       "             -0.0254,     -0.0111,     -0.0069,     -0.0169,      0.0123,\n",
       "             -0.0264,      0.0024,      0.0349,     -0.0104,     -0.0059,\n",
       "              0.0309,      0.0212,      0.0241,     -0.0279,      0.0022,\n",
       "             -0.0038,      0.0274,     -0.0307,     -0.0265,      0.0076,\n",
       "             -0.0348,      0.0302,      0.0223,     -0.0272,     -0.0016,\n",
       "             -0.0091,     -0.0265,     -0.0314,     -0.0002,     -0.0231,\n",
       "             -0.0032,     -0.0084,      0.0341,     -0.0003,      0.0086,\n",
       "              0.0123,      0.0042,      0.0271,      0.0060,      0.0161,\n",
       "              0.0017,     -0.0123,     -0.0086,      0.0231,      0.0207,\n",
       "             -0.0228,      0.0121,     -0.0205,     -0.0208,      0.0333,\n",
       "             -0.0053,     -0.0060,     -0.0046,     -0.0089,      0.0353,\n",
       "             -0.0195,     -0.0056,     -0.0278,      0.0316,      0.0317,\n",
       "             -0.0146,     -0.0322,     -0.0189,     -0.0328,     -0.0026,\n",
       "              0.0351,     -0.0289,     -0.0166,      0.0142,      0.0283,\n",
       "              0.0093,     -0.0309,      0.0357,     -0.0029,     -0.0297,\n",
       "              0.0346,     -0.0204,      0.0256,     -0.0282,     -0.0334,\n",
       "              0.0010,     -0.0021,      0.0165,      0.0199,     -0.0197,\n",
       "              0.0020,      0.0345,     -0.0183,     -0.0312,     -0.0178,\n",
       "             -0.0017,     -0.0127,     -0.0148,      0.0153,      0.0014,\n",
       "             -0.0135,      0.0037,      0.0165,      0.0206,      0.0256,\n",
       "              0.0006,     -0.0103,     -0.0097,     -0.0121,      0.0239,\n",
       "              0.0189,      0.0212,      0.0284,     -0.0262,     -0.0356,\n",
       "              0.0340,     -0.0245,     -0.0323,      0.0353,     -0.0116,\n",
       "              0.0027,     -0.0132,      0.0182,      0.0170,      0.0333,\n",
       "             -0.0150,     -0.0155,     -0.0308,     -0.0228,     -0.0339,\n",
       "             -0.0229,     -0.0106,      0.0141,      0.0348,     -0.0135,\n",
       "             -0.0113,     -0.0132,     -0.0247,      0.0216,     -0.0036,\n",
       "              0.0076,      0.0237,      0.0244,     -0.0318,     -0.0087,\n",
       "             -0.0163,     -0.0329,     -0.0157,     -0.0066,      0.0028,\n",
       "              0.0189,     -0.0164,     -0.0191,     -0.0196,     -0.0117,\n",
       "             -0.0117,      0.0309,     -0.0264,     -0.0301,     -0.0063,\n",
       "             -0.0317,     -0.0261,      0.0194,      0.0241,     -0.0230,\n",
       "             -0.0242,      0.0360,      0.0317,      0.0270,      0.0159,\n",
       "              0.0079,      0.0245,     -0.0025,     -0.0192,     -0.0127,\n",
       "              0.0116,      0.0147,      0.0194,      0.0360,     -0.0251,\n",
       "              0.0024,     -0.0327,     -0.0305,      0.0339,      0.0025,\n",
       "             -0.0049,      0.0080,     -0.0204,     -0.0175,      0.0008,\n",
       "              0.0341,      0.0325,     -0.0058,     -0.0017,     -0.0040,\n",
       "             -0.0270,      0.0014,      0.0249,     -0.0333,      0.0097,\n",
       "              0.0037,     -0.0326,     -0.0016,     -0.0013,      0.0240,\n",
       "             -0.0246,      0.0040,     -0.0094,      0.0276,      0.0122,\n",
       "              0.0048,      0.0257,      0.0097,      0.0300,      0.0240,\n",
       "              0.0082,     -0.0285,      0.0058,      0.0087,      0.0044,\n",
       "             -0.0234,     -0.0270,     -0.0197,      0.0193,      0.0247,\n",
       "             -0.0306,     -0.0221,      0.0096,     -0.0167,     -0.0331,\n",
       "              0.0281,     -0.0166,     -0.0163,     -0.0345,     -0.0311,\n",
       "             -0.0193,      0.0313,      0.0169,     -0.0060,     -0.0048,\n",
       "              0.0112,     -0.0151,     -0.0062,      0.0138,      0.0112,\n",
       "             -0.0305,     -0.0274,      0.0208,     -0.0111,     -0.0344,\n",
       "              0.0010,      0.0254,      0.0085,     -0.0054,      0.0179,\n",
       "              0.0026,     -0.0263,     -0.0019,      0.0319,      0.0087,\n",
       "             -0.0056,      0.0062,     -0.0218,      0.0087,      0.0162,\n",
       "              0.0016,     -0.0045,      0.0337,     -0.0323,     -0.0253,\n",
       "             -0.0152,      0.0234,     -0.0097,     -0.0271,     -0.0004,\n",
       "              0.0210,      0.0201,     -0.0303,      0.0220,     -0.0074,\n",
       "             -0.0291,     -0.0028,      0.0122,      0.0317,      0.0145,\n",
       "              0.0032,     -0.0013,     -0.0023,     -0.0140,     -0.0137,\n",
       "              0.0293,     -0.0315,      0.0001,      0.0255,     -0.0068,\n",
       "              0.0165,     -0.0008,     -0.0361,     -0.0327,     -0.0322,\n",
       "              0.0309,      0.0201,      0.0236,      0.0039,      0.0358,\n",
       "             -0.0061,     -0.0320,      0.0159,     -0.0056,      0.0032,\n",
       "              0.0277,     -0.0214,      0.0356,      0.0281,     -0.0248,\n",
       "              0.0165,      0.0219,     -0.0103,     -0.0180,      0.0285,\n",
       "             -0.0274,      0.0011,      0.0343,      0.0340,      0.0320,\n",
       "              0.0243,     -0.0022,     -0.0051,      0.0260,      0.0120,\n",
       "             -0.0052,     -0.0246,      0.0093,     -0.0221,      0.0230,\n",
       "              0.0192,     -0.0150,     -0.0351,      0.0354,      0.0159,\n",
       "              0.0074,      0.0255,      0.0294,     -0.0250,      0.0089,\n",
       "              0.0237,     -0.0160,     -0.0304,     -0.0302,      0.0310,\n",
       "             -0.0172,     -0.0356,     -0.0340,     -0.0072,      0.0058,\n",
       "              0.0116,     -0.0188,     -0.0020,     -0.0198,      0.0312,\n",
       "             -0.0183,      0.0025,     -0.0195,     -0.0027,     -0.0182,\n",
       "              0.0107,     -0.0264,      0.0041,      0.0002,     -0.0176,\n",
       "             -0.0259,      0.0169,     -0.0350,     -0.0076,      0.0325,\n",
       "              0.0185,      0.0147,      0.0099,     -0.0180,     -0.0272,\n",
       "              0.0222,     -0.0335,      0.0262,      0.0024,      0.0268,\n",
       "              0.0075,     -0.0195,     -0.0233,      0.0314,     -0.0232,\n",
       "             -0.0112,      0.0176,      0.0087,      0.0066,      0.0027,\n",
       "             -0.0211,      0.0152,     -0.0173,      0.0133,     -0.0265,\n",
       "              0.0336,     -0.0241,     -0.0274,      0.0304,      0.0237,\n",
       "              0.0006,      0.0194,      0.0042,     -0.0199,      0.0319,\n",
       "              0.0276,      0.0353,     -0.0042,      0.0051,      0.0184,\n",
       "             -0.0136,     -0.0046,      0.0334,      0.0051,      0.0352,\n",
       "              0.0177,      0.0047,      0.0098,     -0.0107,     -0.0357,\n",
       "             -0.0029,     -0.0254,      0.0148,     -0.0152,      0.0267,\n",
       "              0.0127,      0.0310,      0.0264,     -0.0320,     -0.0009,\n",
       "             -0.0261,     -0.0188,      0.0247,     -0.0010,     -0.0343,\n",
       "              0.0343,     -0.0038,      0.0323,     -0.0130,      0.0320,\n",
       "              0.0140,      0.0095,     -0.0165,      0.0316,     -0.0238,\n",
       "              0.0261,     -0.0113,      0.0110,     -0.0060,      0.0334,\n",
       "             -0.0272,      0.0237,      0.0342,     -0.0063,     -0.0010,\n",
       "              0.0200,      0.0008,      0.0027,      0.0212,     -0.0181,\n",
       "              0.0034,     -0.0078,      0.0279,      0.0236,      0.0207,\n",
       "             -0.0337,     -0.0322,     -0.0323,     -0.0298,      0.0164,\n",
       "              0.0200,     -0.0253,     -0.0134,      0.0255,     -0.0113,\n",
       "              0.0136,     -0.0287,     -0.0312,     -0.0208,     -0.0195,\n",
       "             -0.0061,      0.0014,     -0.0068,     -0.0088,      0.0196,\n",
       "              0.0024,     -0.0089,     -0.0207,      0.0171,      0.0151,\n",
       "             -0.0162,     -0.0033,     -0.0322,     -0.0351,      0.0181,\n",
       "              0.0137,     -0.0219,     -0.0155,      0.0029,      0.0026,\n",
       "              0.0023,      0.0256,     -0.0214,     -0.0114,     -0.0349,\n",
       "             -0.0072,     -0.0278,      0.0245,     -0.0273,      0.0279,\n",
       "             -0.0337,      0.0283,     -0.0049,      0.0198,     -0.0058,\n",
       "              0.0270,      0.0209,      0.0093,     -0.0139,     -0.0270,\n",
       "             -0.0151,      0.0316,     -0.0182,      0.0301,      0.0312,\n",
       "             -0.0285,     -0.0322,      0.0156,      0.0310,     -0.0152,\n",
       "              0.0337,      0.0355,      0.0155,      0.0017,     -0.0261,\n",
       "             -0.0198,      0.0305,     -0.0296,      0.0083,     -0.0060,\n",
       "              0.0010,      0.0141,     -0.0190,      0.0122,      0.0323,\n",
       "              0.0107,      0.0242,      0.0269,      0.0205,      0.0074,\n",
       "              0.0031,     -0.0270,      0.0226,     -0.0143,     -0.0350,\n",
       "             -0.0255,      0.0171,      0.0253,     -0.0275,     -0.0240,\n",
       "              0.0252,      0.0334,     -0.0218,     -0.0352,      0.0242,\n",
       "             -0.0038,     -0.0155,      0.0161,      0.0237,      0.0079,\n",
       "             -0.0039,     -0.0221,     -0.0221,      0.0071,      0.0047,\n",
       "             -0.0028,      0.0168,     -0.0311,      0.0012,     -0.0186,\n",
       "              0.0223,     -0.0171,     -0.0240,     -0.0112,     -0.0043,\n",
       "             -0.0064,      0.0186,      0.0211,      0.0330,     -0.0152,\n",
       "             -0.0047,     -0.0142,     -0.0145,     -0.0261,      0.0001,\n",
       "             -0.0153,      0.0061,      0.0217,      0.0009,      0.0217,\n",
       "              0.0066,     -0.0114,      0.0177,     -0.0144,      0.0057,\n",
       "             -0.0083,      0.0110,      0.0299,      0.0010,      0.0002,\n",
       "              0.0133,      0.0232,      0.0081,     -0.0004,     -0.0266,\n",
       "             -0.0237,      0.0009,     -0.0178,      0.0021,     -0.0093,\n",
       "             -0.0245,      0.0143,      0.0093,     -0.0113,     -0.0218,\n",
       "              0.0355,     -0.0284,      0.0159,     -0.0286,      0.0299,\n",
       "             -0.0008,     -0.0228,     -0.0027,     -0.0182,     -0.0151,\n",
       "             -0.0205,     -0.0298,      0.0148,      0.0291,      0.0062,\n",
       "             -0.0305,      0.0005,     -0.0323,     -0.0311,     -0.0077,\n",
       "             -0.0285,      0.0296,     -0.0140,      0.0096,     -0.0205,\n",
       "             -0.0153,      0.0057,      0.0145,      0.0290,     -0.0342,\n",
       "              0.0292,      0.0047,      0.0236,      0.0017,     -0.0343,\n",
       "              0.0064,      0.0203,     -0.0345,     -0.0171,      0.0021,\n",
       "             -0.0144,     -0.0337,      0.0335], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0323, -0.0131, -0.0104,  ..., -0.0312, -0.0127,  0.0049],\n",
       "         [ 0.0199,  0.0028,  0.0197,  ..., -0.0209, -0.0354, -0.0029],\n",
       "         [ 0.0329, -0.0195, -0.0289,  ...,  0.0041,  0.0308, -0.0295],\n",
       "         ...,\n",
       "         [-0.0309,  0.0163, -0.0211,  ...,  0.0015,  0.0231,  0.0231],\n",
       "         [-0.0061, -0.0064,  0.0225,  ..., -0.0047,  0.0113, -0.0167],\n",
       "         [ 0.0002, -0.0050,  0.0053,  ...,  0.0187, -0.0338,  0.0342]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.0003,  0.0252,  0.0278,  ..., -0.0047, -0.0356, -0.0118],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0012,  0.0128,  0.0017,  ..., -0.0158, -0.0009, -0.0055],\n",
       "         [-0.0058,  0.0116,  0.0038,  ..., -0.0045, -0.0073,  0.0022],\n",
       "         [ 0.0036,  0.0055,  0.0058,  ..., -0.0039,  0.0132,  0.0118],\n",
       "         ...,\n",
       "         [-0.0010, -0.0144,  0.0122,  ...,  0.0061,  0.0140,  0.0153],\n",
       "         [-0.0071,  0.0094, -0.0001,  ...,  0.0089,  0.0086, -0.0039],\n",
       "         [-0.0165,  0.0050, -0.0145,  ...,  0.0085,  0.0178,  0.0020]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([     0.0060,      0.0166,      0.0162,      0.0086,      0.0172,\n",
       "              0.0179,      0.0004,      0.0157,      0.0094,      0.0018,\n",
       "             -0.0056,      0.0127,      0.0123,     -0.0078,      0.0046,\n",
       "              0.0137,     -0.0078,      0.0144,      0.0127,      0.0026,\n",
       "             -0.0057,     -0.0029,     -0.0068,      0.0162,     -0.0137,\n",
       "             -0.0122,      0.0111,     -0.0028,      0.0032,     -0.0170,\n",
       "              0.0010,      0.0154,      0.0018,      0.0172,      0.0164,\n",
       "              0.0046,     -0.0150,      0.0076,     -0.0127,      0.0010,\n",
       "             -0.0039,     -0.0058,      0.0116,     -0.0091,     -0.0144,\n",
       "              0.0125,     -0.0060,      0.0060,      0.0091,      0.0107,\n",
       "              0.0157,     -0.0022,     -0.0017,      0.0045,     -0.0046,\n",
       "              0.0082,      0.0176,      0.0168,     -0.0011,      0.0174,\n",
       "             -0.0164,     -0.0014,      0.0132,      0.0042,     -0.0171,\n",
       "              0.0043,     -0.0153,      0.0040,     -0.0161,     -0.0146,\n",
       "             -0.0141,     -0.0166,     -0.0163,      0.0030,     -0.0065,\n",
       "              0.0040,      0.0020,     -0.0137,     -0.0171,      0.0006,\n",
       "             -0.0157,     -0.0061,      0.0093,      0.0080,      0.0103,\n",
       "             -0.0171,     -0.0144,     -0.0056,     -0.0132,     -0.0051,\n",
       "              0.0175,     -0.0166,      0.0093,     -0.0050,      0.0005,\n",
       "             -0.0161,     -0.0055,     -0.0020,      0.0105,      0.0075,\n",
       "             -0.0076,     -0.0083,     -0.0149,     -0.0178,      0.0025,\n",
       "              0.0151,      0.0108,      0.0103,      0.0157,     -0.0142,\n",
       "             -0.0021,     -0.0105,     -0.0021,      0.0018,      0.0005,\n",
       "             -0.0020,     -0.0014,      0.0037,      0.0142,      0.0025,\n",
       "             -0.0082,      0.0116,      0.0094,      0.0167,      0.0124,\n",
       "              0.0139,      0.0131,      0.0114,     -0.0084,      0.0153,\n",
       "             -0.0129,      0.0093,      0.0054,      0.0153,     -0.0126,\n",
       "              0.0149,     -0.0036,      0.0047,      0.0083,      0.0057,\n",
       "             -0.0096,     -0.0103,      0.0180,     -0.0061,      0.0087,\n",
       "             -0.0009,     -0.0043,      0.0179,      0.0138,      0.0078,\n",
       "              0.0078,      0.0030,      0.0140,     -0.0036,      0.0151,\n",
       "              0.0142,     -0.0032,      0.0096,     -0.0045,      0.0150,\n",
       "              0.0175,     -0.0025,      0.0144,      0.0109,      0.0061,\n",
       "             -0.0134,     -0.0042,     -0.0156,     -0.0167,     -0.0175,\n",
       "              0.0069,     -0.0132,      0.0107,     -0.0156,     -0.0112,\n",
       "              0.0028,     -0.0002,      0.0019,     -0.0015,     -0.0012,\n",
       "             -0.0137,      0.0019,      0.0169,      0.0141,      0.0086,\n",
       "              0.0125,      0.0129,     -0.0165,     -0.0154,      0.0008,\n",
       "             -0.0044,      0.0078,      0.0138,      0.0008,      0.0008,\n",
       "              0.0035,      0.0038,      0.0027,     -0.0131,     -0.0121,\n",
       "              0.0148,     -0.0064,      0.0143,      0.0145,      0.0056,\n",
       "              0.0138,     -0.0130,      0.0024,      0.0137,     -0.0003,\n",
       "              0.0168,      0.0069,      0.0157,     -0.0165,     -0.0015,\n",
       "             -0.0109,     -0.0030,      0.0147,      0.0038,      0.0084,\n",
       "             -0.0036,     -0.0166,      0.0113,     -0.0122,     -0.0065,\n",
       "             -0.0022,      0.0025,     -0.0061,     -0.0133,     -0.0152,\n",
       "              0.0160,     -0.0105,      0.0083,      0.0063,      0.0029,\n",
       "             -0.0107,      0.0133,     -0.0068,      0.0025,     -0.0136,\n",
       "             -0.0001,     -0.0170,     -0.0092,      0.0132,      0.0165,\n",
       "             -0.0074,      0.0043,     -0.0065,      0.0068,      0.0081,\n",
       "              0.0024,     -0.0138,     -0.0092,      0.0137,      0.0152,\n",
       "              0.0075,     -0.0005,      0.0032,     -0.0017,     -0.0072,\n",
       "             -0.0167,     -0.0093,     -0.0051,      0.0150,     -0.0128,\n",
       "              0.0009,      0.0080,      0.0159,      0.0000,     -0.0080,\n",
       "              0.0084,     -0.0083,      0.0001,     -0.0036,     -0.0068,\n",
       "              0.0061,      0.0062,     -0.0018,     -0.0134,      0.0091,\n",
       "              0.0079,     -0.0172,      0.0011,     -0.0021,     -0.0105,\n",
       "              0.0076,     -0.0006,     -0.0031,      0.0071,     -0.0109,\n",
       "             -0.0100,      0.0059,     -0.0008,     -0.0026,     -0.0035,\n",
       "             -0.0109,      0.0087,     -0.0119,      0.0090,      0.0028,\n",
       "             -0.0140,     -0.0165,     -0.0160,     -0.0031,     -0.0097,\n",
       "             -0.0031,     -0.0135,      0.0178,     -0.0114,      0.0015,\n",
       "             -0.0148,      0.0093,     -0.0163,      0.0137,      0.0091,\n",
       "             -0.0128,     -0.0067,     -0.0162,      0.0071,     -0.0092,\n",
       "              0.0080,      0.0122,     -0.0065,      0.0088,     -0.0062,\n",
       "              0.0013,     -0.0026,     -0.0141,     -0.0137,      0.0088,\n",
       "             -0.0020,      0.0033,      0.0179,      0.0114,     -0.0072,\n",
       "              0.0060,      0.0116,     -0.0048,     -0.0038,      0.0156,\n",
       "             -0.0107,      0.0147,     -0.0078,      0.0102,      0.0044,\n",
       "             -0.0088,      0.0151,     -0.0014,      0.0056,     -0.0023,\n",
       "             -0.0045,      0.0169,      0.0036,      0.0031,      0.0153,\n",
       "              0.0038,      0.0033,     -0.0101,      0.0027,     -0.0105,\n",
       "             -0.0116,     -0.0035,      0.0119,      0.0044,      0.0076,\n",
       "              0.0008,      0.0103,      0.0168,     -0.0056,      0.0170,\n",
       "              0.0030,      0.0128,     -0.0030,      0.0052,      0.0037,\n",
       "              0.0032,     -0.0153,     -0.0020,      0.0062,     -0.0125,\n",
       "             -0.0004,     -0.0082,      0.0010,     -0.0134,     -0.0068,\n",
       "              0.0014,      0.0149,      0.0108,      0.0023,      0.0039,\n",
       "              0.0101,      0.0110,      0.0002,      0.0159,     -0.0062,\n",
       "              0.0073,      0.0052,      0.0092,      0.0108,      0.0025,\n",
       "             -0.0090,     -0.0137,      0.0147,      0.0077,      0.0133,\n",
       "             -0.0115,     -0.0029,      0.0113,      0.0043,      0.0158,\n",
       "              0.0056,      0.0042,      0.0165,      0.0108,     -0.0171,\n",
       "             -0.0047,     -0.0091,      0.0148,     -0.0059,      0.0154,\n",
       "              0.0083,     -0.0161,      0.0060,      0.0088,     -0.0103,\n",
       "              0.0063,      0.0011,     -0.0032,      0.0042,      0.0132,\n",
       "             -0.0054,      0.0102,      0.0085,      0.0037,     -0.0156,\n",
       "             -0.0002,      0.0119,      0.0000,      0.0015,     -0.0008,\n",
       "             -0.0173,     -0.0116,     -0.0089,     -0.0047,     -0.0144,\n",
       "              0.0112,     -0.0105,      0.0160,      0.0076,      0.0153,\n",
       "             -0.0167,      0.0099,     -0.0162,      0.0119,     -0.0051,\n",
       "             -0.0018,      0.0165,      0.0095,      0.0078,      0.0106,\n",
       "              0.0100,     -0.0118,      0.0076,      0.0060,     -0.0084,\n",
       "              0.0035,      0.0068,     -0.0027,     -0.0054,     -0.0180,\n",
       "              0.0102,     -0.0162,     -0.0173,     -0.0084,     -0.0096,\n",
       "              0.0023,      0.0019,      0.0011,     -0.0105,     -0.0163,\n",
       "             -0.0163,     -0.0152,     -0.0020,      0.0163,      0.0084,\n",
       "             -0.0089,     -0.0119,     -0.0180,     -0.0045,      0.0180,\n",
       "              0.0069,     -0.0125,     -0.0091,      0.0108,      0.0144,\n",
       "              0.0007,     -0.0094,      0.0164,     -0.0162,      0.0088,\n",
       "              0.0022,     -0.0134,     -0.0119,     -0.0153,      0.0089,\n",
       "             -0.0097,      0.0176,      0.0070,      0.0094,     -0.0116,\n",
       "             -0.0062,      0.0052,     -0.0117,      0.0088,      0.0157,\n",
       "             -0.0130,     -0.0021,     -0.0157,      0.0169,     -0.0040,\n",
       "              0.0027,      0.0149,      0.0051,     -0.0091,      0.0047,\n",
       "             -0.0007,     -0.0160,     -0.0099,     -0.0021,     -0.0138,\n",
       "             -0.0026,     -0.0111,      0.0084,     -0.0004,     -0.0079,\n",
       "              0.0141,     -0.0139,     -0.0026,      0.0077,     -0.0017,\n",
       "             -0.0164,      0.0129,     -0.0175,     -0.0052,      0.0086,\n",
       "              0.0174,     -0.0050,      0.0067,     -0.0137,     -0.0020,\n",
       "             -0.0155,     -0.0025,     -0.0000,      0.0120,     -0.0078,\n",
       "             -0.0114,      0.0118,     -0.0012,      0.0015,      0.0021,\n",
       "             -0.0067,      0.0075,     -0.0035,     -0.0081,     -0.0056,\n",
       "             -0.0018,      0.0180,      0.0139,      0.0108,     -0.0053,\n",
       "              0.0066,      0.0006,     -0.0156,      0.0118,      0.0065,\n",
       "             -0.0161,      0.0112,     -0.0042,      0.0006,     -0.0155,\n",
       "             -0.0083,      0.0134,     -0.0061,      0.0063,     -0.0032,\n",
       "             -0.0139,     -0.0011,     -0.0089,      0.0104,     -0.0138,\n",
       "             -0.0119,     -0.0052,      0.0123,     -0.0105,      0.0014,\n",
       "             -0.0180,      0.0173,     -0.0104,      0.0168,      0.0040,\n",
       "             -0.0018,     -0.0060,     -0.0106,     -0.0054,      0.0109,\n",
       "             -0.0151,     -0.0010,      0.0136,     -0.0016,      0.0143,\n",
       "             -0.0079,      0.0113,     -0.0136,      0.0177,     -0.0045,\n",
       "             -0.0068,      0.0131,      0.0024,      0.0040,      0.0082,\n",
       "             -0.0072,     -0.0155,      0.0025,      0.0167,     -0.0089,\n",
       "              0.0022,     -0.0083,     -0.0017,     -0.0127,     -0.0009,\n",
       "             -0.0067,     -0.0160,      0.0025,      0.0042,     -0.0126,\n",
       "              0.0104,     -0.0025,     -0.0168,     -0.0075,     -0.0161,\n",
       "              0.0175,      0.0079,      0.0059,     -0.0051,     -0.0110,\n",
       "             -0.0154,     -0.0122,     -0.0133,      0.0049,      0.0102,\n",
       "              0.0070,      0.0028,     -0.0107,      0.0140,     -0.0044,\n",
       "             -0.0162,     -0.0056,     -0.0063,      0.0149,      0.0039,\n",
       "             -0.0149,      0.0014,     -0.0132,     -0.0110,      0.0134,\n",
       "              0.0057,     -0.0057,     -0.0175,     -0.0036,     -0.0179,\n",
       "             -0.0068,      0.0008,      0.0020,      0.0031,      0.0078,\n",
       "              0.0169,      0.0160,     -0.0079,     -0.0077,     -0.0176,\n",
       "              0.0035,      0.0167,      0.0149,      0.0102,      0.0104,\n",
       "             -0.0167,      0.0159,     -0.0017,      0.0105,      0.0073,\n",
       "              0.0119,     -0.0094,      0.0052,     -0.0101,      0.0106,\n",
       "              0.0125,     -0.0036,      0.0037,     -0.0177,     -0.0146,\n",
       "             -0.0016,      0.0153,      0.0069,      0.0057,      0.0023,\n",
       "              0.0049,      0.0141,      0.0176,      0.0161,      0.0027,\n",
       "             -0.0102,      0.0151,      0.0055,      0.0027,      0.0168,\n",
       "              0.0144,      0.0083,     -0.0015,      0.0176,      0.0134,\n",
       "              0.0105,      0.0144,     -0.0048,      0.0120,     -0.0170,\n",
       "              0.0055,      0.0016,     -0.0051,     -0.0069,      0.0016,\n",
       "             -0.0010,     -0.0145,     -0.0134,     -0.0042,      0.0073,\n",
       "              0.0091,     -0.0125,     -0.0023,     -0.0157,     -0.0062,\n",
       "              0.0069,     -0.0176,      0.0030,      0.0037,     -0.0074,\n",
       "              0.0128,      0.0069,      0.0135,     -0.0066,      0.0062,\n",
       "             -0.0093,     -0.0044,     -0.0121,     -0.0170,      0.0018,\n",
       "              0.0048,     -0.0165,      0.0170,      0.0120,     -0.0053,\n",
       "             -0.0044,      0.0002,      0.0089,      0.0054,      0.0113,\n",
       "             -0.0042,     -0.0092,      0.0105], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0005,  0.0216,  0.0075,  ...,  0.0268, -0.0348,  0.0038],\n",
       "         [ 0.0195,  0.0053,  0.0269,  ...,  0.0360, -0.0067,  0.0149],\n",
       "         [ 0.0078, -0.0303, -0.0238,  ...,  0.0358, -0.0340,  0.0246],\n",
       "         ...,\n",
       "         [-0.0157,  0.0265, -0.0050,  ...,  0.0005, -0.0051, -0.0268],\n",
       "         [ 0.0013, -0.0032,  0.0328,  ...,  0.0319, -0.0135,  0.0025],\n",
       "         [ 0.0315, -0.0147,  0.0215,  ..., -0.0336, -0.0222, -0.0205]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0156, -0.0104,  0.0039,  ...,  0.0034,  0.0014,  0.0310],\n",
       "         [-0.0258, -0.0180, -0.0124,  ...,  0.0060, -0.0349,  0.0343],\n",
       "         [ 0.0272,  0.0086,  0.0294,  ..., -0.0119,  0.0074, -0.0329],\n",
       "         ...,\n",
       "         [-0.0285,  0.0214, -0.0266,  ..., -0.0050, -0.0219, -0.0248],\n",
       "         [ 0.0194,  0.0277,  0.0077,  ...,  0.0122, -0.0205, -0.0139],\n",
       "         [ 0.0145, -0.0059,  0.0173,  ...,  0.0005,  0.0315, -0.0019]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0120,  0.0098, -0.0210,  ...,  0.0047, -0.0007, -0.0245],\n",
       "         [-0.0196,  0.0249,  0.0223,  ...,  0.0145, -0.0207,  0.0273],\n",
       "         [ 0.0197, -0.0047, -0.0307,  ..., -0.0191,  0.0345, -0.0310],\n",
       "         ...,\n",
       "         [-0.0151, -0.0249, -0.0328,  ..., -0.0287, -0.0315,  0.0243],\n",
       "         [-0.0243, -0.0063, -0.0314,  ..., -0.0330,  0.0112,  0.0318],\n",
       "         [-0.0342, -0.0139,  0.0005,  ...,  0.0040,  0.0008,  0.0138]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0141, -0.0259,  0.0272,  ...,  0.0322, -0.0316, -0.0020],\n",
       "         [-0.0144,  0.0064, -0.0274,  ..., -0.0283, -0.0316,  0.0205],\n",
       "         [ 0.0287, -0.0026, -0.0116,  ..., -0.0344, -0.0125, -0.0185],\n",
       "         ...,\n",
       "         [-0.0102, -0.0182, -0.0072,  ..., -0.0089, -0.0297, -0.0309],\n",
       "         [ 0.0288, -0.0035,  0.0189,  ..., -0.0336, -0.0043, -0.0281],\n",
       "         [-0.0217, -0.0215, -0.0315,  ..., -0.0009, -0.0214, -0.0343]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.0299, -0.0333,  0.0125, -0.0168,  0.0238,  0.0296,  0.0016,  0.0237,\n",
       "         -0.0281, -0.0072,  0.0352,  0.0041,  0.0341, -0.0005, -0.0019, -0.0112,\n",
       "          0.0354,  0.0323, -0.0103,  0.0242,  0.0239,  0.0356, -0.0053, -0.0327,\n",
       "          0.0069,  0.0229, -0.0311,  0.0193, -0.0189, -0.0244,  0.0306,  0.0250,\n",
       "         -0.0027, -0.0149, -0.0153,  0.0144,  0.0205, -0.0074, -0.0067, -0.0018,\n",
       "          0.0141, -0.0228, -0.0054,  0.0138, -0.0256, -0.0246, -0.0087,  0.0289,\n",
       "          0.0181,  0.0095, -0.0021, -0.0007, -0.0242,  0.0103, -0.0199,  0.0346,\n",
       "         -0.0085,  0.0163,  0.0281, -0.0337, -0.0226,  0.0225,  0.0028,  0.0130,\n",
       "         -0.0329, -0.0153, -0.0303,  0.0097, -0.0088, -0.0337, -0.0191,  0.0335,\n",
       "          0.0015,  0.0273, -0.0110,  0.0090, -0.0357, -0.0311,  0.0072,  0.0104,\n",
       "          0.0337, -0.0280,  0.0233,  0.0321, -0.0005, -0.0127,  0.0205, -0.0188,\n",
       "         -0.0031,  0.0356,  0.0030, -0.0305, -0.0224,  0.0262,  0.0071,  0.0028,\n",
       "         -0.0163, -0.0197, -0.0053, -0.0047,  0.0138, -0.0269, -0.0085,  0.0114,\n",
       "          0.0221, -0.0136, -0.0191,  0.0098, -0.0274, -0.0119, -0.0137,  0.0170,\n",
       "         -0.0327, -0.0106, -0.0185,  0.0269, -0.0160, -0.0330,  0.0219, -0.0351,\n",
       "         -0.0007,  0.0005, -0.0002, -0.0158,  0.0333,  0.0331,  0.0274, -0.0036,\n",
       "         -0.0158, -0.0031,  0.0012,  0.0240,  0.0313, -0.0114,  0.0078,  0.0168,\n",
       "         -0.0138, -0.0312,  0.0034,  0.0143, -0.0259,  0.0016,  0.0324,  0.0121,\n",
       "          0.0287,  0.0304, -0.0250, -0.0308,  0.0077,  0.0351,  0.0141,  0.0269,\n",
       "         -0.0230,  0.0101, -0.0209, -0.0280,  0.0308, -0.0272, -0.0167,  0.0287,\n",
       "          0.0007, -0.0123, -0.0180, -0.0216, -0.0354, -0.0164,  0.0094, -0.0027,\n",
       "          0.0065,  0.0200,  0.0303, -0.0281,  0.0064, -0.0336, -0.0074, -0.0118,\n",
       "          0.0261,  0.0308,  0.0018,  0.0303, -0.0039,  0.0024,  0.0047, -0.0011,\n",
       "          0.0024,  0.0356, -0.0344, -0.0308,  0.0285,  0.0250,  0.0254, -0.0242,\n",
       "          0.0084,  0.0072, -0.0147,  0.0172,  0.0190,  0.0360,  0.0043,  0.0066,\n",
       "          0.0123,  0.0269,  0.0141, -0.0125, -0.0025, -0.0186, -0.0283, -0.0302,\n",
       "          0.0142,  0.0279, -0.0091, -0.0013, -0.0005,  0.0173, -0.0060, -0.0294,\n",
       "         -0.0185,  0.0206, -0.0142, -0.0028, -0.0101, -0.0093,  0.0167, -0.0349,\n",
       "         -0.0104,  0.0308, -0.0101,  0.0033, -0.0006,  0.0357, -0.0274, -0.0352,\n",
       "          0.0157,  0.0259, -0.0354, -0.0359,  0.0186,  0.0270, -0.0151, -0.0253,\n",
       "          0.0141, -0.0002, -0.0056,  0.0346, -0.0319, -0.0101, -0.0243,  0.0255,\n",
       "          0.0196, -0.0206,  0.0006, -0.0025,  0.0015,  0.0171,  0.0169,  0.0085,\n",
       "          0.0128, -0.0184, -0.0262, -0.0215, -0.0216, -0.0129,  0.0330, -0.0003,\n",
       "          0.0130,  0.0246,  0.0136,  0.0156, -0.0230, -0.0264,  0.0028, -0.0302,\n",
       "          0.0199,  0.0101,  0.0041,  0.0238,  0.0015,  0.0029, -0.0198,  0.0355,\n",
       "         -0.0142, -0.0062,  0.0287,  0.0050, -0.0137,  0.0050,  0.0321,  0.0030,\n",
       "          0.0272,  0.0195, -0.0305, -0.0338,  0.0045,  0.0337,  0.0236,  0.0210,\n",
       "         -0.0265,  0.0102, -0.0212, -0.0249,  0.0306, -0.0025, -0.0293, -0.0146,\n",
       "         -0.0300, -0.0342,  0.0292, -0.0335, -0.0329, -0.0215, -0.0125, -0.0290,\n",
       "         -0.0323, -0.0064, -0.0045,  0.0092,  0.0207,  0.0016,  0.0236,  0.0139,\n",
       "          0.0269, -0.0282, -0.0011, -0.0067, -0.0019, -0.0061,  0.0263, -0.0300,\n",
       "         -0.0072,  0.0023, -0.0085,  0.0264, -0.0333, -0.0280,  0.0198,  0.0285,\n",
       "         -0.0352,  0.0008,  0.0349, -0.0187,  0.0058, -0.0065, -0.0249, -0.0302,\n",
       "         -0.0039, -0.0354,  0.0011,  0.0222,  0.0261, -0.0315, -0.0094,  0.0146,\n",
       "         -0.0096,  0.0208, -0.0201,  0.0038, -0.0144, -0.0207,  0.0109, -0.0010,\n",
       "          0.0272, -0.0221, -0.0026, -0.0326, -0.0196,  0.0135, -0.0275,  0.0132,\n",
       "         -0.0224,  0.0358, -0.0081, -0.0215, -0.0228,  0.0216,  0.0225,  0.0324,\n",
       "         -0.0100,  0.0170,  0.0260,  0.0213,  0.0213, -0.0152,  0.0123,  0.0093,\n",
       "         -0.0135, -0.0256,  0.0134,  0.0356,  0.0075, -0.0122,  0.0020,  0.0080,\n",
       "         -0.0166, -0.0230, -0.0314,  0.0203,  0.0193, -0.0283, -0.0261, -0.0250,\n",
       "          0.0277,  0.0136,  0.0283, -0.0088, -0.0318,  0.0284,  0.0342, -0.0213,\n",
       "         -0.0015, -0.0209,  0.0236,  0.0181, -0.0231,  0.0078,  0.0182, -0.0356,\n",
       "         -0.0045,  0.0219, -0.0002, -0.0122,  0.0159,  0.0316,  0.0233, -0.0168,\n",
       "         -0.0263, -0.0082, -0.0078,  0.0021,  0.0094, -0.0050,  0.0205, -0.0296,\n",
       "         -0.0031, -0.0053, -0.0190, -0.0112,  0.0202, -0.0065, -0.0301,  0.0007,\n",
       "          0.0325,  0.0111, -0.0187,  0.0189,  0.0010,  0.0098,  0.0014,  0.0070,\n",
       "          0.0093, -0.0018, -0.0259,  0.0256, -0.0185, -0.0099,  0.0226,  0.0111,\n",
       "         -0.0337,  0.0067, -0.0018, -0.0188, -0.0150,  0.0036,  0.0152,  0.0121,\n",
       "          0.0118,  0.0358,  0.0051, -0.0062, -0.0149, -0.0021, -0.0027,  0.0096,\n",
       "          0.0011, -0.0083, -0.0123,  0.0058,  0.0318,  0.0221,  0.0190, -0.0277,\n",
       "          0.0298, -0.0092,  0.0131,  0.0036, -0.0019,  0.0027,  0.0335,  0.0098,\n",
       "          0.0068,  0.0048, -0.0354,  0.0174,  0.0032, -0.0304,  0.0064, -0.0245,\n",
       "         -0.0312, -0.0124, -0.0134, -0.0143,  0.0333, -0.0243,  0.0112, -0.0288,\n",
       "         -0.0136, -0.0207, -0.0208,  0.0294, -0.0160,  0.0216,  0.0029, -0.0148,\n",
       "         -0.0278, -0.0281, -0.0228, -0.0089, -0.0153,  0.0050, -0.0296, -0.0182,\n",
       "          0.0070,  0.0336, -0.0177, -0.0338,  0.0171,  0.0305,  0.0017, -0.0253,\n",
       "         -0.0329,  0.0265, -0.0292,  0.0067, -0.0267,  0.0327, -0.0290, -0.0345,\n",
       "         -0.0167,  0.0070,  0.0288, -0.0308, -0.0067, -0.0016,  0.0051, -0.0192,\n",
       "          0.0152, -0.0278, -0.0093,  0.0054, -0.0160, -0.0268,  0.0230,  0.0245,\n",
       "         -0.0020,  0.0026,  0.0051,  0.0260, -0.0196, -0.0099, -0.0057,  0.0059,\n",
       "          0.0311, -0.0037,  0.0022, -0.0202,  0.0085, -0.0240,  0.0110,  0.0289,\n",
       "          0.0232, -0.0348, -0.0314,  0.0099,  0.0302,  0.0342,  0.0320,  0.0333,\n",
       "         -0.0042,  0.0008,  0.0048, -0.0079,  0.0180,  0.0258, -0.0056, -0.0157,\n",
       "         -0.0186, -0.0146,  0.0309,  0.0006,  0.0009, -0.0327, -0.0299,  0.0198,\n",
       "         -0.0159,  0.0250, -0.0199,  0.0049, -0.0268, -0.0331,  0.0152,  0.0244,\n",
       "         -0.0349, -0.0023, -0.0218, -0.0331, -0.0153,  0.0223, -0.0303, -0.0315,\n",
       "          0.0152,  0.0326, -0.0347, -0.0097, -0.0116,  0.0192,  0.0098, -0.0246,\n",
       "          0.0219,  0.0030, -0.0312,  0.0128, -0.0100, -0.0053,  0.0322,  0.0114,\n",
       "          0.0242,  0.0095,  0.0106, -0.0162,  0.0123,  0.0352,  0.0272,  0.0119,\n",
       "         -0.0108,  0.0124,  0.0066,  0.0294,  0.0255,  0.0271, -0.0139,  0.0029,\n",
       "         -0.0116,  0.0294,  0.0170, -0.0017, -0.0203,  0.0276, -0.0033, -0.0071,\n",
       "          0.0123, -0.0020,  0.0021,  0.0197,  0.0260,  0.0052,  0.0145, -0.0298,\n",
       "         -0.0282,  0.0017,  0.0142, -0.0125,  0.0285,  0.0257, -0.0351,  0.0357,\n",
       "         -0.0320, -0.0120,  0.0354, -0.0019, -0.0252, -0.0218, -0.0236, -0.0307,\n",
       "         -0.0106,  0.0020,  0.0337,  0.0277,  0.0076, -0.0213,  0.0052, -0.0050,\n",
       "         -0.0195, -0.0263,  0.0283, -0.0076,  0.0028, -0.0082, -0.0073,  0.0201,\n",
       "         -0.0236,  0.0065, -0.0062, -0.0093, -0.0051,  0.0359,  0.0048, -0.0108,\n",
       "         -0.0167,  0.0272, -0.0131,  0.0085, -0.0281,  0.0295,  0.0227, -0.0256,\n",
       "          0.0290, -0.0185,  0.0124, -0.0169, -0.0043,  0.0097,  0.0178,  0.0041,\n",
       "         -0.0053,  0.0088,  0.0163, -0.0298, -0.0148,  0.0308,  0.0195, -0.0049,\n",
       "          0.0354, -0.0055, -0.0260, -0.0193,  0.0305,  0.0186, -0.0103, -0.0153,\n",
       "         -0.0176,  0.0151, -0.0079, -0.0204,  0.0030,  0.0046,  0.0108,  0.0163,\n",
       "          0.0179, -0.0107,  0.0265,  0.0349,  0.0203,  0.0123,  0.0200, -0.0203,\n",
       "         -0.0113, -0.0041, -0.0142,  0.0119, -0.0128,  0.0170, -0.0224, -0.0091,\n",
       "         -0.0226,  0.0024, -0.0052, -0.0156, -0.0090, -0.0223, -0.0284, -0.0033,\n",
       "         -0.0156, -0.0168, -0.0327, -0.0291, -0.0173, -0.0330, -0.0030,  0.0295],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0220,  0.0102,  0.0179,  ..., -0.0168,  0.0026,  0.0099],\n",
       "         [-0.0165, -0.0360,  0.0351,  ...,  0.0087,  0.0301,  0.0201],\n",
       "         [-0.0066,  0.0116, -0.0039,  ...,  0.0271, -0.0341,  0.0105],\n",
       "         ...,\n",
       "         [-0.0158,  0.0346,  0.0019,  ...,  0.0250, -0.0108, -0.0083],\n",
       "         [-0.0263, -0.0312, -0.0069,  ..., -0.0166, -0.0022,  0.0287],\n",
       "         [ 0.0213,  0.0189,  0.0120,  ...,  0.0162, -0.0028, -0.0078]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0088, -0.0317, -0.0203,  ..., -0.0059, -0.0059,  0.0201],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0138,  0.0116,  0.0061,  ...,  0.0149, -0.0087, -0.0102],\n",
       "         [-0.0155, -0.0124, -0.0130,  ..., -0.0061, -0.0120, -0.0057],\n",
       "         [ 0.0129, -0.0073,  0.0002,  ..., -0.0154,  0.0058, -0.0173],\n",
       "         ...,\n",
       "         [-0.0157, -0.0103, -0.0060,  ..., -0.0146,  0.0032, -0.0129],\n",
       "         [ 0.0070,  0.0108,  0.0095,  ...,  0.0103,  0.0133, -0.0069],\n",
       "         [ 0.0170,  0.0154, -0.0025,  ...,  0.0150,  0.0123, -0.0026]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([    -0.0006,     -0.0081,     -0.0150,      0.0120,     -0.0016,\n",
       "              0.0055,     -0.0029,      0.0090,     -0.0111,     -0.0065,\n",
       "             -0.0122,     -0.0056,      0.0006,     -0.0171,      0.0177,\n",
       "              0.0129,      0.0021,     -0.0025,      0.0016,     -0.0058,\n",
       "              0.0179,      0.0105,      0.0083,      0.0166,     -0.0138,\n",
       "             -0.0009,     -0.0075,      0.0091,      0.0144,      0.0121,\n",
       "              0.0023,      0.0142,     -0.0007,     -0.0163,     -0.0044,\n",
       "             -0.0059,      0.0030,      0.0109,      0.0120,      0.0133,\n",
       "             -0.0048,      0.0077,      0.0161,      0.0102,     -0.0140,\n",
       "              0.0086,      0.0148,     -0.0175,      0.0138,     -0.0178,\n",
       "              0.0167,     -0.0157,      0.0071,     -0.0167,     -0.0101,\n",
       "             -0.0163,     -0.0081,      0.0067,     -0.0077,      0.0007,\n",
       "              0.0120,      0.0144,      0.0005,      0.0112,     -0.0034,\n",
       "              0.0091,     -0.0062,      0.0031,     -0.0062,     -0.0017,\n",
       "             -0.0061,     -0.0146,     -0.0025,      0.0046,      0.0123,\n",
       "              0.0109,      0.0139,      0.0061,     -0.0001,      0.0143,\n",
       "              0.0026,     -0.0137,     -0.0083,     -0.0096,     -0.0160,\n",
       "             -0.0035,     -0.0098,     -0.0115,      0.0134,     -0.0045,\n",
       "             -0.0050,      0.0015,      0.0165,      0.0151,      0.0091,\n",
       "              0.0156,     -0.0089,      0.0068,      0.0037,      0.0085,\n",
       "              0.0058,      0.0122,     -0.0104,      0.0076,      0.0149,\n",
       "             -0.0161,      0.0171,      0.0001,      0.0165,      0.0046,\n",
       "              0.0025,      0.0158,     -0.0013,      0.0156,     -0.0065,\n",
       "              0.0080,     -0.0163,     -0.0178,     -0.0097,     -0.0125,\n",
       "             -0.0085,     -0.0099,     -0.0171,     -0.0078,     -0.0051,\n",
       "             -0.0021,      0.0165,      0.0172,     -0.0037,     -0.0076,\n",
       "              0.0153,      0.0136,     -0.0160,     -0.0174,      0.0061,\n",
       "             -0.0003,      0.0153,      0.0173,      0.0034,      0.0022,\n",
       "              0.0055,      0.0180,      0.0034,     -0.0000,     -0.0013,\n",
       "              0.0065,      0.0064,      0.0030,     -0.0029,      0.0040,\n",
       "             -0.0128,      0.0170,      0.0053,      0.0136,      0.0169,\n",
       "             -0.0111,      0.0028,      0.0065,      0.0166,      0.0036,\n",
       "             -0.0029,      0.0000,      0.0034,      0.0134,     -0.0132,\n",
       "              0.0143,      0.0126,     -0.0043,     -0.0000,      0.0096,\n",
       "             -0.0144,     -0.0063,     -0.0108,     -0.0005,      0.0038,\n",
       "             -0.0053,      0.0081,      0.0036,     -0.0049,      0.0073,\n",
       "             -0.0077,     -0.0127,      0.0097,     -0.0067,     -0.0140,\n",
       "              0.0022,     -0.0094,      0.0065,      0.0011,      0.0130,\n",
       "             -0.0164,      0.0035,      0.0006,      0.0022,     -0.0033,\n",
       "             -0.0021,     -0.0130,      0.0090,      0.0069,      0.0026,\n",
       "              0.0142,     -0.0026,      0.0052,     -0.0162,      0.0077,\n",
       "             -0.0121,      0.0167,     -0.0141,      0.0142,      0.0065,\n",
       "             -0.0108,      0.0104,     -0.0148,      0.0036,     -0.0093,\n",
       "              0.0127,     -0.0072,     -0.0072,      0.0087,      0.0048,\n",
       "             -0.0001,     -0.0099,      0.0148,      0.0014,     -0.0082,\n",
       "              0.0033,      0.0032,      0.0094,     -0.0090,      0.0024,\n",
       "             -0.0025,      0.0139,      0.0179,     -0.0119,      0.0014,\n",
       "              0.0162,      0.0035,     -0.0080,      0.0144,      0.0180,\n",
       "             -0.0175,      0.0078,     -0.0003,     -0.0168,      0.0068,\n",
       "              0.0033,     -0.0154,     -0.0165,     -0.0023,      0.0157,\n",
       "              0.0003,     -0.0055,     -0.0051,     -0.0164,      0.0057,\n",
       "             -0.0153,     -0.0129,     -0.0078,     -0.0070,      0.0103,\n",
       "             -0.0000,      0.0124,     -0.0126,      0.0080,      0.0022,\n",
       "              0.0080,      0.0080,      0.0111,     -0.0162,     -0.0028,\n",
       "             -0.0154,     -0.0100,      0.0078,     -0.0175,     -0.0075,\n",
       "             -0.0121,     -0.0010,     -0.0035,     -0.0096,     -0.0025,\n",
       "              0.0013,      0.0134,     -0.0140,     -0.0172,      0.0058,\n",
       "              0.0099,      0.0064,      0.0040,     -0.0079,     -0.0063,\n",
       "              0.0025,     -0.0169,     -0.0144,      0.0144,      0.0155,\n",
       "             -0.0103,      0.0105,      0.0139,      0.0161,      0.0177,\n",
       "             -0.0068,     -0.0177,     -0.0149,      0.0011,      0.0175,\n",
       "             -0.0170,     -0.0136,     -0.0148,     -0.0022,     -0.0077,\n",
       "              0.0058,      0.0001,     -0.0143,     -0.0082,      0.0005,\n",
       "              0.0033,     -0.0031,      0.0069,      0.0028,     -0.0046,\n",
       "             -0.0040,      0.0165,     -0.0043,     -0.0156,     -0.0109,\n",
       "             -0.0116,      0.0051,      0.0013,     -0.0058,      0.0091,\n",
       "             -0.0070,     -0.0084,      0.0072,      0.0049,     -0.0023,\n",
       "             -0.0064,      0.0009,     -0.0051,     -0.0128,     -0.0014,\n",
       "              0.0139,     -0.0113,      0.0176,      0.0172,     -0.0170,\n",
       "              0.0175,     -0.0085,     -0.0151,     -0.0079,     -0.0117,\n",
       "              0.0064,      0.0084,      0.0164,     -0.0043,     -0.0012,\n",
       "             -0.0004,     -0.0177,     -0.0068,      0.0097,     -0.0124,\n",
       "              0.0105,     -0.0018,     -0.0037,      0.0140,      0.0052,\n",
       "              0.0092,      0.0021,      0.0124,      0.0038,      0.0170,\n",
       "             -0.0006,      0.0030,      0.0169,     -0.0072,      0.0159,\n",
       "              0.0161,      0.0102,      0.0155,      0.0010,     -0.0104,\n",
       "             -0.0098,      0.0086,      0.0016,      0.0114,     -0.0173,\n",
       "              0.0035,     -0.0122,     -0.0061,     -0.0176,     -0.0152,\n",
       "             -0.0123,      0.0145,      0.0072,      0.0154,     -0.0150,\n",
       "             -0.0168,      0.0129,     -0.0145,      0.0028,      0.0017,\n",
       "              0.0163,      0.0062,     -0.0167,     -0.0014,     -0.0023,\n",
       "             -0.0162,     -0.0073,     -0.0094,      0.0144,      0.0085,\n",
       "             -0.0158,     -0.0099,      0.0137,     -0.0040,      0.0032,\n",
       "             -0.0132,      0.0122,     -0.0104,      0.0124,      0.0065,\n",
       "              0.0143,      0.0101,     -0.0117,     -0.0127,     -0.0072,\n",
       "             -0.0127,     -0.0047,      0.0028,      0.0066,     -0.0178,\n",
       "             -0.0103,      0.0044,      0.0141,     -0.0152,      0.0008,\n",
       "             -0.0140,     -0.0022,      0.0035,     -0.0120,     -0.0132,\n",
       "              0.0019,     -0.0049,     -0.0135,     -0.0042,      0.0035,\n",
       "              0.0113,     -0.0167,     -0.0140,      0.0036,     -0.0092,\n",
       "             -0.0134,     -0.0167,      0.0051,     -0.0017,     -0.0085,\n",
       "             -0.0035,     -0.0118,     -0.0159,     -0.0020,      0.0033,\n",
       "             -0.0006,     -0.0007,      0.0059,      0.0041,      0.0097,\n",
       "             -0.0084,     -0.0036,     -0.0084,      0.0114,     -0.0049,\n",
       "             -0.0134,      0.0054,     -0.0067,     -0.0133,     -0.0154,\n",
       "              0.0111,      0.0087,     -0.0108,     -0.0160,     -0.0124,\n",
       "             -0.0086,      0.0166,     -0.0115,     -0.0023,      0.0125,\n",
       "              0.0133,      0.0044,      0.0086,      0.0026,      0.0017,\n",
       "              0.0106,      0.0016,      0.0002,      0.0102,      0.0001,\n",
       "              0.0058,      0.0091,     -0.0082,      0.0029,      0.0109,\n",
       "             -0.0011,     -0.0068,     -0.0163,     -0.0119,     -0.0052,\n",
       "             -0.0055,      0.0156,     -0.0122,      0.0081,     -0.0122,\n",
       "             -0.0022,      0.0090,      0.0001,      0.0006,     -0.0139,\n",
       "              0.0041,     -0.0112,     -0.0152,     -0.0147,      0.0030,\n",
       "             -0.0167,      0.0142,      0.0055,     -0.0137,     -0.0086,\n",
       "             -0.0166,     -0.0100,     -0.0042,     -0.0106,     -0.0150,\n",
       "             -0.0125,     -0.0091,     -0.0000,      0.0118,      0.0096,\n",
       "             -0.0037,      0.0084,     -0.0156,     -0.0070,     -0.0032,\n",
       "              0.0027,     -0.0023,     -0.0097,      0.0011,     -0.0092,\n",
       "             -0.0002,      0.0121,      0.0055,     -0.0062,      0.0080,\n",
       "             -0.0073,      0.0143,      0.0085,      0.0085,      0.0010,\n",
       "             -0.0074,     -0.0139,     -0.0101,      0.0052,     -0.0006,\n",
       "             -0.0129,      0.0071,      0.0116,      0.0178,     -0.0131,\n",
       "              0.0069,     -0.0112,     -0.0161,      0.0179,      0.0017,\n",
       "             -0.0100,     -0.0153,      0.0159,     -0.0173,     -0.0102,\n",
       "             -0.0061,      0.0071,      0.0142,      0.0026,      0.0022,\n",
       "              0.0026,      0.0145,      0.0024,     -0.0101,     -0.0051,\n",
       "             -0.0035,     -0.0094,      0.0103,     -0.0032,     -0.0071,\n",
       "             -0.0150,      0.0064,      0.0147,      0.0006,      0.0173,\n",
       "             -0.0043,      0.0162,     -0.0144,      0.0062,      0.0098,\n",
       "             -0.0112,     -0.0016,      0.0066,     -0.0126,      0.0093,\n",
       "              0.0023,     -0.0128,      0.0044,      0.0054,      0.0176,\n",
       "             -0.0079,     -0.0065,      0.0110,     -0.0125,     -0.0141,\n",
       "              0.0101,      0.0137,      0.0005,      0.0159,      0.0109,\n",
       "             -0.0096,      0.0078,      0.0038,     -0.0140,      0.0126,\n",
       "             -0.0143,      0.0122,     -0.0035,      0.0135,     -0.0066,\n",
       "             -0.0007,     -0.0014,      0.0172,     -0.0074,      0.0007,\n",
       "              0.0040,      0.0143,      0.0098,     -0.0008,     -0.0023,\n",
       "              0.0057,     -0.0148,     -0.0005,     -0.0061,     -0.0006,\n",
       "             -0.0056,      0.0161,     -0.0033,     -0.0033,     -0.0151,\n",
       "              0.0127,     -0.0080,     -0.0011,      0.0128,      0.0176,\n",
       "             -0.0035,      0.0170,      0.0060,      0.0036,     -0.0039,\n",
       "             -0.0061,     -0.0099,      0.0080,     -0.0170,      0.0001,\n",
       "              0.0066,     -0.0172,     -0.0076,     -0.0176,     -0.0047,\n",
       "              0.0178,     -0.0003,      0.0069,     -0.0068,      0.0011,\n",
       "              0.0059,      0.0076,      0.0115,      0.0055,      0.0161,\n",
       "              0.0118,      0.0146,      0.0128,     -0.0088,     -0.0107,\n",
       "              0.0165,      0.0083,     -0.0030,      0.0035,     -0.0016,\n",
       "              0.0009,      0.0018,     -0.0046,     -0.0093,     -0.0023,\n",
       "             -0.0119,      0.0043,     -0.0022,      0.0159,      0.0180,\n",
       "              0.0044,     -0.0000,     -0.0135,      0.0069,     -0.0113,\n",
       "             -0.0129,      0.0015,     -0.0139,      0.0007,      0.0177,\n",
       "              0.0037,      0.0046,     -0.0139,     -0.0178,      0.0131,\n",
       "              0.0101,     -0.0148,      0.0162,     -0.0068,      0.0014,\n",
       "             -0.0061,     -0.0063,     -0.0104,      0.0148,      0.0159,\n",
       "              0.0169,     -0.0058,     -0.0058,      0.0114,      0.0169,\n",
       "             -0.0139,      0.0121,     -0.0059,     -0.0069,      0.0009,\n",
       "             -0.0070,     -0.0134,      0.0056,      0.0140,     -0.0098,\n",
       "              0.0158,      0.0090,      0.0027,     -0.0068,     -0.0018,\n",
       "             -0.0021,     -0.0059,     -0.0160,     -0.0040,      0.0167,\n",
       "              0.0068,     -0.0088,      0.0053,      0.0077,      0.0048,\n",
       "              0.0012,      0.0113,     -0.0041,     -0.0028,      0.0010,\n",
       "              0.0059,      0.0149,     -0.0028,      0.0061,     -0.0165,\n",
       "              0.0122,      0.0122,     -0.0135], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[    -0.0288,     -0.0241,      0.0258,  ...,      0.0114,\n",
       "              -0.0263,     -0.0218],\n",
       "         [    -0.0182,      0.0022,      0.0244,  ...,     -0.0000,\n",
       "              -0.0225,     -0.0132],\n",
       "         [     0.0035,     -0.0003,     -0.0249,  ...,      0.0287,\n",
       "              -0.0292,      0.0133],\n",
       "         ...,\n",
       "         [    -0.0128,     -0.0115,     -0.0055,  ...,     -0.0282,\n",
       "               0.0340,     -0.0034],\n",
       "         [    -0.0105,      0.0206,     -0.0219,  ...,      0.0169,\n",
       "              -0.0128,      0.0079],\n",
       "         [    -0.0230,     -0.0152,     -0.0331,  ...,     -0.0163,\n",
       "               0.0337,      0.0178]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0069,  0.0185, -0.0098,  ..., -0.0338, -0.0125,  0.0194],\n",
       "         [-0.0239,  0.0297,  0.0121,  ...,  0.0124, -0.0269,  0.0093],\n",
       "         [-0.0117, -0.0114, -0.0178,  ..., -0.0314, -0.0129, -0.0256],\n",
       "         ...,\n",
       "         [-0.0052, -0.0310,  0.0321,  ..., -0.0130,  0.0088, -0.0142],\n",
       "         [-0.0160, -0.0002,  0.0204,  ..., -0.0048,  0.0298, -0.0132],\n",
       "         [-0.0293,  0.0145,  0.0075,  ...,  0.0281, -0.0204, -0.0283]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0222, -0.0053,  0.0121,  ...,  0.0265, -0.0291,  0.0282],\n",
       "         [-0.0338,  0.0180,  0.0298,  ...,  0.0085, -0.0220,  0.0060],\n",
       "         [ 0.0103, -0.0150,  0.0290,  ...,  0.0069,  0.0183,  0.0274],\n",
       "         ...,\n",
       "         [-0.0084, -0.0072, -0.0220,  ..., -0.0008, -0.0077,  0.0300],\n",
       "         [ 0.0088, -0.0111, -0.0186,  ...,  0.0152,  0.0291,  0.0343],\n",
       "         [-0.0344, -0.0312,  0.0328,  ..., -0.0164, -0.0269, -0.0010]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0103,  0.0032,  0.0343,  ...,  0.0277,  0.0145,  0.0356],\n",
       "         [-0.0263, -0.0085,  0.0156,  ..., -0.0102,  0.0098,  0.0053],\n",
       "         [-0.0331, -0.0311, -0.0302,  ...,  0.0282, -0.0352, -0.0304],\n",
       "         ...,\n",
       "         [-0.0192, -0.0053, -0.0327,  ..., -0.0304, -0.0001,  0.0072],\n",
       "         [ 0.0243,  0.0129, -0.0250,  ..., -0.0286,  0.0121,  0.0169],\n",
       "         [-0.0261, -0.0087, -0.0180,  ..., -0.0066, -0.0085, -0.0256]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([     0.0196,     -0.0349,      0.0105,     -0.0203,     -0.0134,\n",
       "              0.0195,      0.0195,     -0.0034,     -0.0335,     -0.0337,\n",
       "             -0.0112,     -0.0279,      0.0356,      0.0165,     -0.0051,\n",
       "              0.0246,      0.0021,     -0.0042,     -0.0188,      0.0199,\n",
       "             -0.0148,      0.0260,     -0.0326,     -0.0260,      0.0215,\n",
       "             -0.0117,      0.0103,     -0.0005,      0.0177,     -0.0213,\n",
       "             -0.0274,     -0.0047,     -0.0328,     -0.0317,     -0.0261,\n",
       "              0.0142,      0.0228,     -0.0131,     -0.0234,     -0.0357,\n",
       "             -0.0324,      0.0336,      0.0010,     -0.0100,      0.0348,\n",
       "              0.0201,     -0.0310,     -0.0298,     -0.0076,     -0.0096,\n",
       "             -0.0191,     -0.0031,     -0.0267,     -0.0004,      0.0245,\n",
       "              0.0311,     -0.0130,      0.0328,     -0.0292,     -0.0010,\n",
       "             -0.0334,     -0.0343,     -0.0236,     -0.0284,      0.0298,\n",
       "             -0.0151,     -0.0055,      0.0220,      0.0134,      0.0072,\n",
       "              0.0348,     -0.0056,     -0.0028,     -0.0354,     -0.0328,\n",
       "             -0.0278,     -0.0309,     -0.0066,      0.0159,     -0.0355,\n",
       "              0.0236,      0.0166,     -0.0165,      0.0249,     -0.0292,\n",
       "             -0.0132,      0.0320,     -0.0237,     -0.0176,     -0.0078,\n",
       "              0.0310,     -0.0053,     -0.0049,     -0.0118,     -0.0157,\n",
       "             -0.0341,      0.0161,      0.0133,      0.0180,      0.0146,\n",
       "              0.0159,      0.0264,     -0.0132,     -0.0169,      0.0349,\n",
       "              0.0269,     -0.0177,     -0.0081,      0.0146,     -0.0186,\n",
       "              0.0146,      0.0156,      0.0150,     -0.0236,     -0.0200,\n",
       "              0.0317,      0.0223,     -0.0059,      0.0035,     -0.0050,\n",
       "              0.0058,     -0.0305,     -0.0298,      0.0152,     -0.0215,\n",
       "              0.0154,      0.0054,     -0.0271,     -0.0079,     -0.0079,\n",
       "              0.0135,      0.0149,     -0.0228,      0.0229,      0.0314,\n",
       "              0.0299,      0.0180,      0.0041,      0.0286,     -0.0308,\n",
       "             -0.0197,     -0.0348,      0.0071,      0.0231,      0.0269,\n",
       "             -0.0115,      0.0035,     -0.0352,     -0.0129,      0.0344,\n",
       "              0.0015,      0.0251,     -0.0358,      0.0052,     -0.0046,\n",
       "              0.0340,      0.0360,     -0.0176,      0.0162,      0.0197,\n",
       "              0.0014,      0.0155,      0.0176,     -0.0330,      0.0273,\n",
       "             -0.0206,     -0.0020,     -0.0093,      0.0098,     -0.0325,\n",
       "             -0.0119,      0.0210,     -0.0071,     -0.0291,     -0.0308,\n",
       "             -0.0179,     -0.0326,     -0.0098,      0.0314,      0.0237,\n",
       "             -0.0241,      0.0113,     -0.0048,      0.0206,      0.0054,\n",
       "              0.0306,      0.0299,     -0.0093,      0.0030,      0.0113,\n",
       "             -0.0081,     -0.0167,      0.0157,     -0.0066,      0.0205,\n",
       "              0.0324,      0.0145,     -0.0203,      0.0260,     -0.0244,\n",
       "             -0.0235,     -0.0077,      0.0053,      0.0280,      0.0098,\n",
       "             -0.0099,     -0.0065,     -0.0199,     -0.0334,     -0.0334,\n",
       "              0.0211,      0.0264,     -0.0205,     -0.0302,      0.0196,\n",
       "             -0.0172,      0.0262,     -0.0180,      0.0227,      0.0286,\n",
       "             -0.0341,      0.0195,      0.0287,     -0.0235,      0.0242,\n",
       "              0.0139,      0.0060,     -0.0081,      0.0061,      0.0214,\n",
       "              0.0023,     -0.0000,      0.0346,     -0.0009,      0.0171,\n",
       "             -0.0260,      0.0195,     -0.0094,      0.0033,      0.0113,\n",
       "             -0.0247,     -0.0079,     -0.0119,      0.0334,     -0.0100,\n",
       "             -0.0119,     -0.0196,     -0.0133,     -0.0016,     -0.0164,\n",
       "              0.0178,      0.0209,      0.0012,     -0.0261,      0.0045,\n",
       "             -0.0343,      0.0022,      0.0346,      0.0242,      0.0284,\n",
       "             -0.0001,      0.0357,      0.0024,     -0.0127,     -0.0282,\n",
       "              0.0095,      0.0000,      0.0141,      0.0355,      0.0053,\n",
       "              0.0160,      0.0305,     -0.0355,      0.0324,     -0.0088,\n",
       "              0.0311,      0.0312,     -0.0041,     -0.0027,      0.0060,\n",
       "              0.0325,     -0.0209,     -0.0343,     -0.0108,      0.0231,\n",
       "              0.0053,     -0.0109,     -0.0052,      0.0176,     -0.0055,\n",
       "              0.0355,     -0.0315,     -0.0255,     -0.0319,      0.0303,\n",
       "              0.0069,      0.0252,     -0.0163,      0.0295,     -0.0082,\n",
       "             -0.0178,     -0.0074,     -0.0092,      0.0162,     -0.0167,\n",
       "             -0.0015,     -0.0192,      0.0094,     -0.0050,     -0.0189,\n",
       "              0.0314,      0.0186,      0.0299,      0.0353,     -0.0059,\n",
       "              0.0231,     -0.0042,     -0.0317,      0.0339,      0.0228,\n",
       "             -0.0119,      0.0243,      0.0130,      0.0164,     -0.0015,\n",
       "             -0.0266,     -0.0273,     -0.0312,     -0.0285,      0.0123,\n",
       "             -0.0198,     -0.0157,     -0.0294,     -0.0303,     -0.0084,\n",
       "             -0.0169,     -0.0043,      0.0030,     -0.0025,     -0.0314,\n",
       "              0.0070,     -0.0192,      0.0123,     -0.0110,      0.0222,\n",
       "             -0.0317,      0.0083,     -0.0338,     -0.0052,     -0.0067,\n",
       "             -0.0039,     -0.0170,      0.0150,     -0.0000,     -0.0111,\n",
       "              0.0182,      0.0076,     -0.0348,      0.0258,     -0.0132,\n",
       "             -0.0245,      0.0067,     -0.0289,      0.0087,      0.0018,\n",
       "              0.0257,     -0.0196,     -0.0289,     -0.0239,     -0.0303,\n",
       "              0.0165,      0.0279,     -0.0122,      0.0206,      0.0203,\n",
       "              0.0175,     -0.0211,      0.0265,      0.0001,      0.0325,\n",
       "             -0.0228,     -0.0295,      0.0232,      0.0050,      0.0304,\n",
       "              0.0051,     -0.0191,      0.0201,     -0.0222,      0.0113,\n",
       "             -0.0208,     -0.0177,     -0.0011,      0.0158,      0.0344,\n",
       "             -0.0120,     -0.0076,      0.0256,      0.0251,      0.0284,\n",
       "              0.0195,      0.0211,      0.0301,      0.0168,     -0.0345,\n",
       "             -0.0237,      0.0106,     -0.0310,     -0.0298,     -0.0118,\n",
       "             -0.0185,      0.0010,      0.0057,      0.0046,     -0.0290,\n",
       "             -0.0117,      0.0200,     -0.0156,     -0.0047,     -0.0140,\n",
       "             -0.0186,     -0.0261,     -0.0097,      0.0158,      0.0037,\n",
       "              0.0308,      0.0297,      0.0248,      0.0281,     -0.0292,\n",
       "             -0.0154,     -0.0177,      0.0211,      0.0093,     -0.0033,\n",
       "              0.0307,      0.0106,     -0.0341,      0.0192,      0.0183,\n",
       "             -0.0242,     -0.0317,     -0.0066,      0.0212,     -0.0184,\n",
       "             -0.0323,      0.0280,     -0.0106,     -0.0286,     -0.0284,\n",
       "             -0.0041,     -0.0088,     -0.0238,     -0.0161,      0.0040,\n",
       "             -0.0232,      0.0216,      0.0286,      0.0312,      0.0333,\n",
       "             -0.0246,      0.0349,      0.0261,     -0.0308,     -0.0181,\n",
       "              0.0019,     -0.0205,     -0.0351,     -0.0091,     -0.0105,\n",
       "              0.0132,     -0.0205,      0.0112,     -0.0042,     -0.0277,\n",
       "              0.0031,      0.0355,     -0.0041,     -0.0004,      0.0220,\n",
       "              0.0214,     -0.0196,      0.0055,     -0.0357,     -0.0134,\n",
       "             -0.0267,      0.0242,      0.0343,     -0.0254,      0.0037,\n",
       "             -0.0341,     -0.0162,     -0.0097,      0.0035,     -0.0063,\n",
       "             -0.0061,     -0.0095,     -0.0210,     -0.0352,      0.0298,\n",
       "             -0.0183,     -0.0094,     -0.0195,      0.0058,      0.0060,\n",
       "              0.0003,      0.0026,      0.0189,     -0.0124,     -0.0142,\n",
       "             -0.0048,      0.0233,     -0.0210,     -0.0230,     -0.0039,\n",
       "              0.0026,      0.0101,     -0.0294,     -0.0104,      0.0229,\n",
       "             -0.0175,     -0.0289,     -0.0063,     -0.0161,      0.0193,\n",
       "             -0.0321,      0.0167,     -0.0121,     -0.0357,      0.0356,\n",
       "             -0.0251,     -0.0310,      0.0314,     -0.0260,      0.0134,\n",
       "              0.0055,     -0.0114,     -0.0081,      0.0360,      0.0183,\n",
       "              0.0160,     -0.0118,     -0.0156,     -0.0095,     -0.0307,\n",
       "              0.0315,     -0.0253,      0.0253,      0.0056,      0.0015,\n",
       "             -0.0266,      0.0144,      0.0048,      0.0305,      0.0284,\n",
       "             -0.0230,     -0.0286,     -0.0291,      0.0144,      0.0071,\n",
       "              0.0193,     -0.0085,      0.0245,      0.0108,      0.0308,\n",
       "             -0.0192,      0.0065,      0.0257,     -0.0065,      0.0239,\n",
       "             -0.0000,      0.0160,      0.0292,      0.0044,     -0.0058,\n",
       "             -0.0304,      0.0038,      0.0138,      0.0059,     -0.0358,\n",
       "              0.0330,      0.0182,      0.0089,     -0.0204,     -0.0356,\n",
       "              0.0293,      0.0112,     -0.0156,      0.0071,     -0.0063,\n",
       "              0.0095,     -0.0122,      0.0229,     -0.0228,     -0.0316,\n",
       "              0.0001,      0.0012,     -0.0317,     -0.0195,     -0.0126,\n",
       "             -0.0357,     -0.0333,     -0.0308,      0.0027,      0.0130,\n",
       "              0.0199,     -0.0131,     -0.0052,      0.0128,      0.0049,\n",
       "              0.0315,     -0.0075,      0.0232,      0.0182,     -0.0065,\n",
       "             -0.0119,     -0.0085,     -0.0263,     -0.0272,     -0.0230,\n",
       "              0.0267,     -0.0066,     -0.0072,     -0.0164,      0.0062,\n",
       "             -0.0024,      0.0152,      0.0334,     -0.0020,      0.0048,\n",
       "              0.0231,     -0.0182,     -0.0169,      0.0041,      0.0282,\n",
       "             -0.0233,     -0.0002,     -0.0151,      0.0320,      0.0272,\n",
       "             -0.0256,      0.0267,     -0.0230,     -0.0354,      0.0288,\n",
       "              0.0357,      0.0094,     -0.0229,      0.0061,      0.0261,\n",
       "              0.0310,     -0.0262,     -0.0255,     -0.0085,      0.0220,\n",
       "             -0.0062,     -0.0325,      0.0246,      0.0198,      0.0148,\n",
       "              0.0338,     -0.0007,      0.0023,      0.0335,     -0.0006,\n",
       "             -0.0020,     -0.0248,     -0.0344,     -0.0325,     -0.0236,\n",
       "             -0.0339,      0.0122,     -0.0276,     -0.0289,     -0.0147,\n",
       "              0.0175,     -0.0043,      0.0215,     -0.0062,     -0.0278,\n",
       "              0.0073,      0.0160,     -0.0005,     -0.0339,      0.0031,\n",
       "              0.0267,     -0.0263,     -0.0204,      0.0313,     -0.0328,\n",
       "              0.0190,     -0.0264,      0.0218,      0.0165,     -0.0104,\n",
       "             -0.0126,     -0.0113,     -0.0129,     -0.0072,     -0.0338,\n",
       "              0.0346,     -0.0351,     -0.0295,     -0.0045,      0.0075,\n",
       "             -0.0066,     -0.0222,     -0.0342,      0.0240,     -0.0116,\n",
       "             -0.0340,      0.0021,      0.0284,      0.0197,     -0.0107,\n",
       "             -0.0014,     -0.0280,     -0.0298,     -0.0159,     -0.0009,\n",
       "             -0.0030,      0.0177,     -0.0139,      0.0357,     -0.0089,\n",
       "             -0.0067,      0.0191,      0.0169,      0.0002,      0.0314,\n",
       "             -0.0180,      0.0086,      0.0117,      0.0079,      0.0092,\n",
       "              0.0109,     -0.0275,      0.0339,      0.0023,     -0.0318,\n",
       "             -0.0262,     -0.0289,      0.0056,     -0.0173,      0.0354,\n",
       "             -0.0251,      0.0213,      0.0113,     -0.0306,      0.0154,\n",
       "             -0.0054,      0.0199,      0.0299,     -0.0314,     -0.0060,\n",
       "             -0.0086,     -0.0169,      0.0246,      0.0023,     -0.0015,\n",
       "              0.0093,      0.0328,     -0.0256,      0.0074,      0.0105,\n",
       "             -0.0183,      0.0225,      0.0139], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0103, -0.0360,  0.0230,  ..., -0.0194,  0.0186,  0.0060],\n",
       "         [ 0.0201,  0.0359,  0.0009,  ..., -0.0267,  0.0107, -0.0199],\n",
       "         [-0.0292, -0.0326, -0.0317,  ...,  0.0243, -0.0088,  0.0032],\n",
       "         ...,\n",
       "         [-0.0057, -0.0144,  0.0081,  ...,  0.0232, -0.0151, -0.0192],\n",
       "         [ 0.0011, -0.0250, -0.0108,  ..., -0.0349, -0.0258, -0.0092],\n",
       "         [ 0.0011,  0.0305,  0.0177,  ..., -0.0336,  0.0067,  0.0054]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0229, -0.0333,  0.0345,  ..., -0.0100, -0.0274,  0.0327],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0051, -0.0116, -0.0105,  ..., -0.0164,  0.0024, -0.0097],\n",
       "         [-0.0147,  0.0092, -0.0043,  ..., -0.0082,  0.0137,  0.0101],\n",
       "         [-0.0044,  0.0125,  0.0142,  ..., -0.0105,  0.0089,  0.0033],\n",
       "         ...,\n",
       "         [-0.0035, -0.0142,  0.0104,  ...,  0.0069, -0.0094,  0.0153],\n",
       "         [ 0.0111, -0.0043,  0.0004,  ..., -0.0123, -0.0126, -0.0054],\n",
       "         [-0.0150, -0.0046,  0.0054,  ...,  0.0113, -0.0080, -0.0109]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([     0.0053,      0.0004,      0.0041,     -0.0124,     -0.0012,\n",
       "              0.0079,      0.0068,     -0.0052,     -0.0086,      0.0119,\n",
       "              0.0027,     -0.0064,     -0.0146,     -0.0162,      0.0175,\n",
       "             -0.0164,     -0.0067,     -0.0001,      0.0121,     -0.0180,\n",
       "             -0.0027,     -0.0071,     -0.0120,     -0.0057,      0.0041,\n",
       "             -0.0086,      0.0164,      0.0012,     -0.0095,     -0.0104,\n",
       "              0.0053,     -0.0086,     -0.0043,     -0.0131,     -0.0137,\n",
       "             -0.0063,      0.0126,     -0.0029,      0.0121,      0.0100,\n",
       "              0.0145,      0.0033,     -0.0152,      0.0080,     -0.0059,\n",
       "             -0.0140,     -0.0110,      0.0090,     -0.0116,      0.0116,\n",
       "              0.0173,      0.0048,      0.0113,     -0.0135,      0.0148,\n",
       "              0.0071,     -0.0046,     -0.0157,     -0.0037,     -0.0123,\n",
       "             -0.0103,      0.0165,     -0.0016,     -0.0084,      0.0151,\n",
       "             -0.0082,      0.0166,      0.0038,      0.0046,      0.0045,\n",
       "              0.0112,      0.0018,      0.0003,      0.0045,     -0.0161,\n",
       "              0.0078,     -0.0104,      0.0102,      0.0008,     -0.0089,\n",
       "             -0.0134,      0.0120,     -0.0008,     -0.0140,      0.0180,\n",
       "             -0.0138,     -0.0012,     -0.0100,     -0.0136,      0.0050,\n",
       "              0.0113,     -0.0166,      0.0144,     -0.0145,     -0.0001,\n",
       "             -0.0064,     -0.0033,     -0.0027,      0.0081,     -0.0106,\n",
       "             -0.0119,     -0.0125,     -0.0047,     -0.0146,     -0.0108,\n",
       "             -0.0064,     -0.0083,     -0.0167,      0.0131,     -0.0057,\n",
       "             -0.0161,     -0.0174,     -0.0144,      0.0123,      0.0122,\n",
       "              0.0073,     -0.0142,     -0.0021,     -0.0015,     -0.0087,\n",
       "              0.0103,      0.0016,      0.0154,     -0.0032,     -0.0003,\n",
       "             -0.0158,     -0.0144,     -0.0064,     -0.0008,     -0.0067,\n",
       "             -0.0041,     -0.0176,     -0.0157,      0.0091,     -0.0062,\n",
       "             -0.0176,      0.0027,     -0.0166,     -0.0142,      0.0006,\n",
       "             -0.0007,      0.0059,      0.0040,      0.0017,      0.0067,\n",
       "              0.0040,     -0.0174,     -0.0173,     -0.0024,      0.0178,\n",
       "              0.0061,     -0.0153,     -0.0092,      0.0177,     -0.0017,\n",
       "              0.0087,      0.0147,      0.0169,      0.0172,     -0.0137,\n",
       "             -0.0174,     -0.0043,     -0.0085,      0.0119,     -0.0163,\n",
       "             -0.0120,     -0.0018,     -0.0115,     -0.0124,     -0.0027,\n",
       "              0.0133,      0.0032,     -0.0075,     -0.0149,     -0.0053,\n",
       "             -0.0033,      0.0153,     -0.0000,      0.0057,     -0.0088,\n",
       "              0.0147,      0.0110,      0.0176,     -0.0103,      0.0175,\n",
       "             -0.0033,      0.0088,     -0.0049,     -0.0049,     -0.0092,\n",
       "              0.0040,      0.0144,      0.0176,     -0.0121,     -0.0143,\n",
       "             -0.0157,     -0.0124,     -0.0068,     -0.0067,     -0.0072,\n",
       "             -0.0031,      0.0130,      0.0007,     -0.0010,     -0.0020,\n",
       "              0.0074,     -0.0050,      0.0034,     -0.0032,      0.0142,\n",
       "              0.0104,      0.0101,      0.0050,      0.0052,      0.0147,\n",
       "              0.0053,     -0.0025,      0.0150,     -0.0080,      0.0051,\n",
       "             -0.0084,      0.0165,      0.0004,     -0.0116,     -0.0069,\n",
       "              0.0180,     -0.0142,      0.0023,     -0.0025,     -0.0052,\n",
       "             -0.0003,     -0.0143,      0.0046,      0.0017,     -0.0160,\n",
       "             -0.0036,      0.0115,     -0.0107,      0.0104,      0.0054,\n",
       "              0.0164,      0.0047,     -0.0163,     -0.0164,     -0.0038,\n",
       "              0.0024,     -0.0139,     -0.0013,      0.0070,      0.0118,\n",
       "              0.0078,      0.0157,     -0.0167,     -0.0093,      0.0031,\n",
       "             -0.0091,     -0.0017,      0.0104,     -0.0098,      0.0085,\n",
       "             -0.0067,      0.0157,      0.0094,     -0.0074,     -0.0036,\n",
       "             -0.0119,      0.0171,      0.0017,      0.0059,     -0.0060,\n",
       "              0.0013,      0.0050,      0.0155,     -0.0044,     -0.0029,\n",
       "              0.0058,      0.0027,      0.0159,     -0.0010,      0.0134,\n",
       "              0.0147,     -0.0078,     -0.0060,      0.0038,     -0.0173,\n",
       "              0.0033,     -0.0104,     -0.0045,     -0.0140,      0.0005,\n",
       "             -0.0101,     -0.0014,      0.0155,      0.0137,     -0.0094,\n",
       "             -0.0164,     -0.0024,      0.0017,      0.0166,      0.0087,\n",
       "              0.0162,     -0.0178,     -0.0022,     -0.0112,     -0.0135,\n",
       "              0.0177,      0.0140,     -0.0104,      0.0168,      0.0015,\n",
       "             -0.0046,      0.0046,     -0.0158,      0.0168,     -0.0163,\n",
       "              0.0150,      0.0010,      0.0144,      0.0178,     -0.0007,\n",
       "              0.0148,      0.0173,      0.0142,     -0.0177,      0.0040,\n",
       "              0.0036,      0.0104,     -0.0122,      0.0121,     -0.0070,\n",
       "              0.0090,     -0.0089,     -0.0091,      0.0129,      0.0028,\n",
       "             -0.0163,      0.0058,      0.0028,     -0.0068,     -0.0044,\n",
       "              0.0164,     -0.0094,      0.0062,     -0.0043,      0.0134,\n",
       "             -0.0091,     -0.0094,     -0.0019,      0.0049,     -0.0156,\n",
       "             -0.0056,      0.0162,      0.0156,     -0.0043,      0.0172,\n",
       "             -0.0107,      0.0044,     -0.0147,     -0.0038,      0.0021,\n",
       "             -0.0178,      0.0063,      0.0131,      0.0124,      0.0048,\n",
       "             -0.0180,     -0.0003,      0.0173,     -0.0082,      0.0068,\n",
       "              0.0043,     -0.0164,      0.0140,     -0.0142,      0.0078,\n",
       "             -0.0172,      0.0150,      0.0179,      0.0042,      0.0174,\n",
       "             -0.0034,      0.0071,      0.0079,     -0.0157,      0.0008,\n",
       "              0.0123,      0.0088,      0.0050,     -0.0177,     -0.0112,\n",
       "             -0.0167,     -0.0019,      0.0172,      0.0008,      0.0063,\n",
       "              0.0092,     -0.0110,      0.0073,      0.0067,      0.0064,\n",
       "              0.0140,     -0.0179,     -0.0036,     -0.0166,     -0.0024,\n",
       "              0.0039,     -0.0042,     -0.0179,     -0.0067,      0.0149,\n",
       "              0.0178,     -0.0163,     -0.0050,      0.0100,     -0.0008,\n",
       "             -0.0045,      0.0045,     -0.0156,      0.0141,      0.0154,\n",
       "             -0.0038,     -0.0027,      0.0049,     -0.0029,     -0.0158,\n",
       "             -0.0039,      0.0154,      0.0071,     -0.0083,      0.0176,\n",
       "             -0.0144,     -0.0096,      0.0090,     -0.0004,      0.0065,\n",
       "              0.0034,      0.0047,     -0.0026,     -0.0077,      0.0083,\n",
       "             -0.0052,      0.0001,      0.0136,     -0.0083,      0.0125,\n",
       "              0.0094,      0.0086,      0.0114,     -0.0024,      0.0022,\n",
       "              0.0116,     -0.0049,     -0.0163,      0.0108,      0.0071,\n",
       "              0.0008,     -0.0072,     -0.0180,     -0.0081,      0.0015,\n",
       "             -0.0068,      0.0035,      0.0069,     -0.0032,     -0.0180,\n",
       "             -0.0060,      0.0157,      0.0174,      0.0072,      0.0071,\n",
       "              0.0021,      0.0076,     -0.0116,     -0.0051,     -0.0043,\n",
       "              0.0103,      0.0147,     -0.0115,      0.0029,     -0.0091,\n",
       "              0.0171,      0.0004,      0.0092,      0.0021,      0.0114,\n",
       "             -0.0008,      0.0017,     -0.0177,      0.0097,     -0.0006,\n",
       "              0.0160,     -0.0018,      0.0059,     -0.0011,      0.0025,\n",
       "             -0.0093,     -0.0008,      0.0016,     -0.0161,     -0.0062,\n",
       "             -0.0090,      0.0122,     -0.0025,      0.0135,     -0.0038,\n",
       "              0.0032,     -0.0164,      0.0126,     -0.0146,     -0.0032,\n",
       "             -0.0175,      0.0079,      0.0039,     -0.0100,      0.0132,\n",
       "             -0.0122,     -0.0034,     -0.0160,     -0.0004,      0.0073,\n",
       "              0.0072,      0.0127,     -0.0090,      0.0132,     -0.0178,\n",
       "             -0.0055,      0.0060,     -0.0103,     -0.0156,     -0.0019,\n",
       "              0.0145,      0.0177,     -0.0062,      0.0155,     -0.0150,\n",
       "             -0.0104,     -0.0033,     -0.0013,      0.0091,     -0.0003,\n",
       "              0.0006,     -0.0025,     -0.0153,     -0.0001,      0.0064,\n",
       "             -0.0148,      0.0166,      0.0030,     -0.0070,      0.0058,\n",
       "              0.0072,     -0.0068,      0.0015,     -0.0027,      0.0064,\n",
       "              0.0019,     -0.0121,      0.0165,     -0.0074,     -0.0104,\n",
       "              0.0076,     -0.0021,      0.0097,      0.0120,     -0.0068,\n",
       "             -0.0056,      0.0139,     -0.0019,      0.0117,     -0.0064,\n",
       "             -0.0069,      0.0014,     -0.0065,     -0.0106,      0.0019,\n",
       "              0.0088,      0.0060,      0.0128,      0.0071,     -0.0001,\n",
       "             -0.0152,     -0.0016,     -0.0164,      0.0005,     -0.0006,\n",
       "              0.0039,     -0.0101,      0.0127,     -0.0011,      0.0016,\n",
       "              0.0022,     -0.0112,      0.0089,      0.0113,      0.0147,\n",
       "              0.0061,     -0.0054,     -0.0155,      0.0170,     -0.0079,\n",
       "              0.0154,      0.0029,      0.0092,     -0.0021,      0.0123,\n",
       "             -0.0027,     -0.0099,      0.0133,      0.0091,     -0.0030,\n",
       "             -0.0073,      0.0149,     -0.0010,     -0.0089,      0.0151,\n",
       "             -0.0124,      0.0034,      0.0113,     -0.0052,     -0.0149,\n",
       "             -0.0089,     -0.0051,      0.0162,      0.0039,     -0.0014,\n",
       "             -0.0002,      0.0138,      0.0014,     -0.0036,     -0.0007,\n",
       "              0.0010,     -0.0120,      0.0140,     -0.0002,     -0.0030,\n",
       "              0.0128,     -0.0106,      0.0035,      0.0168,     -0.0164,\n",
       "             -0.0056,     -0.0129,     -0.0097,     -0.0172,      0.0177,\n",
       "              0.0056,     -0.0050,     -0.0027,     -0.0024,      0.0047,\n",
       "              0.0108,     -0.0001,      0.0039,      0.0159,     -0.0065,\n",
       "             -0.0123,      0.0076,      0.0123,      0.0019,     -0.0013,\n",
       "             -0.0099,     -0.0131,     -0.0147,     -0.0078,      0.0139,\n",
       "              0.0013,      0.0116,     -0.0134,     -0.0117,      0.0137,\n",
       "              0.0011,      0.0064,      0.0144,     -0.0150,      0.0086,\n",
       "             -0.0157,      0.0041,     -0.0050,      0.0043,      0.0033,\n",
       "              0.0133,      0.0118,     -0.0157,     -0.0118,      0.0095,\n",
       "              0.0058,     -0.0125,     -0.0133,     -0.0103,      0.0003,\n",
       "              0.0123,     -0.0120,      0.0042,      0.0132,     -0.0026,\n",
       "              0.0119,      0.0090,     -0.0023,      0.0034,      0.0003,\n",
       "              0.0026,      0.0049,      0.0150,     -0.0081,      0.0051,\n",
       "             -0.0172,      0.0020,      0.0151,      0.0008,     -0.0125,\n",
       "              0.0031,     -0.0015,     -0.0036,     -0.0175,      0.0133,\n",
       "             -0.0011,     -0.0009,      0.0063,      0.0150,     -0.0073,\n",
       "             -0.0050,     -0.0073,     -0.0061,     -0.0105,      0.0134,\n",
       "             -0.0169,     -0.0032,      0.0147,      0.0162,     -0.0165,\n",
       "             -0.0001,     -0.0086,      0.0162,     -0.0018,      0.0006,\n",
       "             -0.0096,     -0.0163,     -0.0164,     -0.0094,      0.0098,\n",
       "              0.0100,      0.0174,      0.0148,     -0.0066,      0.0077,\n",
       "             -0.0016,      0.0044,     -0.0160,      0.0072,      0.0134,\n",
       "              0.0096,     -0.0057,      0.0158,      0.0084,     -0.0103,\n",
       "             -0.0138,     -0.0040,     -0.0144,      0.0174,      0.0042,\n",
       "              0.0025,      0.0133,      0.0061,     -0.0113,     -0.0056,\n",
       "             -0.0173,     -0.0003,      0.0156], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0091, -0.0291, -0.0291,  ..., -0.0319,  0.0028, -0.0041],\n",
       "         [ 0.0341,  0.0066, -0.0184,  ...,  0.0033,  0.0324,  0.0202],\n",
       "         [-0.0153,  0.0086,  0.0200,  ..., -0.0089, -0.0207, -0.0193],\n",
       "         ...,\n",
       "         [ 0.0264, -0.0083,  0.0298,  ..., -0.0010, -0.0352,  0.0061],\n",
       "         [-0.0309,  0.0348, -0.0241,  ...,  0.0180,  0.0342, -0.0249],\n",
       "         [ 0.0211, -0.0018,  0.0169,  ..., -0.0175, -0.0210, -0.0135]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0135, -0.0099,  0.0116,  ...,  0.0176,  0.0195,  0.0209],\n",
       "         [-0.0123, -0.0190, -0.0011,  ..., -0.0018,  0.0128,  0.0140],\n",
       "         [-0.0054, -0.0165, -0.0340,  ..., -0.0048, -0.0305,  0.0310],\n",
       "         ...,\n",
       "         [-0.0265, -0.0210,  0.0150,  ..., -0.0166,  0.0277, -0.0233],\n",
       "         [-0.0118, -0.0217,  0.0245,  ...,  0.0225, -0.0325,  0.0343],\n",
       "         [-0.0253, -0.0156, -0.0124,  ...,  0.0104, -0.0250,  0.0313]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0057, -0.0111, -0.0034,  ...,  0.0313, -0.0079,  0.0130],\n",
       "         [ 0.0230, -0.0054,  0.0059,  ...,  0.0126, -0.0104,  0.0294],\n",
       "         [-0.0120,  0.0288,  0.0287,  ..., -0.0297, -0.0199,  0.0004],\n",
       "         ...,\n",
       "         [ 0.0209,  0.0216,  0.0117,  ..., -0.0160, -0.0330, -0.0244],\n",
       "         [-0.0142, -0.0260,  0.0244,  ..., -0.0011,  0.0302,  0.0045],\n",
       "         [ 0.0221, -0.0339, -0.0285,  ...,  0.0339,  0.0280,  0.0264]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0334,  0.0206, -0.0180,  ...,  0.0229,  0.0100,  0.0202],\n",
       "         [ 0.0285, -0.0039,  0.0050,  ..., -0.0018,  0.0194,  0.0350],\n",
       "         [ 0.0298,  0.0020,  0.0075,  ...,  0.0126,  0.0287, -0.0051],\n",
       "         ...,\n",
       "         [ 0.0276,  0.0042, -0.0156,  ..., -0.0247, -0.0147, -0.0016],\n",
       "         [ 0.0104,  0.0187,  0.0058,  ...,  0.0278,  0.0154,  0.0145],\n",
       "         [-0.0132, -0.0251,  0.0186,  ...,  0.0329,  0.0134,  0.0177]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([     0.0002,      0.0107,     -0.0276,      0.0019,     -0.0322,\n",
       "             -0.0036,     -0.0103,     -0.0306,     -0.0119,     -0.0176,\n",
       "             -0.0114,     -0.0293,     -0.0360,      0.0099,      0.0295,\n",
       "             -0.0329,     -0.0278,      0.0143,     -0.0221,      0.0115,\n",
       "             -0.0251,      0.0009,      0.0182,      0.0293,     -0.0004,\n",
       "              0.0135,     -0.0206,     -0.0289,      0.0033,      0.0071,\n",
       "              0.0264,      0.0193,      0.0181,     -0.0209,      0.0031,\n",
       "              0.0024,     -0.0017,      0.0352,      0.0292,      0.0131,\n",
       "              0.0186,     -0.0056,     -0.0020,      0.0194,     -0.0194,\n",
       "              0.0055,     -0.0358,      0.0119,     -0.0145,      0.0259,\n",
       "              0.0140,     -0.0161,      0.0129,      0.0182,      0.0171,\n",
       "             -0.0172,      0.0023,      0.0001,     -0.0100,     -0.0090,\n",
       "             -0.0002,      0.0239,      0.0222,     -0.0035,      0.0163,\n",
       "             -0.0308,      0.0036,     -0.0029,     -0.0356,      0.0018,\n",
       "              0.0074,      0.0293,     -0.0089,     -0.0105,      0.0335,\n",
       "              0.0083,     -0.0168,      0.0164,     -0.0235,      0.0301,\n",
       "             -0.0334,      0.0361,     -0.0266,      0.0341,     -0.0255,\n",
       "             -0.0039,     -0.0300,      0.0296,      0.0162,      0.0044,\n",
       "              0.0003,      0.0358,     -0.0219,      0.0318,      0.0239,\n",
       "             -0.0277,     -0.0213,      0.0198,      0.0037,     -0.0054,\n",
       "              0.0004,     -0.0050,     -0.0279,     -0.0186,      0.0178,\n",
       "             -0.0323,     -0.0182,     -0.0194,      0.0189,     -0.0082,\n",
       "             -0.0331,     -0.0279,      0.0031,      0.0095,      0.0323,\n",
       "              0.0315,      0.0035,      0.0182,     -0.0131,     -0.0283,\n",
       "             -0.0295,      0.0029,      0.0240,      0.0211,      0.0312,\n",
       "              0.0025,     -0.0332,      0.0303,     -0.0169,      0.0193,\n",
       "              0.0199,     -0.0235,      0.0247,      0.0228,     -0.0021,\n",
       "              0.0145,      0.0316,     -0.0159,     -0.0180,     -0.0106,\n",
       "             -0.0242,     -0.0225,     -0.0097,     -0.0259,     -0.0215,\n",
       "             -0.0050,     -0.0039,     -0.0329,      0.0233,      0.0197,\n",
       "             -0.0055,      0.0003,      0.0087,      0.0230,      0.0065,\n",
       "              0.0040,     -0.0117,     -0.0285,      0.0083,     -0.0124,\n",
       "             -0.0137,     -0.0233,     -0.0241,     -0.0243,      0.0216,\n",
       "              0.0214,      0.0023,      0.0175,     -0.0155,     -0.0335,\n",
       "             -0.0300,     -0.0165,      0.0094,      0.0267,     -0.0170,\n",
       "              0.0226,     -0.0109,      0.0056,     -0.0090,      0.0164,\n",
       "              0.0048,     -0.0179,     -0.0303,     -0.0025,      0.0008,\n",
       "             -0.0114,     -0.0120,     -0.0142,     -0.0260,     -0.0088,\n",
       "             -0.0018,     -0.0183,      0.0357,     -0.0181,      0.0045,\n",
       "              0.0011,      0.0187,     -0.0319,      0.0266,     -0.0332,\n",
       "             -0.0054,      0.0312,      0.0329,      0.0332,      0.0314,\n",
       "             -0.0028,     -0.0002,     -0.0229,     -0.0180,      0.0153,\n",
       "              0.0279,      0.0325,      0.0189,      0.0173,     -0.0244,\n",
       "              0.0124,      0.0146,      0.0084,     -0.0134,      0.0133,\n",
       "              0.0179,     -0.0273,      0.0027,      0.0251,     -0.0005,\n",
       "              0.0056,      0.0327,      0.0227,     -0.0016,     -0.0111,\n",
       "              0.0118,      0.0242,     -0.0288,     -0.0088,      0.0348,\n",
       "             -0.0330,      0.0299,      0.0025,     -0.0315,      0.0222,\n",
       "              0.0012,      0.0210,      0.0270,     -0.0224,     -0.0073,\n",
       "             -0.0016,     -0.0091,      0.0285,     -0.0211,      0.0300,\n",
       "             -0.0188,      0.0154,      0.0257,     -0.0260,      0.0080,\n",
       "             -0.0023,      0.0292,     -0.0120,      0.0201,     -0.0102,\n",
       "             -0.0291,      0.0229,     -0.0343,     -0.0264,     -0.0313,\n",
       "             -0.0196,      0.0022,     -0.0047,      0.0054,      0.0318,\n",
       "              0.0042,     -0.0093,     -0.0025,     -0.0304,      0.0349,\n",
       "              0.0113,     -0.0349,      0.0151,      0.0007,      0.0146,\n",
       "              0.0035,      0.0305,     -0.0161,     -0.0119,     -0.0013,\n",
       "             -0.0223,      0.0219,      0.0047,     -0.0282,     -0.0203,\n",
       "              0.0137,     -0.0314,     -0.0091,     -0.0096,      0.0154,\n",
       "              0.0163,     -0.0155,     -0.0156,      0.0069,     -0.0175,\n",
       "             -0.0294,      0.0062,     -0.0110,     -0.0196,      0.0280,\n",
       "              0.0004,      0.0330,     -0.0153,     -0.0016,     -0.0077,\n",
       "              0.0273,      0.0144,     -0.0008,      0.0289,     -0.0360,\n",
       "              0.0085,     -0.0311,      0.0132,     -0.0315,      0.0138,\n",
       "             -0.0069,      0.0242,     -0.0208,     -0.0218,     -0.0204,\n",
       "              0.0037,     -0.0104,     -0.0181,      0.0031,     -0.0085,\n",
       "              0.0020,     -0.0085,      0.0026,     -0.0035,     -0.0066,\n",
       "             -0.0148,     -0.0333,     -0.0175,     -0.0068,     -0.0309,\n",
       "              0.0124,      0.0243,      0.0103,     -0.0212,     -0.0329,\n",
       "              0.0173,     -0.0357,     -0.0086,     -0.0235,     -0.0077,\n",
       "             -0.0314,      0.0189,      0.0067,      0.0336,     -0.0308,\n",
       "              0.0047,     -0.0326,     -0.0270,      0.0160,     -0.0243,\n",
       "              0.0107,     -0.0174,      0.0261,      0.0149,      0.0238,\n",
       "             -0.0257,     -0.0033,      0.0144,     -0.0023,     -0.0232,\n",
       "             -0.0060,      0.0134,     -0.0249,      0.0337,      0.0331,\n",
       "              0.0339,      0.0221,      0.0227,      0.0059,      0.0011,\n",
       "             -0.0184,      0.0189,      0.0053,     -0.0127,      0.0212,\n",
       "              0.0111,      0.0290,      0.0296,     -0.0298,     -0.0226,\n",
       "             -0.0282,      0.0275,      0.0338,      0.0216,      0.0140,\n",
       "              0.0012,      0.0147,      0.0128,      0.0291,      0.0257,\n",
       "              0.0191,     -0.0339,      0.0099,      0.0323,      0.0104,\n",
       "              0.0238,     -0.0260,      0.0233,     -0.0305,      0.0252,\n",
       "             -0.0298,      0.0351,      0.0282,      0.0092,      0.0211,\n",
       "              0.0115,     -0.0298,      0.0337,     -0.0313,     -0.0348,\n",
       "             -0.0024,      0.0285,      0.0332,      0.0192,      0.0287,\n",
       "             -0.0108,     -0.0223,      0.0038,      0.0154,     -0.0091,\n",
       "             -0.0203,      0.0321,      0.0338,     -0.0120,     -0.0150,\n",
       "              0.0098,      0.0077,      0.0034,      0.0061,      0.0035,\n",
       "             -0.0190,     -0.0328,      0.0172,      0.0249,      0.0019,\n",
       "              0.0090,     -0.0263,     -0.0324,      0.0210,      0.0139,\n",
       "             -0.0275,      0.0338,     -0.0334,      0.0281,     -0.0088,\n",
       "             -0.0191,      0.0299,     -0.0105,     -0.0251,     -0.0182,\n",
       "              0.0035,      0.0278,      0.0184,      0.0024,      0.0268,\n",
       "             -0.0122,      0.0117,     -0.0341,     -0.0222,     -0.0084,\n",
       "              0.0216,     -0.0169,      0.0357,      0.0323,     -0.0339,\n",
       "             -0.0301,     -0.0058,     -0.0085,     -0.0114,     -0.0112,\n",
       "             -0.0000,     -0.0111,     -0.0246,     -0.0088,      0.0096,\n",
       "              0.0344,      0.0324,      0.0216,     -0.0200,      0.0197,\n",
       "              0.0038,     -0.0264,     -0.0115,      0.0182,      0.0313,\n",
       "             -0.0039,     -0.0029,     -0.0308,     -0.0081,     -0.0199,\n",
       "              0.0278,     -0.0306,      0.0243,      0.0272,     -0.0337,\n",
       "             -0.0015,     -0.0343,     -0.0346,      0.0242,     -0.0200,\n",
       "             -0.0266,      0.0322,     -0.0078,     -0.0103,      0.0211,\n",
       "              0.0015,     -0.0268,      0.0296,      0.0324,     -0.0219,\n",
       "             -0.0105,     -0.0241,      0.0104,      0.0063,      0.0229,\n",
       "             -0.0120,     -0.0118,      0.0301,     -0.0110,      0.0331,\n",
       "             -0.0204,     -0.0120,      0.0110,      0.0118,      0.0145,\n",
       "              0.0276,     -0.0017,     -0.0009,      0.0309,      0.0147,\n",
       "              0.0312,     -0.0019,      0.0143,      0.0006,     -0.0121,\n",
       "             -0.0319,      0.0038,     -0.0198,      0.0291,     -0.0073,\n",
       "             -0.0088,      0.0280,      0.0298,      0.0185,      0.0301,\n",
       "             -0.0050,     -0.0051,     -0.0281,      0.0135,      0.0336,\n",
       "              0.0269,     -0.0139,      0.0281,      0.0047,     -0.0270,\n",
       "             -0.0295,      0.0148,     -0.0094,     -0.0267,      0.0262,\n",
       "              0.0220,     -0.0022,      0.0070,     -0.0279,     -0.0079,\n",
       "              0.0231,      0.0280,      0.0030,      0.0220,      0.0214,\n",
       "             -0.0349,      0.0068,     -0.0107,      0.0287,     -0.0191,\n",
       "              0.0279,     -0.0219,      0.0154,     -0.0001,      0.0325,\n",
       "              0.0146,      0.0068,     -0.0146,      0.0224,      0.0225,\n",
       "              0.0147,      0.0036,     -0.0307,      0.0120,      0.0038,\n",
       "             -0.0266,     -0.0058,      0.0360,     -0.0189,     -0.0311,\n",
       "              0.0359,     -0.0350,      0.0110,      0.0149,     -0.0326,\n",
       "             -0.0176,      0.0100,      0.0323,      0.0040,     -0.0094,\n",
       "             -0.0314,     -0.0181,     -0.0106,      0.0273,     -0.0068,\n",
       "              0.0075,      0.0233,     -0.0188,     -0.0329,      0.0140,\n",
       "             -0.0025,      0.0154,      0.0228,     -0.0277,     -0.0071,\n",
       "             -0.0156,     -0.0321,     -0.0008,     -0.0002,      0.0073,\n",
       "             -0.0169,     -0.0322,     -0.0186,      0.0120,     -0.0171,\n",
       "             -0.0003,     -0.0217,      0.0104,      0.0280,     -0.0194,\n",
       "              0.0249,      0.0083,     -0.0027,      0.0354,     -0.0318,\n",
       "             -0.0003,      0.0333,      0.0147,     -0.0145,     -0.0134,\n",
       "             -0.0169,      0.0155,      0.0048,      0.0232,     -0.0257,\n",
       "              0.0130,      0.0006,      0.0359,     -0.0023,     -0.0058,\n",
       "              0.0353,      0.0271,      0.0247,     -0.0193,     -0.0208,\n",
       "              0.0154,      0.0355,      0.0127,     -0.0288,     -0.0303,\n",
       "             -0.0317,     -0.0118,     -0.0220,      0.0148,      0.0233,\n",
       "              0.0091,     -0.0052,     -0.0171,     -0.0078,     -0.0262,\n",
       "              0.0215,     -0.0092,     -0.0347,     -0.0058,      0.0120,\n",
       "             -0.0295,     -0.0267,     -0.0017,     -0.0151,     -0.0117,\n",
       "             -0.0337,     -0.0278,      0.0025,     -0.0063,      0.0320,\n",
       "             -0.0333,      0.0312,      0.0087,      0.0065,      0.0206,\n",
       "              0.0042,      0.0328,      0.0214,      0.0022,     -0.0278,\n",
       "             -0.0183,     -0.0254,      0.0009,     -0.0027,      0.0068,\n",
       "             -0.0054,      0.0161,      0.0113,      0.0084,      0.0043,\n",
       "             -0.0083,     -0.0243,      0.0175,     -0.0276,     -0.0006,\n",
       "             -0.0250,      0.0344,      0.0218,      0.0345,     -0.0009,\n",
       "              0.0139,     -0.0252,     -0.0316,     -0.0175,     -0.0330,\n",
       "              0.0307,     -0.0222,      0.0169,     -0.0127,     -0.0266,\n",
       "              0.0351,      0.0191,     -0.0176,      0.0016,      0.0270,\n",
       "              0.0097,      0.0217,      0.0103,      0.0328,      0.0072,\n",
       "              0.0214,     -0.0083,     -0.0039,      0.0143,     -0.0042,\n",
       "             -0.0087,      0.0308,     -0.0285,      0.0064,      0.0295,\n",
       "              0.0172,      0.0117,      0.0071,     -0.0099,     -0.0164,\n",
       "              0.0133,     -0.0026,      0.0333], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0218, -0.0093,  0.0129,  ..., -0.0161,  0.0017,  0.0062],\n",
       "         [ 0.0190,  0.0027,  0.0220,  ..., -0.0234,  0.0100, -0.0001],\n",
       "         [ 0.0291,  0.0181,  0.0276,  ..., -0.0008,  0.0059,  0.0102],\n",
       "         ...,\n",
       "         [-0.0312,  0.0163, -0.0206,  ..., -0.0124, -0.0161,  0.0176],\n",
       "         [-0.0328, -0.0092,  0.0160,  ..., -0.0177, -0.0259, -0.0294],\n",
       "         [ 0.0267,  0.0122, -0.0084,  ...,  0.0112, -0.0148,  0.0285]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.0334, -0.0237, -0.0118,  ..., -0.0033,  0.0214,  0.0123],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0179, -0.0158, -0.0001,  ...,  0.0141,  0.0048, -0.0173],\n",
       "         [-0.0142,  0.0161,  0.0125,  ...,  0.0069, -0.0114,  0.0111],\n",
       "         [ 0.0162, -0.0074, -0.0103,  ..., -0.0130, -0.0061,  0.0175],\n",
       "         ...,\n",
       "         [-0.0036,  0.0111,  0.0070,  ...,  0.0112, -0.0074, -0.0162],\n",
       "         [ 0.0075, -0.0016, -0.0105,  ...,  0.0069,  0.0086,  0.0105],\n",
       "         [ 0.0035, -0.0083,  0.0071,  ..., -0.0101,  0.0150,  0.0004]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([    -0.0078,      0.0040,      0.0123,      0.0032,     -0.0090,\n",
       "              0.0105,      0.0036,     -0.0107,     -0.0144,      0.0131,\n",
       "             -0.0011,     -0.0118,      0.0001,     -0.0018,     -0.0124,\n",
       "             -0.0067,      0.0005,     -0.0164,     -0.0059,      0.0019,\n",
       "              0.0064,      0.0161,     -0.0085,     -0.0174,      0.0006,\n",
       "              0.0040,     -0.0128,      0.0039,      0.0005,      0.0158,\n",
       "              0.0157,     -0.0030,     -0.0025,     -0.0105,     -0.0089,\n",
       "              0.0055,     -0.0035,     -0.0167,     -0.0111,     -0.0163,\n",
       "              0.0116,     -0.0035,      0.0092,     -0.0003,      0.0082,\n",
       "             -0.0046,     -0.0150,      0.0029,      0.0094,      0.0053,\n",
       "             -0.0139,      0.0045,     -0.0066,      0.0097,     -0.0063,\n",
       "              0.0141,     -0.0163,     -0.0141,     -0.0048,     -0.0032,\n",
       "             -0.0147,      0.0152,     -0.0008,     -0.0037,     -0.0170,\n",
       "             -0.0110,      0.0112,      0.0102,      0.0063,      0.0025,\n",
       "             -0.0156,      0.0088,      0.0007,     -0.0061,     -0.0060,\n",
       "              0.0020,     -0.0090,     -0.0151,     -0.0176,     -0.0094,\n",
       "              0.0014,      0.0162,      0.0098,      0.0022,      0.0090,\n",
       "              0.0105,      0.0137,     -0.0036,     -0.0134,      0.0106,\n",
       "              0.0157,      0.0026,     -0.0029,      0.0119,     -0.0178,\n",
       "             -0.0142,      0.0010,      0.0119,      0.0073,      0.0001,\n",
       "             -0.0140,      0.0116,      0.0144,     -0.0056,      0.0085,\n",
       "             -0.0016,     -0.0017,     -0.0023,     -0.0007,      0.0005,\n",
       "             -0.0131,     -0.0057,      0.0107,     -0.0137,     -0.0097,\n",
       "              0.0057,     -0.0043,     -0.0162,      0.0087,      0.0109,\n",
       "              0.0066,     -0.0152,     -0.0153,      0.0135,      0.0171,\n",
       "              0.0061,      0.0103,     -0.0158,     -0.0071,      0.0132,\n",
       "              0.0103,     -0.0144,     -0.0164,     -0.0062,     -0.0025,\n",
       "             -0.0016,     -0.0017,      0.0056,      0.0088,      0.0152,\n",
       "             -0.0057,     -0.0085,     -0.0068,      0.0169,      0.0017,\n",
       "              0.0146,     -0.0142,     -0.0081,     -0.0026,     -0.0119,\n",
       "             -0.0128,      0.0004,     -0.0135,      0.0005,      0.0080,\n",
       "              0.0131,      0.0106,      0.0052,     -0.0081,      0.0010,\n",
       "              0.0100,     -0.0163,     -0.0117,     -0.0171,     -0.0086,\n",
       "              0.0142,     -0.0031,      0.0159,      0.0159,      0.0013,\n",
       "             -0.0052,     -0.0163,     -0.0137,     -0.0025,     -0.0009,\n",
       "             -0.0150,      0.0052,      0.0094,      0.0144,      0.0032,\n",
       "             -0.0122,      0.0151,      0.0116,      0.0179,     -0.0024,\n",
       "              0.0152,     -0.0001,      0.0175,     -0.0165,     -0.0126,\n",
       "             -0.0178,      0.0073,     -0.0068,     -0.0053,     -0.0034,\n",
       "              0.0081,     -0.0037,      0.0074,      0.0152,     -0.0053,\n",
       "              0.0070,      0.0043,      0.0071,     -0.0116,      0.0071,\n",
       "              0.0103,      0.0077,     -0.0123,      0.0148,     -0.0020,\n",
       "             -0.0080,      0.0118,      0.0013,     -0.0026,      0.0175,\n",
       "              0.0029,      0.0150,      0.0151,     -0.0130,      0.0172,\n",
       "             -0.0156,     -0.0117,      0.0110,      0.0176,      0.0086,\n",
       "             -0.0161,      0.0125,      0.0014,      0.0035,     -0.0046,\n",
       "              0.0062,     -0.0145,      0.0134,      0.0047,      0.0042,\n",
       "             -0.0089,      0.0046,     -0.0160,      0.0169,      0.0175,\n",
       "             -0.0104,      0.0133,     -0.0019,     -0.0164,      0.0161,\n",
       "              0.0106,     -0.0143,      0.0146,     -0.0014,      0.0060,\n",
       "             -0.0053,      0.0022,      0.0044,     -0.0071,     -0.0174,\n",
       "             -0.0033,      0.0180,     -0.0118,      0.0068,      0.0068,\n",
       "              0.0048,     -0.0036,      0.0050,     -0.0076,     -0.0084,\n",
       "              0.0142,      0.0147,     -0.0131,      0.0077,      0.0154,\n",
       "              0.0057,      0.0020,      0.0170,     -0.0075,      0.0115,\n",
       "              0.0093,      0.0173,      0.0031,      0.0147,      0.0161,\n",
       "             -0.0130,     -0.0154,      0.0156,     -0.0073,     -0.0011,\n",
       "             -0.0178,      0.0067,     -0.0033,     -0.0025,     -0.0015,\n",
       "             -0.0150,      0.0121,      0.0082,      0.0007,      0.0139,\n",
       "             -0.0065,     -0.0104,      0.0114,     -0.0147,      0.0090,\n",
       "              0.0162,      0.0001,      0.0074,      0.0084,     -0.0128,\n",
       "              0.0088,      0.0140,     -0.0161,      0.0032,      0.0019,\n",
       "              0.0164,      0.0006,      0.0068,     -0.0160,     -0.0060,\n",
       "              0.0077,      0.0048,      0.0167,     -0.0093,     -0.0147,\n",
       "              0.0051,     -0.0037,     -0.0156,      0.0006,      0.0077,\n",
       "              0.0062,      0.0170,      0.0136,     -0.0069,      0.0090,\n",
       "              0.0089,     -0.0147,      0.0115,     -0.0016,     -0.0067,\n",
       "             -0.0113,     -0.0153,      0.0085,     -0.0168,     -0.0124,\n",
       "             -0.0123,      0.0132,      0.0180,      0.0061,      0.0002,\n",
       "             -0.0081,     -0.0062,      0.0150,      0.0065,      0.0171,\n",
       "              0.0108,     -0.0012,     -0.0119,     -0.0070,     -0.0041,\n",
       "             -0.0017,     -0.0127,     -0.0099,      0.0120,      0.0116,\n",
       "             -0.0172,     -0.0038,     -0.0077,     -0.0034,     -0.0012,\n",
       "             -0.0038,     -0.0073,      0.0033,      0.0057,      0.0132,\n",
       "             -0.0125,     -0.0093,     -0.0118,      0.0102,      0.0143,\n",
       "             -0.0120,      0.0167,     -0.0054,     -0.0058,      0.0055,\n",
       "             -0.0124,     -0.0077,     -0.0110,      0.0043,     -0.0038,\n",
       "             -0.0103,      0.0097,      0.0047,     -0.0036,     -0.0167,\n",
       "             -0.0107,     -0.0071,     -0.0073,     -0.0094,      0.0178,\n",
       "              0.0038,      0.0114,      0.0148,     -0.0043,     -0.0165,\n",
       "             -0.0070,     -0.0006,      0.0149,      0.0064,      0.0169,\n",
       "              0.0036,      0.0119,      0.0100,      0.0125,     -0.0027,\n",
       "              0.0107,     -0.0136,      0.0171,     -0.0138,      0.0088,\n",
       "              0.0078,      0.0059,      0.0136,      0.0101,     -0.0118,\n",
       "             -0.0009,      0.0127,     -0.0074,     -0.0129,      0.0014,\n",
       "             -0.0104,      0.0138,      0.0036,     -0.0130,     -0.0077,\n",
       "              0.0109,      0.0004,      0.0169,      0.0070,     -0.0027,\n",
       "             -0.0126,      0.0004,      0.0055,     -0.0072,     -0.0075,\n",
       "             -0.0056,     -0.0081,      0.0034,      0.0002,      0.0145,\n",
       "             -0.0107,     -0.0140,     -0.0168,     -0.0022,      0.0106,\n",
       "             -0.0100,      0.0037,      0.0130,     -0.0058,     -0.0105,\n",
       "              0.0079,     -0.0077,     -0.0165,     -0.0086,      0.0118,\n",
       "             -0.0086,     -0.0063,      0.0159,     -0.0031,      0.0165,\n",
       "              0.0053,     -0.0140,     -0.0063,      0.0071,     -0.0133,\n",
       "             -0.0174,     -0.0135,      0.0131,      0.0006,     -0.0054,\n",
       "             -0.0114,     -0.0078,      0.0158,      0.0072,      0.0067,\n",
       "             -0.0130,     -0.0052,     -0.0030,     -0.0122,      0.0093,\n",
       "             -0.0031,      0.0042,      0.0063,      0.0138,      0.0102,\n",
       "             -0.0098,      0.0095,     -0.0174,     -0.0030,     -0.0162,\n",
       "              0.0129,     -0.0174,      0.0083,      0.0095,     -0.0175,\n",
       "              0.0115,     -0.0080,     -0.0011,      0.0056,     -0.0008,\n",
       "             -0.0019,      0.0114,      0.0112,      0.0080,      0.0077,\n",
       "             -0.0070,      0.0031,     -0.0132,     -0.0140,      0.0106,\n",
       "              0.0111,      0.0109,      0.0036,      0.0071,      0.0030,\n",
       "             -0.0008,     -0.0007,     -0.0024,      0.0089,      0.0037,\n",
       "             -0.0066,     -0.0117,     -0.0103,      0.0123,      0.0009,\n",
       "              0.0176,      0.0095,     -0.0030,     -0.0124,      0.0103,\n",
       "             -0.0099,      0.0055,      0.0168,      0.0072,      0.0146,\n",
       "             -0.0167,      0.0163,     -0.0171,      0.0157,     -0.0124,\n",
       "              0.0146,     -0.0030,      0.0081,      0.0152,     -0.0167,\n",
       "              0.0012,     -0.0031,      0.0011,      0.0179,     -0.0178,\n",
       "              0.0003,     -0.0173,     -0.0014,     -0.0094,      0.0150,\n",
       "              0.0131,     -0.0064,      0.0014,      0.0156,      0.0178,\n",
       "              0.0104,      0.0149,     -0.0161,      0.0132,      0.0096,\n",
       "             -0.0158,     -0.0120,     -0.0131,      0.0071,     -0.0003,\n",
       "              0.0049,     -0.0055,     -0.0122,      0.0026,     -0.0088,\n",
       "              0.0045,      0.0105,      0.0019,     -0.0070,      0.0135,\n",
       "              0.0084,     -0.0013,      0.0163,     -0.0063,     -0.0100,\n",
       "             -0.0098,      0.0154,     -0.0009,     -0.0123,      0.0020,\n",
       "             -0.0129,     -0.0039,     -0.0075,      0.0071,      0.0094,\n",
       "              0.0001,     -0.0081,      0.0142,     -0.0017,      0.0130,\n",
       "             -0.0027,     -0.0100,     -0.0116,      0.0158,      0.0063,\n",
       "             -0.0018,      0.0138,      0.0139,     -0.0052,     -0.0165,\n",
       "              0.0002,      0.0140,      0.0001,     -0.0126,      0.0009,\n",
       "             -0.0003,     -0.0031,     -0.0077,      0.0166,      0.0145,\n",
       "              0.0053,      0.0058,     -0.0030,     -0.0003,     -0.0098,\n",
       "             -0.0091,     -0.0146,      0.0059,     -0.0079,      0.0076,\n",
       "              0.0067,     -0.0067,     -0.0061,     -0.0150,      0.0158,\n",
       "              0.0171,      0.0146,     -0.0072,     -0.0073,      0.0118,\n",
       "              0.0175,      0.0116,     -0.0032,      0.0030,     -0.0167,\n",
       "             -0.0178,      0.0108,      0.0154,     -0.0156,     -0.0041,\n",
       "             -0.0154,     -0.0042,      0.0044,      0.0111,      0.0039,\n",
       "              0.0071,     -0.0033,      0.0124,     -0.0165,     -0.0074,\n",
       "             -0.0031,     -0.0070,     -0.0156,      0.0156,      0.0173,\n",
       "              0.0158,     -0.0165,      0.0020,      0.0133,     -0.0067,\n",
       "              0.0026,      0.0057,      0.0166,     -0.0170,      0.0158,\n",
       "             -0.0156,      0.0105,      0.0131,     -0.0122,      0.0088,\n",
       "             -0.0165,     -0.0068,     -0.0041,      0.0106,      0.0163,\n",
       "             -0.0024,     -0.0036,     -0.0078,      0.0061,     -0.0152,\n",
       "              0.0039,     -0.0148,     -0.0178,      0.0054,     -0.0061,\n",
       "              0.0089,     -0.0117,     -0.0014,     -0.0092,     -0.0135,\n",
       "             -0.0165,      0.0140,     -0.0125,      0.0130,      0.0097,\n",
       "             -0.0146,     -0.0014,     -0.0141,     -0.0029,     -0.0149,\n",
       "             -0.0084,      0.0072,     -0.0127,      0.0087,     -0.0099,\n",
       "              0.0119,      0.0107,      0.0102,     -0.0005,     -0.0011,\n",
       "             -0.0109,      0.0079,     -0.0125,      0.0120,     -0.0160,\n",
       "             -0.0046,      0.0114,     -0.0176,      0.0097,     -0.0042,\n",
       "              0.0050,     -0.0148,      0.0015,     -0.0082,      0.0103,\n",
       "              0.0009,     -0.0099,     -0.0060,      0.0161,     -0.0006,\n",
       "              0.0138,     -0.0017,      0.0159,     -0.0013,     -0.0106,\n",
       "              0.0116,      0.0172,      0.0175,      0.0165,      0.0050,\n",
       "              0.0111,     -0.0005,      0.0122,     -0.0016,      0.0089,\n",
       "              0.0098,     -0.0143,      0.0102,      0.0110,      0.0146,\n",
       "             -0.0008,     -0.0119,      0.0123], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0276, -0.0322,  0.0332,  ..., -0.0339, -0.0001, -0.0263],\n",
       "         [-0.0323, -0.0122,  0.0275,  ...,  0.0188,  0.0303,  0.0059],\n",
       "         [-0.0320, -0.0148,  0.0135,  ...,  0.0011, -0.0072, -0.0062],\n",
       "         ...,\n",
       "         [ 0.0201,  0.0251, -0.0285,  ..., -0.0163, -0.0334, -0.0055],\n",
       "         [-0.0226,  0.0110,  0.0217,  ...,  0.0059,  0.0289, -0.0359],\n",
       "         [-0.0284,  0.0094, -0.0019,  ...,  0.0064, -0.0057, -0.0124]],\n",
       "        requires_grad=True)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")\n",
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6f076322-1ab1-42d6-9b89-3421ba39e8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "层名: tok_emb.weight | 形状: torch.Size([50257, 768]) | 参数量: 38,597,376\n",
      "层名: pos_emb.weight | 形状: torch.Size([1024, 768]) | 参数量: 786,432\n",
      "层名: trf_blocks.0.att.W_query.weight | 形状: torch.Size([768, 768]) | 参数量: 589,824\n",
      "层名: trf_blocks.0.att.W_key.weight | 形状: torch.Size([768, 768]) | 参数量: 589,824\n",
      "层名: trf_blocks.0.att.W_value.weight | 形状: torch.Size([768, 768]) | 参数量: 589,824\n",
      "层名: trf_blocks.0.att.out_proj.weight | 形状: torch.Size([768, 768]) | 参数量: 589,824\n",
      "层名: trf_blocks.0.att.out_proj.bias | 形状: torch.Size([768]) | 参数量: 768\n",
      "层名: trf_blocks.0.ff.layers.0.weight | 形状: torch.Size([3072, 768]) | 参数量: 2,359,296\n",
      "层名: trf_blocks.0.ff.layers.0.bias | 形状: torch.Size([3072]) | 参数量: 3,072\n",
      "层名: trf_blocks.0.ff.layers.2.weight | 形状: torch.Size([768, 3072]) | 参数量: 2,359,296\n",
      "层名: trf_blocks.0.ff.layers.2.bias | 形状: torch.Size([768]) | 参数量: 768\n",
      "层名: trf_blocks.0.norm1.scale | 形状: torch.Size([768]) | 参数量: 768\n",
      "层名: trf_blocks.0.norm1.shift | 形状: torch.Size([768]) | 参数量: 768\n",
      "层名: trf_blocks.0.norm2.scale | 形状: torch.Size([768]) | 参数量: 768\n",
      "层名: trf_blocks.0.norm2.shift | 形状: torch.Size([768]) | 参数量: 768\n",
      "层名: trf_blocks.1.att.W_query.weight | 形状: torch.Size([768, 768]) | 参数量: 589,824\n",
      "层名: trf_blocks.1.att.W_key.weight | 形状: torch.Size([768, 768]) | 参数量: 589,824\n",
      "层名: trf_blocks.1.att.W_value.weight | 形状: torch.Size([768, 768]) | 参数量: 589,824\n",
      "层名: trf_blocks.1.att.out_proj.weight | 形状: torch.Size([768, 768]) | 参数量: 589,824\n",
      "层名: trf_blocks.1.att.out_proj.bias | 形状: torch.Size([768]) | 参数量: 768\n",
      "层名: trf_blocks.1.ff.layers.0.weight | 形状: torch.Size([3072, 768]) | 参数量: 2,359,296\n",
      "层名: trf_blocks.1.ff.layers.0.bias | 形状: torch.Size([3072]) | 参数量: 3,072\n",
      "层名: trf_blocks.1.ff.layers.2.weight | 形状: torch.Size([768, 3072]) | 参数量: 2,359,296\n",
      "层名: trf_blocks.1.ff.layers.2.bias | 形状: torch.Size([768]) | 参数量: 768\n",
      "层名: trf_blocks.1.norm1.scale | 形状: torch.Size([768]) | 参数量: 768\n",
      "层名: trf_blocks.1.norm1.shift | 形状: torch.Size([768]) | 参数量: 768\n",
      "层名: trf_blocks.1.norm2.scale | 形状: torch.Size([768]) | 参数量: 768\n",
      "层名: trf_blocks.1.norm2.shift | 形状: torch.Size([768]) | 参数量: 768\n",
      "层名: trf_blocks.2.att.W_query.weight | 形状: torch.Size([768, 768]) | 参数量: 589,824\n",
      "层名: trf_blocks.2.att.W_key.weight | 形状: torch.Size([768, 768]) | 参数量: 589,824\n",
      "层名: trf_blocks.2.att.W_value.weight | 形状: torch.Size([768, 768]) | 参数量: 589,824\n",
      "层名: trf_blocks.2.att.out_proj.weight | 形状: torch.Size([768, 768]) | 参数量: 589,824\n",
      "层名: trf_blocks.2.att.out_proj.bias | 形状: torch.Size([768]) | 参数量: 768\n",
      "层名: trf_blocks.2.ff.layers.0.weight | 形状: torch.Size([3072, 768]) | 参数量: 2,359,296\n",
      "层名: trf_blocks.2.ff.layers.0.bias | 形状: torch.Size([3072]) | 参数量: 3,072\n",
      "层名: trf_blocks.2.ff.layers.2.weight | 形状: torch.Size([768, 3072]) | 参数量: 2,359,296\n",
      "层名: trf_blocks.2.ff.layers.2.bias | 形状: torch.Size([768]) | 参数量: 768\n",
      "层名: trf_blocks.2.norm1.scale | 形状: torch.Size([768]) | 参数量: 768\n",
      "层名: trf_blocks.2.norm1.shift | 形状: torch.Size([768]) | 参数量: 768\n",
      "层名: trf_blocks.2.norm2.scale | 形状: torch.Size([768]) | 参数量: 768\n",
      "层名: trf_blocks.2.norm2.shift | 形状: torch.Size([768]) | 参数量: 768\n",
      "层名: trf_blocks.3.att.W_query.weight | 形状: torch.Size([768, 768]) | 参数量: 589,824\n",
      "层名: trf_blocks.3.att.W_key.weight | 形状: torch.Size([768, 768]) | 参数量: 589,824\n",
      "层名: trf_blocks.3.att.W_value.weight | 形状: torch.Size([768, 768]) | 参数量: 589,824\n",
      "层名: trf_blocks.3.att.out_proj.weight | 形状: torch.Size([768, 768]) | 参数量: 589,824\n",
      "层名: trf_blocks.3.att.out_proj.bias | 形状: torch.Size([768]) | 参数量: 768\n",
      "层名: trf_blocks.3.ff.layers.0.weight | 形状: torch.Size([3072, 768]) | 参数量: 2,359,296\n",
      "层名: trf_blocks.3.ff.layers.0.bias | 形状: torch.Size([3072]) | 参数量: 3,072\n",
      "层名: trf_blocks.3.ff.layers.2.weight | 形状: torch.Size([768, 3072]) | 参数量: 2,359,296\n",
      "层名: trf_blocks.3.ff.layers.2.bias | 形状: torch.Size([768]) | 参数量: 768\n",
      "层名: trf_blocks.3.norm1.scale | 形状: torch.Size([768]) | 参数量: 768\n",
      "层名: trf_blocks.3.norm1.shift | 形状: torch.Size([768]) | 参数量: 768\n",
      "层名: trf_blocks.3.norm2.scale | 形状: torch.Size([768]) | 参数量: 768\n",
      "层名: trf_blocks.3.norm2.shift | 形状: torch.Size([768]) | 参数量: 768\n",
      "层名: trf_blocks.4.att.W_query.weight | 形状: torch.Size([768, 768]) | 参数量: 589,824\n",
      "层名: trf_blocks.4.att.W_key.weight | 形状: torch.Size([768, 768]) | 参数量: 589,824\n",
      "层名: trf_blocks.4.att.W_value.weight | 形状: torch.Size([768, 768]) | 参数量: 589,824\n",
      "层名: trf_blocks.4.att.out_proj.weight | 形状: torch.Size([768, 768]) | 参数量: 589,824\n",
      "层名: trf_blocks.4.att.out_proj.bias | 形状: torch.Size([768]) | 参数量: 768\n",
      "层名: trf_blocks.4.ff.layers.0.weight | 形状: torch.Size([3072, 768]) | 参数量: 2,359,296\n",
      "层名: trf_blocks.4.ff.layers.0.bias | 形状: torch.Size([3072]) | 参数量: 3,072\n",
      "层名: trf_blocks.4.ff.layers.2.weight | 形状: torch.Size([768, 3072]) | 参数量: 2,359,296\n",
      "层名: trf_blocks.4.ff.layers.2.bias | 形状: torch.Size([768]) | 参数量: 768\n",
      "层名: trf_blocks.4.norm1.scale | 形状: torch.Size([768]) | 参数量: 768\n",
      "层名: trf_blocks.4.norm1.shift | 形状: torch.Size([768]) | 参数量: 768\n",
      "层名: trf_blocks.4.norm2.scale | 形状: torch.Size([768]) | 参数量: 768\n",
      "层名: trf_blocks.4.norm2.shift | 形状: torch.Size([768]) | 参数量: 768\n",
      "层名: trf_blocks.5.att.W_query.weight | 形状: torch.Size([768, 768]) | 参数量: 589,824\n",
      "层名: trf_blocks.5.att.W_key.weight | 形状: torch.Size([768, 768]) | 参数量: 589,824\n",
      "层名: trf_blocks.5.att.W_value.weight | 形状: torch.Size([768, 768]) | 参数量: 589,824\n",
      "层名: trf_blocks.5.att.out_proj.weight | 形状: torch.Size([768, 768]) | 参数量: 589,824\n",
      "层名: trf_blocks.5.att.out_proj.bias | 形状: torch.Size([768]) | 参数量: 768\n",
      "层名: trf_blocks.5.ff.layers.0.weight | 形状: torch.Size([3072, 768]) | 参数量: 2,359,296\n",
      "层名: trf_blocks.5.ff.layers.0.bias | 形状: torch.Size([3072]) | 参数量: 3,072\n",
      "层名: trf_blocks.5.ff.layers.2.weight | 形状: torch.Size([768, 3072]) | 参数量: 2,359,296\n",
      "层名: trf_blocks.5.ff.layers.2.bias | 形状: torch.Size([768]) | 参数量: 768\n",
      "层名: trf_blocks.5.norm1.scale | 形状: torch.Size([768]) | 参数量: 768\n",
      "层名: trf_blocks.5.norm1.shift | 形状: torch.Size([768]) | 参数量: 768\n",
      "层名: trf_blocks.5.norm2.scale | 形状: torch.Size([768]) | 参数量: 768\n",
      "层名: trf_blocks.5.norm2.shift | 形状: torch.Size([768]) | 参数量: 768\n",
      "层名: trf_blocks.6.att.W_query.weight | 形状: torch.Size([768, 768]) | 参数量: 589,824\n",
      "层名: trf_blocks.6.att.W_key.weight | 形状: torch.Size([768, 768]) | 参数量: 589,824\n",
      "层名: trf_blocks.6.att.W_value.weight | 形状: torch.Size([768, 768]) | 参数量: 589,824\n",
      "层名: trf_blocks.6.att.out_proj.weight | 形状: torch.Size([768, 768]) | 参数量: 589,824\n",
      "层名: trf_blocks.6.att.out_proj.bias | 形状: torch.Size([768]) | 参数量: 768\n",
      "层名: trf_blocks.6.ff.layers.0.weight | 形状: torch.Size([3072, 768]) | 参数量: 2,359,296\n",
      "层名: trf_blocks.6.ff.layers.0.bias | 形状: torch.Size([3072]) | 参数量: 3,072\n",
      "层名: trf_blocks.6.ff.layers.2.weight | 形状: torch.Size([768, 3072]) | 参数量: 2,359,296\n",
      "层名: trf_blocks.6.ff.layers.2.bias | 形状: torch.Size([768]) | 参数量: 768\n",
      "层名: trf_blocks.6.norm1.scale | 形状: torch.Size([768]) | 参数量: 768\n",
      "层名: trf_blocks.6.norm1.shift | 形状: torch.Size([768]) | 参数量: 768\n",
      "层名: trf_blocks.6.norm2.scale | 形状: torch.Size([768]) | 参数量: 768\n",
      "层名: trf_blocks.6.norm2.shift | 形状: torch.Size([768]) | 参数量: 768\n",
      "层名: trf_blocks.7.att.W_query.weight | 形状: torch.Size([768, 768]) | 参数量: 589,824\n",
      "层名: trf_blocks.7.att.W_key.weight | 形状: torch.Size([768, 768]) | 参数量: 589,824\n",
      "层名: trf_blocks.7.att.W_value.weight | 形状: torch.Size([768, 768]) | 参数量: 589,824\n",
      "层名: trf_blocks.7.att.out_proj.weight | 形状: torch.Size([768, 768]) | 参数量: 589,824\n",
      "层名: trf_blocks.7.att.out_proj.bias | 形状: torch.Size([768]) | 参数量: 768\n",
      "层名: trf_blocks.7.ff.layers.0.weight | 形状: torch.Size([3072, 768]) | 参数量: 2,359,296\n",
      "层名: trf_blocks.7.ff.layers.0.bias | 形状: torch.Size([3072]) | 参数量: 3,072\n",
      "层名: trf_blocks.7.ff.layers.2.weight | 形状: torch.Size([768, 3072]) | 参数量: 2,359,296\n",
      "层名: trf_blocks.7.ff.layers.2.bias | 形状: torch.Size([768]) | 参数量: 768\n",
      "层名: trf_blocks.7.norm1.scale | 形状: torch.Size([768]) | 参数量: 768\n",
      "层名: trf_blocks.7.norm1.shift | 形状: torch.Size([768]) | 参数量: 768\n",
      "层名: trf_blocks.7.norm2.scale | 形状: torch.Size([768]) | 参数量: 768\n",
      "层名: trf_blocks.7.norm2.shift | 形状: torch.Size([768]) | 参数量: 768\n",
      "层名: trf_blocks.8.att.W_query.weight | 形状: torch.Size([768, 768]) | 参数量: 589,824\n",
      "层名: trf_blocks.8.att.W_key.weight | 形状: torch.Size([768, 768]) | 参数量: 589,824\n",
      "层名: trf_blocks.8.att.W_value.weight | 形状: torch.Size([768, 768]) | 参数量: 589,824\n",
      "层名: trf_blocks.8.att.out_proj.weight | 形状: torch.Size([768, 768]) | 参数量: 589,824\n",
      "层名: trf_blocks.8.att.out_proj.bias | 形状: torch.Size([768]) | 参数量: 768\n",
      "层名: trf_blocks.8.ff.layers.0.weight | 形状: torch.Size([3072, 768]) | 参数量: 2,359,296\n",
      "层名: trf_blocks.8.ff.layers.0.bias | 形状: torch.Size([3072]) | 参数量: 3,072\n",
      "层名: trf_blocks.8.ff.layers.2.weight | 形状: torch.Size([768, 3072]) | 参数量: 2,359,296\n",
      "层名: trf_blocks.8.ff.layers.2.bias | 形状: torch.Size([768]) | 参数量: 768\n",
      "层名: trf_blocks.8.norm1.scale | 形状: torch.Size([768]) | 参数量: 768\n",
      "层名: trf_blocks.8.norm1.shift | 形状: torch.Size([768]) | 参数量: 768\n",
      "层名: trf_blocks.8.norm2.scale | 形状: torch.Size([768]) | 参数量: 768\n",
      "层名: trf_blocks.8.norm2.shift | 形状: torch.Size([768]) | 参数量: 768\n",
      "层名: trf_blocks.9.att.W_query.weight | 形状: torch.Size([768, 768]) | 参数量: 589,824\n",
      "层名: trf_blocks.9.att.W_key.weight | 形状: torch.Size([768, 768]) | 参数量: 589,824\n",
      "层名: trf_blocks.9.att.W_value.weight | 形状: torch.Size([768, 768]) | 参数量: 589,824\n",
      "层名: trf_blocks.9.att.out_proj.weight | 形状: torch.Size([768, 768]) | 参数量: 589,824\n",
      "层名: trf_blocks.9.att.out_proj.bias | 形状: torch.Size([768]) | 参数量: 768\n",
      "层名: trf_blocks.9.ff.layers.0.weight | 形状: torch.Size([3072, 768]) | 参数量: 2,359,296\n",
      "层名: trf_blocks.9.ff.layers.0.bias | 形状: torch.Size([3072]) | 参数量: 3,072\n",
      "层名: trf_blocks.9.ff.layers.2.weight | 形状: torch.Size([768, 3072]) | 参数量: 2,359,296\n",
      "层名: trf_blocks.9.ff.layers.2.bias | 形状: torch.Size([768]) | 参数量: 768\n",
      "层名: trf_blocks.9.norm1.scale | 形状: torch.Size([768]) | 参数量: 768\n",
      "层名: trf_blocks.9.norm1.shift | 形状: torch.Size([768]) | 参数量: 768\n",
      "层名: trf_blocks.9.norm2.scale | 形状: torch.Size([768]) | 参数量: 768\n",
      "层名: trf_blocks.9.norm2.shift | 形状: torch.Size([768]) | 参数量: 768\n",
      "层名: trf_blocks.10.att.W_query.weight | 形状: torch.Size([768, 768]) | 参数量: 589,824\n",
      "层名: trf_blocks.10.att.W_key.weight | 形状: torch.Size([768, 768]) | 参数量: 589,824\n",
      "层名: trf_blocks.10.att.W_value.weight | 形状: torch.Size([768, 768]) | 参数量: 589,824\n",
      "层名: trf_blocks.10.att.out_proj.weight | 形状: torch.Size([768, 768]) | 参数量: 589,824\n",
      "层名: trf_blocks.10.att.out_proj.bias | 形状: torch.Size([768]) | 参数量: 768\n",
      "层名: trf_blocks.10.ff.layers.0.weight | 形状: torch.Size([3072, 768]) | 参数量: 2,359,296\n",
      "层名: trf_blocks.10.ff.layers.0.bias | 形状: torch.Size([3072]) | 参数量: 3,072\n",
      "层名: trf_blocks.10.ff.layers.2.weight | 形状: torch.Size([768, 3072]) | 参数量: 2,359,296\n",
      "层名: trf_blocks.10.ff.layers.2.bias | 形状: torch.Size([768]) | 参数量: 768\n",
      "层名: trf_blocks.10.norm1.scale | 形状: torch.Size([768]) | 参数量: 768\n",
      "层名: trf_blocks.10.norm1.shift | 形状: torch.Size([768]) | 参数量: 768\n",
      "层名: trf_blocks.10.norm2.scale | 形状: torch.Size([768]) | 参数量: 768\n",
      "层名: trf_blocks.10.norm2.shift | 形状: torch.Size([768]) | 参数量: 768\n",
      "层名: trf_blocks.11.att.W_query.weight | 形状: torch.Size([768, 768]) | 参数量: 589,824\n",
      "层名: trf_blocks.11.att.W_key.weight | 形状: torch.Size([768, 768]) | 参数量: 589,824\n",
      "层名: trf_blocks.11.att.W_value.weight | 形状: torch.Size([768, 768]) | 参数量: 589,824\n",
      "层名: trf_blocks.11.att.out_proj.weight | 形状: torch.Size([768, 768]) | 参数量: 589,824\n",
      "层名: trf_blocks.11.att.out_proj.bias | 形状: torch.Size([768]) | 参数量: 768\n",
      "层名: trf_blocks.11.ff.layers.0.weight | 形状: torch.Size([3072, 768]) | 参数量: 2,359,296\n",
      "层名: trf_blocks.11.ff.layers.0.bias | 形状: torch.Size([3072]) | 参数量: 3,072\n",
      "层名: trf_blocks.11.ff.layers.2.weight | 形状: torch.Size([768, 3072]) | 参数量: 2,359,296\n",
      "层名: trf_blocks.11.ff.layers.2.bias | 形状: torch.Size([768]) | 参数量: 768\n",
      "层名: trf_blocks.11.norm1.scale | 形状: torch.Size([768]) | 参数量: 768\n",
      "层名: trf_blocks.11.norm1.shift | 形状: torch.Size([768]) | 参数量: 768\n",
      "层名: trf_blocks.11.norm2.scale | 形状: torch.Size([768]) | 参数量: 768\n",
      "层名: trf_blocks.11.norm2.shift | 形状: torch.Size([768]) | 参数量: 768\n",
      "层名: final_norm.scale | 形状: torch.Size([768]) | 参数量: 768\n",
      "层名: final_norm.shift | 形状: torch.Size([768]) | 参数量: 768\n",
      "层名: out_head.weight | 形状: torch.Size([50257, 768]) | 参数量: 38,597,376\n"
     ]
    }
   ],
   "source": [
    "para_list = list(model.parameters())\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"层名: {name} | 形状: {param.shape} | 参数量: {param.numel():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67d13dd-dd01-4ba6-a2ad-31ca8a9fd660",
   "metadata": {},
   "source": [
    "- As we see above, this model has 163M, not 124M parameters; why?\n",
    "- In the original GPT-2 paper, the researchers applied weight tying, which means that they reused the token embedding layer (`tok_emb`) as the output layer, which means setting `self.out_head.weight = self.tok_emb.weight`\n",
    "- The token embedding layer projects the 50,257-dimensional one-hot encoded input tokens to a 768-dimensional embedding representation\n",
    "- The output layer projects 768-dimensional embeddings back into a 50,257-dimensional representation so that we can convert these back into words (more about that in the next section)\n",
    "- So, the embedding and output layer have the same number of weight parameters, as we can see based on the shape of their weight matrices\n",
    "- However, a quick note about its size: we previously referred to it as a 124M parameter model; we can double check this number as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e3b43233-e9b8-4f5a-b72b-a263ec686982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token embedding layer shape: torch.Size([50257, 768])\n",
      "Output layer shape: torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "print(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\n",
    "print(\"Output layer shape:\", model.out_head.weight.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02259f6-6f79-4c89-a866-4ebeae1c3289",
   "metadata": {},
   "source": [
    "- In the original GPT-2 paper, the researchers reused the token embedding matrix as an output matrix\n",
    "- Correspondingly, if we subtracted the number of parameters of the output layer, we'd get a 124M parameter model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "95a22e02-50d3-48b3-a4e0-d9863343c164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters considering weight tying: 124,412,160\n"
     ]
    }
   ],
   "source": [
    "total_params_gpt2 =  total_params - sum(p.numel() for p in model.out_head.parameters())\n",
    "print(f\"Number of trainable parameters considering weight tying: {total_params_gpt2:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b03f80-b94c-46e7-9d42-d0df399ff3db",
   "metadata": {},
   "source": [
    "- In practice, I found it easier to train the model without weight-tying, which is why we didn't implement it here\n",
    "- However, we will revisit and apply this weight-tying idea later when we load the pretrained weights in chapter 5\n",
    "- Lastly, we can compute the memory requirements of the model as follows, which can be a helpful reference point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5131a752-fab8-4d70-a600-e29870b33528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of the model: 621.83 MB\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total size in bytes (assuming float32, 4 bytes per parameter)\n",
    "total_size_bytes = total_params * 4\n",
    "\n",
    "# Convert to megabytes\n",
    "total_size_mb = total_size_bytes / (1024 * 1024)\n",
    "\n",
    "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309a3be4-c20a-4657-b4e0-77c97510b47c",
   "metadata": {},
   "source": [
    "- Exercise: you can try the following other configurations, which are referenced in the [GPT-2 paper](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=dOad5HoAAAAJ&citation_for_view=dOad5HoAAAAJ:YsMSGLbcyi4C), as well.\n",
    "\n",
    "    - **GPT2-small** (the 124M configuration we already implemented):\n",
    "        - \"emb_dim\" = 768\n",
    "        - \"n_layers\" = 12\n",
    "        - \"n_heads\" = 12\n",
    "\n",
    "    - **GPT2-medium:**\n",
    "        - \"emb_dim\" = 1024\n",
    "        - \"n_layers\" = 24\n",
    "        - \"n_heads\" = 16\n",
    "    \n",
    "    - **GPT2-large:**\n",
    "        - \"emb_dim\" = 1280\n",
    "        - \"n_layers\" = 36\n",
    "        - \"n_heads\" = 20\n",
    "    \n",
    "    - **GPT2-XL:**\n",
    "        - \"emb_dim\" = 1600\n",
    "        - \"n_layers\" = 48\n",
    "        - \"n_heads\" = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5d9bc0-95ab-45d4-9378-417628d86e35",
   "metadata": {},
   "source": [
    "## 4.7 Generating text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48da5deb-6ee0-4b9b-8dd2-abed7ed65172",
   "metadata": {},
   "source": [
    "- LLMs like the GPT model we implemented above are used to generate one word at a time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caade12a-fe97-480f-939c-87d24044edff",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/16.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7061524-a3bd-4803-ade6-2e3b7b79ac13",
   "metadata": {},
   "source": [
    "- The following `generate_text_simple` function implements greedy decoding, which is a simple and fast method to generate text\n",
    "- In greedy decoding, at each step, the model chooses the word (or token) with the highest probability as its next output (the highest logit corresponds to the highest probability, so we technically wouldn't even have to compute the softmax function explicitly)\n",
    "- In the next chapter, we will implement a more advanced `generate_text` function\n",
    "- The figure below depicts how the GPT model, given an input context, generates the next word token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee0f32c-c18c-445e-b294-a879de2aa187",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/17.webp\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c9b428a9-8764-4b36-80cd-7d4e00595ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx is (batch, n_tokens) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "        \n",
    "        # Crop current context if it exceeds the supported context size\n",
    "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
    "        # then only the last 5 tokens are used as context\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        \n",
    "        # Get the predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        \n",
    "        # Focus only on the last time step\n",
    "        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]  \n",
    "\n",
    "        # Apply softmax to get probabilities\n",
    "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
    "\n",
    "        # Get the idx of the vocab entry with the highest probability value\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "        # Append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6515f2c1-3cc7-421c-8d58-cc2f563b7030",
   "metadata": {},
   "source": [
    "- The `generate_text_simple` above implements an iterative process, where it creates one token at a time\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/18.webp\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f682eac4-f9bd-438b-9dec-6b1cc7bc05ce",
   "metadata": {},
   "source": [
    "- Let's prepare an input example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3d7e3e94-df0f-4c0f-a6a1-423f500ac1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Hello, I am\"\n",
    "\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encoded:\", encoded)\n",
    "\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "print(\"encoded_tensor.shape:\", encoded_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a72a9b60-de66-44cf-b2f9-1e638934ada4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[15496,    11,   314,   716, 27018, 24086, 47843, 30961, 42348,  7267]])\n",
      "Output length: 10\n"
     ]
    }
   ],
   "source": [
    "model.eval() # disable dropout\n",
    "\n",
    "out = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=encoded_tensor, \n",
    "    max_new_tokens=6, \n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output:\", out)\n",
    "print(\"Output length:\", len(out[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d131c00-1787-44ba-bec3-7c145497b2c3",
   "metadata": {},
   "source": [
    "- Remove batch dimension and convert back into text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "053d99f6-5710-4446-8d52-117fb34ea9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am Featureiman Byeswickattribute argue\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a894003-51f6-4ccc-996f-3b9c7d5a1d70",
   "metadata": {},
   "source": [
    "- Note that the model is untrained; hence the random output texts above\n",
    "- We will train the model in the next chapter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35278b6-9e5c-480f-83e5-011a1173648f",
   "metadata": {},
   "source": [
    "## Summary and takeaways\n",
    "\n",
    "- See the [./gpt.py](./gpt.py) script, a self-contained script containing the GPT model we implement in this Jupyter notebook\n",
    "- You can find the exercise solutions in [./exercise-solutions.ipynb](./exercise-solutions.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
